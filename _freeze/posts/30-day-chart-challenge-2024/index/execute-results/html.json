{
  "hash": "fac4325cbca3b7e955acba5b48d746b0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"30 Day Chart Challenge 2024\"\ndescription: \"Let's make some charts!\"\nauthor: \"gregers kjerulf dubrow\"\ndate: \"today\"\ncategories: [post, rstats, ggplot, dataviz, chartchallenge, tidytuesday, eurostat]\nimage: \"prompts.png\"\ntoc: true\nlightbox: true\neditor: \n  mode: source\n---\n\n\n\n\n![](prompts.png){width=\"75%\" fig-align=\"left\" fig-alt=\"image listing the prompts for the 30 day chart challenge 2024.\"}\n\n## My Plan for the Challenge {#plan}\n\nIf you regularly scroll through dataviz social media you probably see lots of posts in April where people show off their work for the [30 Day Chart Challenge](https://github.com/30DayChartChallenge/Edition2024). I never participate in these chart or map challenges because usually they take me by surprise and it's alreayd a week in before I notice and then I figure I'm too late.\n\nWell not this year. Even though I'm (checks calendar) *three weeks* late to the party, I'm going to try and add a few charts to the challenge. I figure it's a good way to keep me consistently creating new visualisations, using the prompts to spur me to think about different data sources and dataviz approaches. My plan is to:\n\n-   Produce charts for as many prompts as possible, working in prompt-day order.\n-   Post them to social media on my [Bluesky](https://bsky.app/profile/gregerskjerulf.bsky.social), [Mastodon](https://fosstodon.org/@Greg_Dubrow), [Twitter](https://twitter.com/greg_dubrow), and [LinkedIn](https://www.linkedin.com/in/dubrowg/) accounts.\n-   Add them to this post, meaning the posting date will refresh to the date I've added the chart.So If I get to alot or even all of them, this will be a long post.\n-   Use the table of contents to the right to navigate to a specific chart.\n\nI may not get to every prompt, but I'll do what I can. So let's get going.\n\n## Prompt #1 - Part-to-Whole {#prompt1}\n\nI chose to interpret this prompt by doing a stacked bar chart displaying educational attainment as a proportion of populations by regions in the UK. It doubles as a late-to-the-party entry for the [January 23, 2024 Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/tree/master/data/2024/2024-01-23) challenge.\n\nThe original data is from the UK Office of National Statistics, in a report they produced looking at [educational attainment by socio-economic factors](https://www.ons.gov.uk/peoplepopulationandcommunity/educationandchildcare/articles/whydochildrenandyoungpeopleinsmallertownsdobetteracademicallythanthoseinlargertowns/2023-07-25).\n\nMy plan for this was to knock out the chart in about an hour, but I needed up having to do a bit more data transformation than I thought. I also had to read up a bit on UK education [levels](https://www.gov.uk/what-different-qualification-levels-mean/list-of-qualification-levels) and [stages](https://www.gov.uk/national-curriculum). In the end, it took a few hours to get it as I wanted.\n\nFirst we'll get and clean the data...\n\n\n::: {.cell ECHO='true'}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for getting and cleaning data\"}\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidytuesdayR) #to get data from tidy tuesday repo\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(ggtext) # to help make ggplot text look good\n\n# load tidy tuesday data\nukeduc1 <- tt_load(\"2024-01-23\")\n#> \n#> \tDownloading file 1 of 1: `english_education.csv`\n\n# create tibble from csv, clean data, add some fields for the chart\nukeduc <- as_tibble(ukeduc1$english_education) %>%\n\tmutate(town11nm = ifelse(town11nm == \"Outer london BUAs\", \"Outer London BUAs\", town11nm)) %>%\n\tmutate(ttwa_classification = ifelse(\n\t\ttown11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"), \n\t\t\"Majority conurbation\", ttwa_classification)) %>%\n\tmutate(ttwa11nm = ifelse(\n\t\ttown11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"), \"London\", ttwa11nm)) %>%\n\tmutate(ttwa11cd = ifelse(\n\t\ttown11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"), \"E30000234\", ttwa11cd)) %>%\n\tmutate(across(26:29, ~ifelse(is.na(.),0,.))) %>%\n\tmutate(level_sum = rowSums(.[c(26:29)])) %>%\n\t# to get the number of students in the achievement groups we need to multiply the cohort\n\t# by the percentage and divide by 100 as in the original data they were displayed as \n\t# full numbers, eg 17.2 instead of 0.172.\n\tmutate(highest_level_qualification_achieved_by_age_22_na = 100 - level_sum) %>%\n\tmutate(n_lesslev1_age22 =\n\t\t\tround(highest_level_qualification_achieved_by_age_22_less_than_level_1 * (ks4_2012_2013_counts/100) ,0)) %>%\n\tmutate(n_lev1to2_age22 =\n\t\t\tround(highest_level_qualification_achieved_by_age_22_level_1_to_level_2 * (ks4_2012_2013_counts/100) ,0)) %>%\n\tmutate(n_lev3to5_age22 =\n\t\t\tround(highest_level_qualification_achieved_by_age_22_level_3_to_level_5 * (ks4_2012_2013_counts/100) ,0)) %>%\n\tmutate(n_lev6plus_age22 =\n\t\t\tround(highest_level_qualification_achieved_by_age_22_level_6_or_above * (ks4_2012_2013_counts/100) ,0)) %>%\n\tmutate(n_lev_na_age22 =\n\t\t\tround(highest_level_qualification_achieved_by_age_22_na * (ks4_2012_2013_counts/100) ,0))\n```\n:::\n\n\nOk, so we have data, let's make the chart. It's a horizontal bar chart showing the eudcational attainment at age 22 of a cohort of 15-16 year old students who sat for college qualifying exams (GCSEs) in the 2012-13 school year. Meaning the ending time point here is 2018-19. Attainment is plotted by level and by region. Because the original data is by town, we have to group by region to get what we want.\n\n\n::: {.cell ECHO='true'}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for making the chart\"}\n# need to create cohort population by region\nukeduc %>%\n\trename(region =rgn11nm ) %>%\n\t# group by regions and create region totals \n\tgroup_by(region) %>%\n\tsummarise(region_cohort = sum(ks4_2012_2013_counts),\n\t\t\t\t\tregion_levless1_age22 = sum(n_lesslev1_age22),\n\t\t\t\t\tregion_lev1to2_age22 = sum(n_lev1to2_age22),\n\t\t\t\t\tregion_lev3to5_age22 = sum(n_lev3to5_age22),\n\t\t\t\t\tregion_lev6plus_age22 = sum(n_lev6plus_age22),\n\t\t\t\t\tregion_lev_na_age22 = sum(n_lev_na_age22)) %>%\n\t# pivot longer and make the education attainment field into factors. \n\t# factor labels for the chart\n\tpivot_longer(cols = ends_with(\"age22\"), \n\t\t\t\t\t\tnames_to = \"ed_ettain_age22\", \n\t\t\t\t\t\tvalues_to = \"ed_attain_n\") %>%\n\tmutate(\n\t\ted_ettain_age22 = \n\t\t\tfactor(ed_ettain_age22, \n\t\t\t\t\tlevels = c(\"region_lev_na_age22\", \"region_levless1_age22\", \"region_lev1to2_age22\", \n\t\t\t\t\t\t\t\"region_lev3to5_age22\",  \"region_lev6plus_age22\"), \n\t\t\t\t\tlabels = c(\"No data\", \"Level <1\", \"Level = 1-2\", \"Level = 3-5\", \"Level = 6+\"))) %>%\n\tmutate(ed_attain_pct = ed_attain_n / region_cohort) %>%\n\tmutate(ed_attain_pct2 = round(ed_attain_pct*100, 1)) %>%\n\tungroup() %>%\n\tmutate(region = factor(region)) %>%\n\tfilter(!is.na(region)) %>%\n\t# pass this temporary set thru to the ggplot call\n\t{. ->> tmp} %>%\n\tggplot(aes(ed_attain_pct, fct_rev(region), fill = fct_rev(ed_ettain_age22))) +\n\tgeom_bar(stat = \"identity\") +\n\tscale_x_continuous(\n\t\texpand = c(0,0), \n\t\tbreaks = c(0, 0.25, 0.50, 0.75, 1), \n\t\tlabels = c(\"0\", \"25%\", \"50%\", \"75%\", \"100%\")) +\n\tgeom_text(data = subset(tmp, ed_attain_pct >0.025),\n\t\taes(label = scales::percent(round(ed_attain_pct , 2))),\n\t\t\t\t\tposition = position_stack(vjust = 0.5),\n\t\t\t\t\tcolor= \"white\", vjust = 0.5, size = 5) +\n\tlabs(title = \"Students in London most likely to have at least 4-year degree by Age 22\",\n\t\tsubtitle = \"Sixth Year Educational Outcomes for Level 4 2012-13 Cohort by UK Region\",\n\t\tcaption = \"*Tidy Tuesday data 01/23/2024, from UK Office of National Statistics*\",\n\t\tx = \"\", y = \"Region\") +\n\tscale_fill_brewer(palette = \"Set2\") +\n\ttheme_minimal() +\n\ttheme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'), \n\t\t\tlegend.key.width = unit(1.5, 'cm'), legend.margin=margin(-10, 0, 0, 0),\n\t\t\tplot.title = element_text(hjust = 1), \n\t\t\tplot.caption = element_markdown(),\n\t\t\tpanel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n\tguides(fill = guide_legend(\n\t\tlabel.position = \"bottom\", reverse = TRUE, \n\t\ttitle = \"Cohort at Age 22\", title.position = \"top\"))\n```\n\n::: {.cell-output-display}\n![](images/prompt1_2-1.png){width=100%}\n:::\n:::\n\n\nWe see from this chart that most in regions the rate of 4-year college degree completion (level 6+) was in the mid-high 20s, with students from the London region having the highest rate at 36%.\n\n\n::: {.cell}\n\n:::\n\n\n*created and posted April 21, 2024*\n\n## Prompt #2 - Neo {#prompt2}\n\nThe way I chose to interpret \"neo\" was by using births by country, or new people by country. The data come from [EuroStat](https://ec.europa.eu/eurostat), via the [`eurostat`](https://github.com/rOpenGov/eurostat) package. I'll use total live births and births per 1000 people for the year 2022 (most recent year available). I'll use [`patchwork`](https://patchwork.data-imaginist.com/) to combine the plots. Unlike prompt 1, this only took a little more than an hour to do.\n\nGetting the data is super simple, and it doesn't need much cleaning, so I'll do all the code in one chunk.\n\n\n::: {.cell ECHO='true'}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for getting data and making the plots\"}\n# to know the table code \"tps00204\", I had to look up the dataset on-line and copy that value.\n# there is a function in the package to list all available data.\nbirths <- eurostat::get_eurostat(\"tps00204\", type = \"label\", time_format = \"num\") %>%\n\tselect(-freq) %>%\n\trename(year = TIME_PERIOD)\n#> \nindexed 0B in  0s, 0B/s\nindexed 1.00TB in  0s, 2.91PB/s\n                                                                              \n\n# make the plots\ntotal_births <-\nbirths %>%\n\tfilter(year == 2022) %>%\n\tfilter(indic_de == \"Live births - number\") %>%\n\tfilter(!grepl(\"Euro\",geo)) %>%\n\tarrange(desc(values), geo) %>%\n\tmutate(geo = forcats::fct_inorder(geo)) %>%\n\t{. ->> tmp} %>%\n\tggplot(aes(values, fct_rev(geo))) +\n\tgeom_bar(stat = \"identity\", fill = \"#FFCC00\") +\n\tscale_y_discrete(position = \"right\") +\n\t# need to subset data to get highest label to align similar to the rest\n\tgeom_text(data = subset(tmp, values <800000),\n\t\taes(label = format(values, big.mark=\",\")),\n\t\t\t\t\t\tcolor= \"#003399\", size = 4,\n\t\t\t\t\t\thjust = -.25, nudge_x = .25) +\n\tgeom_text(data = subset(tmp, values >800000),\n\t\t\t\t\t\taes(label = format(values, big.mark = \",\")),\n\t\t\t\t\t\tcolor= \"#003399\", size = 4,\n\t\t\t\t\t\thjust = 1, nudge_x = .5) +\n\tlabs(y = \"\", x = \"Total Live Births - 2022\") +\n\ttheme_minimal() +\n\ttheme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\nbirths_per1k <-\nbirths %>%\n\tfilter(year == 2022) %>%\n\tfilter(indic_de == \"Crude birth rate - per thousand persons\") %>%\n\tfilter(!grepl(\"Euro\",geo)) %>%\n\tarrange(desc(values), geo) %>%\n\tmutate(geo = forcats::fct_inorder(geo)) %>%\n\tggplot(aes(values, fct_rev(geo))) +\n\tgeom_bar(stat = \"identity\", fill = \"#003399\") +\n\tgeom_text(aes(label = values),\n\t\t\t\t\t\tposition = position_stack(vjust = 0.5),\n\t\t\t\t\t\tcolor= \"#FFCC00\", vjust = 0.5, size = 4) +\n\tlabs(y = \"\", x = \"Births per 1000 persons - 2022\") +\n\ttheme_minimal() +\n\ttheme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n# put them together with patchwork\nlibrary(patchwork)\ntotal_births + births_per1k + plot_annotation(\n\ttitle = 'Turkey tops in both total births and births / 1000 people',\n\tsubtitle = 'France only other country top 10 in both measures. San Marino bottom in both',\n\tcaption = 'Data from EuroStats using eurostat package')\n```\n\n::: {.cell-output-display}\n![](images/prompt2-1.png){width=100%}\n:::\n:::\n\n\nThere you have it. Turkey is tops in both total number of births and births per 1000 people. France is the only other country in the top 10. Interestingly, Spain is near the top in total number (it's a big country) but towards the bottom in births per 1000. San Marino is last in both total births and rate.\n\nWhat other interesting insights do you see here?\n\n\n::: {.cell}\n\n:::\n\n\n*created and posted April 22, 2024*\n\n## Prompts #3 & 4 - Redo & Waffle {#prompts3and4}\n\nFor this prompt I fulfilled prompt 3 \"redo\" by redoing the chart for prompt 1, and made it a waffle chart to fulfill prompt 4. I'd never done a waffle chart before, so new chart type...hooray! Cheers to Yann Holtz for his [code-through](https://r-graph-gallery.com/web-waffle-for-time-evolution.html) on his \"R Graph Gallery\" website to help me better understand how to set-up the data and plot it.\n\nI'm once again using educational attainment data from the UK, via a Tidy Tuesday set from January 2024. We'll get the data and make the chart in the same code chunk.\n\n\n::: {.cell ECHO='true'}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for getting data and making the plots\"}\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(tidytuesdayR) #to get data from tidy tuesday repo\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n## ggplot helpers - load if necessary\nlibrary(waffle) # ggplot helper to make a waffle chart\nlibrary(ggtext) # helper functions for ggplot text\n\n# load tidy tuesday data\nukeduc1 <- tt_load(\"2024-01-23\")\n#> \n#> \tDownloading file 1 of 1: `english_education.csv`\n\n# get variable names\nukeduc_names <- as_tibble(names(ukeduc1$english_education))\n\n# create tibble from csv, clean data\nukeduc <- as_tibble(ukeduc1$english_education) %>%\n\tmutate(town11nm = ifelse(town11nm == \"Outer london BUAs\", \"Outer London BUAs\", town11nm)) %>%\n\tmutate(\n\t\tttwa_classification = ifelse(town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \"Majority conurbation\", ttwa_classification)) %>%\n\tmutate(ttwa11nm = ifelse(town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"),\n\t\t\t\t\t\t\t\t\t\t\t\t\t \"London\", ttwa11nm)) %>%\n\tmutate(ttwa11cd = ifelse(town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"),\n\t\t\t\t\t\t\t\t\t\t\t\t\t \"E30000234\", ttwa11cd)) %>%\n\tmutate(across(26:29, ~ifelse(is.na(.),0,.))) %>%\n\tmutate(level_sum = rowSums(.[c(26:29)])) %>%\n\tmutate(highest_level_qualification_achieved_by_age_22_na = 100 - level_sum) %>%\n\tmutate(n_lesslev1_age22 =\n\t\t\t\t \tround(highest_level_qualification_achieved_by_age_22_less_than_level_1 * (ks4_2012_2013_counts/100) ,0)) %>%\n\tmutate(n_lev1to2_age22 =\n\t\t\t\t \tround(highest_level_qualification_achieved_by_age_22_level_1_to_level_2 * (ks4_2012_2013_counts/100) ,0)) %>%\n\tmutate(n_lev3to5_age22 =\n\t\t\t\t \tround(highest_level_qualification_achieved_by_age_22_level_3_to_level_5 * (ks4_2012_2013_counts/100) ,0)) %>%\n\tmutate(n_lev6plus_age22 =\n\t\t\t\t \tround(highest_level_qualification_achieved_by_age_22_level_6_or_above * (ks4_2012_2013_counts/100) ,0)) %>%\n\tmutate(n_lev_na_age22 =\n\t\t\t\t \tround(highest_level_qualification_achieved_by_age_22_na * (ks4_2012_2013_counts/100) ,0))\n\nglimpse(ukeduc)\n#> Rows: 1,104\n#> Columns: 38\n#> $ town11cd                                                          <chr> \"E34…\n#> $ town11nm                                                          <chr> \"Car…\n#> $ population_2011                                                   <dbl> 5456…\n#> $ size_flag                                                         <chr> \"Sma…\n#> $ rgn11nm                                                           <chr> \"Eas…\n#> $ coastal                                                           <chr> \"Non…\n#> $ coastal_detailed                                                  <chr> \"Sma…\n#> $ ttwa11cd                                                          <chr> \"E30…\n#> $ ttwa11nm                                                          <chr> \"Wor…\n#> $ ttwa_classification                                               <chr> \"Maj…\n#> $ job_density_flag                                                  <chr> \"Res…\n#> $ income_flag                                                       <chr> \"Hig…\n#> $ university_flag                                                   <chr> \"No …\n#> $ level4qual_residents35_64_2011                                    <chr> \"Low…\n#> $ ks4_2012_2013_counts                                              <dbl> 65, …\n#> $ key_stage_2_attainment_school_year_2007_to_2008                   <dbl> 65.0…\n#> $ key_stage_4_attainment_school_year_2012_to_2013                   <dbl> 70.7…\n#> $ level_2_at_age_18                                                 <dbl> 72.3…\n#> $ level_3_at_age_18                                                 <dbl> 50.7…\n#> $ activity_at_age_19_full_time_higher_education                     <dbl> 30.7…\n#> $ activity_at_age_19_sustained_further_education                    <dbl> 21.5…\n#> $ activity_at_age_19_appprenticeships                               <dbl> NA, …\n#> $ activity_at_age_19_employment_with_earnings_above_0               <dbl> 52.3…\n#> $ activity_at_age_19_employment_with_earnings_above_10_000          <dbl> 36.9…\n#> $ activity_at_age_19_out_of_work                                    <dbl> NA, …\n#> $ highest_level_qualification_achieved_by_age_22_less_than_level_1  <dbl> 0.0,…\n#> $ highest_level_qualification_achieved_by_age_22_level_1_to_level_2 <dbl> 34.9…\n#> $ highest_level_qualification_achieved_by_age_22_level_3_to_level_5 <dbl> 39.7…\n#> $ highest_level_qualification_achieved_by_age_22_level_6_or_above   <dbl> 0.0,…\n#> $ highest_level_qualification_achieved_b_age_22_average_score       <dbl> 3.32…\n#> $ education_score                                                   <dbl> -0.5…\n#> $ level_sum                                                         <dbl> 74.6…\n#> $ highest_level_qualification_achieved_by_age_22_na                 <dbl> 25.4…\n#> $ n_lesslev1_age22                                                  <dbl> 0, 0…\n#> $ n_lev1to2_age22                                                   <dbl> 23, …\n#> $ n_lev3to5_age22                                                   <dbl> 26, …\n#> $ n_lev6plus_age22                                                  <dbl> 0, 8…\n#> $ n_lev_na_age22                                                    <dbl> 17, …\n\n# cohort ed attainment by region\nukeduc2 <-\n\tukeduc %>%\n\trename(region =rgn11nm ) %>%\n\tgroup_by(region) %>%\n\tsummarise(region_cohort = sum(ks4_2012_2013_counts),\n\t\t\t\t\t\tregion_levless1_age22 = sum(n_lesslev1_age22),\n\t\t\t\t\t\tregion_lev1to2_age22 = sum(n_lev1to2_age22),\n\t\t\t\t\t\tregion_lev3to5_age22 = sum(n_lev3to5_age22),\n\t\t\t\t\t\tregion_lev6plus_age22 = sum(n_lev6plus_age22)) %>%\n\tpivot_longer(cols = ends_with(\"age22\"), names_to = \"ed_attain_age22\", values_to = \"ed_attain_n\") %>%\n\tmutate(ed_attain_pct = ed_attain_n / region_cohort) %>%\n\tmutate(ed_attain_pct2 = round(ed_attain_pct*100)) %>%\n\tmutate(region = factor(region))%>%\n\tfilter(!is.na(region)) %>%\n\tselect(region, region_cohort, ed_attain_age22, ed_attain_pct2) %>%\n\t## the data was a bit messy, so to get the values to sum to 100 to make the waffle\n\t## chart fill the plot area, I had to do a little reshaping after doing the percentages.\n\tpivot_wider(names_from = ed_attain_age22, values_from = ed_attain_pct2) %>%\n\tmutate(region_sum = rowSums(.[c(3:6)])) %>%\n\tmutate(region_lev_na_age22 = ifelse(region_sum < 100, 100 - region_sum, 0)) %>%\n\tselect(-region_sum) %>%\n\tpivot_longer(cols = ends_with(\"age22\"), names_to = \"ed_attain_age22\", values_to = \"ed_attain_pct\") %>%\n\tmutate(ed_attain_age22 =\n\t\t\t \tfactor(ed_attain_age22,\n\t\t\t \t\t\t\t levels = c(\"region_lev_na_age22\", \"region_levless1_age22\", \"region_lev1to2_age22\",\n\t\t\t \t\t\t\t \t\t\t\t\t \"region_lev3to5_age22\", \"region_lev6plus_age22\"),\n\t\t\t \t\t\t\t labels = c(\"No data\", \"Level <1\", \"Level = 1-2\",\n\t\t\t \t\t\t\t \t\t\t\t\t \"Level = 3-5\", \"Level = 6+\")))\nglimpse(ukeduc2)\n#> Rows: 45\n#> Columns: 4\n#> $ region          <fct> East Midlands, East Midlands, East Midlands, East Midl…\n#> $ region_cohort   <dbl> 39811, 39811, 39811, 39811, 39811, 48849, 48849, 48849…\n#> $ ed_attain_age22 <fct> Level <1, Level = 1-2, Level = 3-5, Level = 6+, No dat…\n#> $ ed_attain_pct   <dbl> 2, 30, 41, 25, 2, 2, 27, 43, 27, 1, 2, 19, 43, 36, 0, …\n\n## chart\nukeduc2\t%>%\n\tggplot(aes(fill = ed_attain_age22, values = ed_attain_pct)) +\n\tgeom_waffle(na.rm=TRUE, n_rows=10, flip=TRUE, size = 0.33, colour = \"white\") +\n\tfacet_wrap(~region, nrow=1,strip.position = \"bottom\") +\n\tscale_x_discrete() +\n\tscale_y_continuous(labels = function(x) x * 10, # make this multiplyer the same as n_rows\n\t\t\t\t\t\t\t\t\t\t expand = c(0,0)) +\n\tscale_fill_brewer(palette = \"Set2\") +\n\t\tlabs(title = \"Students in London most likely to have at least 4-year degree by Age 22\",\n\t\t\t\t subtitle = \"Sixth Year Educational Outcomes for Level 4 2012-13 Cohort by UK Region<br>\n\t\t\t\t Each block = 1 %\",\n\t\t\t\t caption = \"*Tidy Tuesday data 01/23/2024, from UK Office of National Statistics*\",\n\t\t\t\t x = \"\", y = \"\") +\n\ttheme_minimal() +\n\t\ttheme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'),\n\t\t\t\t\tlegend.key.width = unit(1.5, 'cm'), legend.margin=margin(-10, 0, 0, 0),\n\t\t\t\t\tplot.title = element_text(hjust = 0), plot.subtitle = element_markdown(),\n\t\t\t\t\tplot.caption = element_markdown(),\n\t\t\t\t\tpanel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n\tguides(fill = guide_legend(label.position = \"bottom\", \n\t\t\t\t\t\t\t\t\t\t\ttitle = \"Cohort at Age 22\", \n\t\t\t\t\t\t\t\t\t\t\ttitle.position = \"top\"))\n```\n\n::: {.cell-output-display}\n![](images/prompt3&4-1.png){width=100%}\n:::\n:::\n\n\nSo alright, a waffle chart. Similar insights from the horizontal bar, just visualized a bit differently.\n\n*created and posted April 23, 2024*\n\n## Prompt #5 - Diverging {#prompt5}\n\nStaying with the theme of educational attainment, but switching countries to Denmark (where I was born and now live again), let's look at differences in educational attainment for people in Denmark aged 25-69. The data come from Danmarks Statistik (Statistics Danmark, the national statistics agency) via the [`danstat`](https://github.com/ValeriVoev/danstat) package. I've been wanting to use the package, this was a great excuse.\n\nI won't do too much explaining about education levels in Denmark, you can read up on them on the [ministry's page](https://ufm.dk/en/education/the-danish-education-system).\n\nThe `danstat` package is fairly easy to use once you get the hang of sorting out the table name and number, and variable names and values you need to filter on. It's a good idea to start at Danmarks Statistik's [StatBank page](https://www.statbank.dk/statbank5a/default.asp?w=1680), search the data you want, and when you find the table, you'll see the code, in this case HFUDD11, and use that in the package calls. To see what I mean, let's start with getting the data.\n\n\n::: {.cell ECHO='true'}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for getting data\"}\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(danstat) # package to get Danish statistics via api\nlibrary(ggtext) # enhancements for text in ggplot\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n\n# metadata for table variables, click thru nested tables to find variables and ids for filters\ntable_meta <- danstat::get_table_metadata(table_id = \"hfudd11\", variables_only = TRUE)\n\n# create variable list using the ID value in the variable\nvariables_ed <- list(\n\tlist(code = \"bopomr\", values = \"000\"),\n\tlist(code = \"hfudd\", values = c(\"H10\", \"H20\", \"H30\", \"H35\",\n\t\t\t\t\t\t\t\t\t\t\"H40\", \"H50\", \"H60\", \"H70\", \"H80\", \"H90\")),\n\tlist(code = \"køn\", values = c(\"M\",\"K\")),\n\tlist(code = \"alder\", values = c(\"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\n\t\t\t\t\t\t\t\t\t\t\t\t\"50-54\", \"55-59\", \"60-64\", \"65-69\")),\n\tlist(code = \"tid\", values = 2023))\n\n# past variable list along with table name. \n# note that in package, table name is lower case, though upper case on statbank page.\nedattain <- get_data(\"hfudd11\", variables_ed, language = \"da\") %>%\n\tas_tibble() %>%\n\tselect(sex = KØN, age = ALDER, edlevel = HFUDD, n = INDHOLD)\n```\n:::\n\n\nThe data I pulled is by sex, age, and education level. I needed to age variable to filter for age 25+, as the set starts at age 15. At some point for another post I plan to look a bit more deeply at educational attainment here, including region and age and other fields. So let's clean the data and get it ready to plot a diverging bar chart.\n\nEven though I've calculated percentages for sex by age by education level, I won't be doing any breakdowns by age here, again, that's a later independent post. But at least I have the code now.\n\n\n::: {.cell ECHO='true'}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for cleaning data\"}\nedattain1 <- edattain %>%\n\t## create factors from the education levels\n\tmutate(edlevel =\n\t\tfactor(edlevel,\n\t\t\tlevels = c(\"H10 Grundskole\", \"H20 Gymnasiale uddannelser\",\n\t\t\t\"H30 Erhvervsfaglige uddannelser\", \"H35 Adgangsgivende uddannelsesforløb\",\n\t\t\t\"H40 Korte videregående uddannelser, KVU\", \"H50 Mellemlange videregående uddannelser, MVU\",\n\t\t\t\"H60 Bacheloruddannelser, BACH\", \"H70 Lange videregående uddannelser, LVU\",\n\t\t\t\"H80 Ph.d. og forskeruddannelser\", \"H90 Uoplyst mv.\"),\n\t\t\tlabels = c(\"Grundskole/Primary\", \"Gymnasium\",\n\t\t\t\"Erhvervsfaglige/Vocational HS\", \"Adgangsgivende/Qualifying\",\n\t\t\t\"KVU/2-year college\", \"MVU/Professional BA\",\n\t\t\t\"Bachelor\", \"LVU/Masters\", \"PhD\", \"Not stated\" ))) %>%\n\tmutate(sex = ifelse(sex == \"Kvinder\", \"Kvinder/Women\", \"Mænd/Men\")) %>%\n\t# calculate total number by sex\n\tarrange(sex, edlevel) %>%\n\tgroup_by(sex) %>%\n\tmutate(tot_sex = sum(n)) %>%\n\tungroup() %>%\n\t# calculate total number by sex and ed level\n\tgroup_by(sex, edlevel) %>%\n\tmutate(tot_sex_edlev = sum(n)) %>%\n\tungroup() %>%\n\t# calculate total number by sex and age\n\tgroup_by(sex, age) %>%\n\tmutate(tot_sex_age = sum(n)) %>%\n\tungroup() %>%\n\t# calculate percentages \n\tmutate(level_pct = round(tot_sex_edlev / tot_sex, 3)) %>%\n\tmutate(level_pct = ifelse(sex == \"Mænd/Men\", level_pct *-1, level_pct)) %>%\n\tmutate(level_pct2 = round(level_pct * 100, 1)) %>%\n\tmutate(age_level_pct = round(n / tot_sex_age, 3)) %>%\n\tmutate(age_level_pct = ifelse(sex == \"Mænd/Men\", age_level_pct *-1, age_level_pct)) %>%\n\tmutate(age_level_pct2 = round(age_level_pct * 100, 1))\n```\n:::\n\n\nNow let's make the plot.\n\n\n::: {.cell ECHO='true'}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for making the plot\"}\nvlines_df <- data.frame(xintercept = seq(-100, 100, 20))\n\nedattain1 %>%\n\tfilter(!edlevel == \"Not stated\") %>%\n\tdistinct(sex, edlevel, .keep_all = TRUE) %>%\n\tselect(sex, edlevel:tot_sex, level_pct, level_pct2 ) %>%\n\t# pass this temporary set thru to the ggplot call\n\t{. ->> tmp} %>%\n\tggplot() +\n\tgeom_col(aes(x = -50, y = edlevel), width = 0.75, fill = \"#e0e0e0\") +\n\tgeom_col(aes(x = 50, y = edlevel), width = 0.75, fill = \"#e0e0e0\") +\n\tgeom_col(aes(x = level_pct2, y = edlevel, fill = sex, color = sex), width = 0.75) +\n\tscale_x_continuous(labels = function(x) abs(x), breaks = seq(-100, 100, 20)) +\n\tgeom_vline(data = vlines_df, aes(xintercept = xintercept), color = \"#FFFFFF\", size = 0.1, alpha = 0.5) +\n\tcoord_cartesian(clip = \"off\") +\n\tscale_fill_manual(values = c(\"#C8102E\", \"#FFFFFF\")) +\n\tscale_color_manual(values = c(\"#C8102E\", \"#C8102E\")) +\n\tgeom_text(data = subset(tmp, sex == \"Mænd/Men\"),\n\t\t\t\taes(x = level_pct2, y = edlevel, label = paste0(abs(level_pct2), \"%\")),\n\t\t\t\tsize = 5, color = \"#C8102E\",\n\t\t\t\thjust = 1, nudge_x = -.5) +\n\tgeom_text(data = subset(tmp, sex == \"Kvinder/Women\"),\n\t\t\t\taes(x = level_pct2, y = edlevel, label = paste0(abs(level_pct2), \"%\")),\n\t\t\t\tsize = 5, color = \"#C8102E\",\n\t\t\t\thjust = -.25) +\n\tlabs(x = \"\", y = \"\",\n\t\t\t title = \"In Denmark, <span style = 'color: #C8102E;'>women</span> more likely than men for highest education\n\t\t\t level to be <br>Professional BA (MVU) or Masters.<br><br>Men more likely to stop at primary level or vocational secondary diploma.\",\n\t\t\t subtitle = \"<br>*Highest level of education attained by all people in Denmark aged 25-69 as of Sept 30 2023.\n\t\t\t <br><span style = 'color: #C8102E;'>Women are red bars</span>, men are white.*\",\n\t\t\t caption = \"*Data from Danmarks Statistik via danstat package*\") +\n\ttheme_minimal() +\n\ttheme(panel.grid = element_blank(), plot.title = element_markdown(),\n\t\t\t\tplot.subtitle = element_markdown(), plot.caption = element_markdown(),\n\t\t\t\tlegend.position = \"none\",\n\t\t\t\taxis.text.y = element_text(size = 10))\n```\n\n::: {.cell-output-display}\n![](images/prompt5_3-1.png){width=100%}\n:::\n:::\n\n\nThere it is, a simple, clean chart showing how highest educational attainment differs for women and men in Denmark. I'm definitely interested in exploring differences by age group and by region.\n\n\n::: {.cell}\n\n:::\n\n\n*created and posted April 24, 2024*\n\n## Prompt #6 - OECD {#prompt6}\n\nStaying with the emerging theme of educational attainment, this prompt was a good excuse to try out the [`oecd` package](https://github.com/expersso/OECD). The package wraps functions to access OECD data through their API for their [Data Explorer platform](https://data-explorer.oecd.org/).\n\nIt's fairly straight forward to use...look up the table you want, set the filters, copy the API script and parse out as the package documentation shows. In the example they do not show it, but make sure if you set time parameters that you put it in the `start_date` and `end_date` calls. You can of course name the dataset and filters anything you want. I chose my names so they wouldn't conflict with function calls.\n\nFor this chart I looked at educational attainment in Nordic countries; Denmark, Finland, Iceland, Norway, & Sweden *(i)*, by sex for all people aged 25 to 64 in 2022. For the sake of expediency, I'll copy the overall format & theme from prompt 1.\n\n*i) let's not get into here the difference between Nordic & Scandinavian, what constitutes Scandinavian & why...*\n\nThe code for getting the data and making the chart is below.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for prompt 6\"}\n## code for 30 Day Chart Challenge 2024, day 6 OECD\n## educational attainment via OECD package\n## nordics, by sex 25 to 64, 2020-2022\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(OECD) # package to get OECD data via api\nlibrary(ggtext) # enhancements for text in ggplot\n\n# table from website\n# https://www.oecd-ilibrary.org/education/data/education-at-a-glance/educational-attainment-and-labour-force-status_889e8641-en\n\nedattdata <- \"OECD.CFE.EDS,DSD_REG_EDU@DF_ATTAIN,1.0\"\nedattfilter <- \"A.CTRY.SWE+NOR+ISL+FIN+DNK...Y25T64.F+M.ISCED11_0T2+ISCED11_5T8+ISCED11_3_4..\"\n\noecd1 <- get_dataset(edattdata, edattfilter, start_time = 2020, end_time = 2022)\n\n# get metadata - helpful to create labels for factors.\n# see package documentation on github \n# commented out here to speed up rendering of post html file...this takes a while to download\n   # oecd_metadata <- get_data_structure(edattdata)\n\n# gets labels for country codes...this step commented out, data loading from local file.\n# datalabs_ct <- oecd_metadata$CL_REGIONAL %>%\n# \trename(COUNTRY = id)\n\ndatalabs_ct <- readRDS(\"~/Data/r/30 Day Chart Challenge/2024/data/oecd_datalabs_ct.rds\") \n\n# keep only necessary vars, change some to factors & add labels\noecd <- oecd1 %>%\n\tmutate(EDUCATION_LEV =\n\t\tfactor(EDUCATION_LEV,\n\t\t\tlevels = c(\"ISCED11_0T2\", \"ISCED11_3_4\", \"ISCED11_5T8\"),\n\t\t\tlabels = c(\"Pre-primary thru lower secondary\",\n\t\t\t\t \t\t\"Upper secondary and non-degree tertiary\", \"Tertiary education\"))) %>%\n\tmutate(SEX = case_when(SEX == \"M\" ~ \"Male\", SEX == \"F\" ~ \"Female\")) %>%\n\tmutate(pct_attain2 = as.numeric(ObsValue)) %>%\n\tmutate(pct_attain = pct_attain2 / 100) %>%\n\t# join to have labels instead of country abbrvs\n\tleft_join(datalabs_ct) %>%\n\tselect(country = label,  year = TIME_PERIOD, SEX, EDUCATION_LEV, pct_attain, pct_attain2) %>%\n\tclean_names()\n\n## chart...horizontal stacked bar\noecd %>%\n\tfilter(year == \"2022\") %>%\n\tggplot(aes(pct_attain, fct_rev(country), fill = fct_rev(education_lev))) +\n\tgeom_bar(stat = \"identity\") +\n\tscale_x_continuous(expand = c(0,0),\n\t\t\t\tbreaks = c(.01, 0.25, 0.50, 0.75, .97),\n\t\t\t\tlabels = c(\"0\", \"25%\", \"50%\", \"75%\", \"100%\")) +\n\tfacet_wrap(~ sex, nrow = 1) +\n\tgeom_text(aes(label = scales::percent(round(pct_attain , 2))),\n\t\t\t\tposition = position_stack(vjust = 0.5),\n\t\t\t\tcolor= \"white\", vjust = 0.5, size = 5) +\n\tlabs(title = \"In Nordic countries, women age 25-64 more likely than men to complete college.<br><br>\n\t\t\t Finns have lowest levels of attainment stopping at lower secondary\",\n\t\t\t subtitle = \"<br>*Educational Attainment in Nordic Countries, by Sex, ages 25-64 combined, 2022*\",\n\t\t\t caption = \"*Data from OECD, via oecd package for r*\",\n\t\t\t x = \"\", y = \"\") +\n\tscale_fill_brewer(palette = \"Set2\") +\n\ttheme_minimal() +\n\ttheme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'),\n\t\t\t\tlegend.key.width = unit(1.5, 'cm'), legend.margin=margin(-10, 0, 0, 0),\n\t\t\t\tplot.title = element_markdown(), plot.subtitle = element_markdown(),\n\t\t\t\tplot.caption = element_markdown(),\n\t\t\t\taxis.text.y = element_text(size = 12),\n\t\t\t\tpanel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n\tguides(fill = guide_legend(label.position = \"bottom\", reverse = TRUE,\n\t\t\t\ttitle = \"Education Levels\", title.position = \"top\"))\n```\n\n::: {.cell-output-display}\n![](images/prompt6-1.png){width=100%}\n:::\n:::\n\n\nYou can see that there are some differences in levels of attainment between countries. I don't know enough context to speculate as to why. Also, as you add more countries to the filter, the education levels and age groups are aggregated into bigger buckets, presumably to facilitate comparisons. Denmark has a good national database where you can drill down more by age and level, so if I wanted to look at differences by smaller age groups and over time, I'd have to dig into the individual countries national statistics databanks and would have to hope that the age groups and attainment levels are similar enough.\n\n*created and posted April 26, 2024*\n\n## Prompt #7 - Hazards {#prompt7}\n\nHazards is vague enough to mean anything, and if you search the socials for maps submitted for this prompt you'll see a variety of approaches. Right away I thought of crime stats, and also right away I knew I wanted to make a map of crime data in Denmark.\n\nSo that's what I set out to do.\n\nBut very soon into building it I realized I wanted to visualise different types of crime, which meant multiple plots. So then I got it in my head that this should be an exercise in really getting comfortable with functional programming and iterating using `map()` or a similar approach.\n\nSo that's what I did.\n\nFollow along now as we get data, build a plot function, map over it, and make some maps.\n\nFirst, the data. As with the [diverging challenge](https://www.gregdubrow.io/posts/30-day-chart-challenge-2024/#prompt5) I got the data from [Danmarks Statistik](https://www.dst.dk/en) (Statistics Danmark, the national statistics agency). Unlike the diverging challenge, I did not end up using the [`danstat`](https://github.com/ValeriVoev/danstat) package.\n\nWhy? Well I wanted to use the [province aggregation](https://en.wikipedia.org/wiki/Provinces_of_Denmark), the middle level between towns and regions. I wanted to see a bit of nuance in the maps. The problem is, while province data can be pulled from the [StatBank data portal](https://www.statbank.dk/) the [API](https://www.dst.dk/en/Statistik/brug-statistikken/muligheder-i-statistikbanken/api#) only has cities, regions, and all of Denmark. So loading flat-files it is.\n\nBut In this first chunk we get the geographic boundary data. For this we'll be using NUTS3 level. What are the NUTS levels? (settle down, Beavis) Well, per the [European Commission and Eurostat](https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Glossary:Nomenclature_of_territorial_units_for_statistics_(NUTS)) they are:\n\n> \"The Nomenclature of territorial units for statistics, abbreviated NUTS (from the French version Nomenclature des Unités territoriales statistiques) is a geographical nomenclature subdividing the economic territory of the European Union (EU) into regions at three different levels (NUTS 1, 2 and 3 respectively, moving from larger to smaller territorial units).\n\nIn Denmark NUTS1 is the entire country, NUTS2 are the major regions, and NUTS3 are the provinces. The process for doing this comes from one of the excellent tutorials by [Milos Popvic](https://github.com/milos-agathon/how-i-make-eurostat-maps/blob/main/R/main.r) in his *Milos Makes Maps* series of [video](https://www.youtube.com/@milos-makes-maps/videos) and code how-tos.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for getting map boundary data\"}\n# load all the packages we'll need\nlibrary(tidyverse) # to do tidyverse things\nlibrary(sf) # to make our geo data mappable\nlibrary(giscoR) # gets the region level boundries\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(danstat) # package to get Danish statistics via api\nlibrary(ggtext) # enhancements for text in ggplot\nlibrary(patchwork) # puts plots together\n\nsource(\"~/Data/r/basic functions.R\")\n\n### Get mapping data\n# define longlat projection\ncrsLONGLAT <- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n\n# get NUTS3 data for Denmark and use sf to make it plottable\nnuts3_dk <- giscoR::gisco_get_nuts(\n\tyear = \"2021\",\tresolution = \"3\",\n\tnuts_level = \"3\", country = \"DK\") |>\n\trename(province_name = NAME_LATN) |>\n\tsf::st_transform(crsLONGLAT)\n```\n:::\n\n\nNow we add the crime and population data via spreadsheet import, and join it to the map data. The crime data needs a bit of tidying up, shortening the names, some other text edits. We'll normalize crime data by incidents per 100,000 people. Because the crime data come by yearly quarters and I wanted one entire year (adding up the four quarters), I pulled population data for the start of the 3rd quarter of 2023. I figured that was a good representative time-point for the entire year.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for getting crime and population data\"}\ncrime_2023_1 <- readxl::read_excel(\"~/Data/r/30 Day Chart Challenge/2024/data/dk_crime_by_province_2023.xlsx\") %>%\n\tclean_names() %>%\n\tfill(offence_cat_code) %>%\n\tfill(offence_cat_name) %>%\n\trename_with(~ sub(\"^x\", \"tot_\", .x), starts_with((\"x\"))) %>%\n\tmutate(tot_2023 = rowSums(.[c(5:8)])) %>%\n\tmutate(province_name = str_replace(province_name, \"Province \", \"\")) %>%\n\tmutate(offence_cat_name = str_replace(offence_cat_name, \", total\", \"\")) %>%\n\tmutate(offence_cat_name = str_replace(\n\t\toffence_cat_name, \"Nature of the offence\", \"All offences\")) %>%\n\tmutate(offence_cat_name = str_replace(\n\t\toffence_cat_name, \"Offences against property\", \"Property crime\")) %>%\n\tmutate(offence_cat_name = str_replace(\n\t\toffence_cat_name, \"Crimes of violence\", \"Violent crime\")) %>%\n\tmutate(offence_cat_name =\n\t\t\tfactor(offence_cat_name,\n\t\t\tlevels = c(\"All offences\", \"Criminal code\", \"Sexual offenses\", \"Violent crime\", \"Property crime\",\n\t\t\t\t \t\t\"Other offences\", \"Special acts\"))) %>%\n\tselect(province_name, offence_cat_name, tot_2023)\n\n## get population data to normalize \n# based on total at start of 2023 Q3\npop_2023 <- readxl::read_excel(\"~/Data/r/30 Day Chart Challenge/2024/data/dk_pop_2023_q3.xlsx\") %>%\n\tclean_names() %>%\n\tmutate(province_name = str_replace(province_name, \"Province \", \"\"))\n\n# join crime & population\ncrime_2023_2 <- crime_2023_1 %>%\n\tleft_join(pop_2023) %>%\n\tmutate(crime_per1k = round(tot_2023 / tot_pop * 100000, 0))\n\n## left_join nuts sf object to crime data to get sf object for plot\ncrime_2023 <- nuts3_dk %>%\n\tleft_join(crime_2023_2, by = \"province_name\")\n```\n:::\n\n\nNow that we have a dataset ready to be mapped, it's time to build the plot function. Being fully honest, functional programming and iterating over objects have long been my `r` bugaboos. I have no idea why, but I just couldn't internalize the logic. But putting together the [bicycle rides post](https://www.gregdubrow.io/posts/my-year-of-riding-danishly/) where I employed [Cedric Scherer's tutorial](https://www.cedricscherer.com/2023/07/05/efficiency-and-consistency-automate-subset-graphics-with-ggplot2-and-purrr/) I started to get it.\n\nIt took much longer to trial-and-error a few different approaches and get the text and layout as I wanted than it would have to copy-paste-adjust the plot code seven times and stitch together with `patchwork`, but now I have a map function I can re-use. A small but helpful part of the function is the [`ggsflabel`](https://yutannihilation.github.io/ggsflabel/) package to help offset labels for Copenhagen (Byen København & Københavns omgen).\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for the map function\"}\ndk_crime_map <- function(offence, maptitle) {\n\tg <-\n\t\tggplot() +\n\t\tgeom_sf(data = (crime_2023 %>% filter(offence_cat_name== offence)),\n\t\t\t\t\t\taes(fill = crime_per1k), color = \"#FFFFFF\", size = 3) +\n\t\tgeom_sf_text(data = (crime_2023 %>%\n\t\t\t\t\tfilter(province_name %notin% c(\"Byen København\", \"Københavns omegn\")) %>%\n\t\t\t\t\tfilter(offence_cat_name == \"Special acts\")),\n\t\t\t\t\t\taes(label = province_name), nudge_x = -.5, size = 2)\t+\n\t\tggsflabel::geom_sf_label_repel(data = (crime_2023 %>%\n\t\t\t\t\tfilter(province_name %in% c(\"Byen København\", \"Københavns omegn\")) %>%\n\t\t\t\t\tfilter(offence_cat_name == offence)),\n\t\t\t\t\t\taes(label = province_name), size = 1.5,\n\t\t\t\t\t\tforce = 1, nudge_x = 4, nudge_y = .75) +\n\t\tscale_fill_gradient(trans = \"reverse\") +\n\t\tlabs(x = \"\", y = \"\") +\n\t\ttheme_minimal() +\n\t\tggtitle(maptitle) +\n\t\ttheme(panel.grid = element_blank(),\n\t\t\t\tplot.title = element_text(size = 11, hjust = .6, vjust = -7),\n\t\t\t\taxis.line = element_blank(), axis.ticks = element_blank(),\n\t\t\t\taxis.text.x = element_blank(), axis.text.y = element_blank(),\n\t\t\t\tlegend.position = c(.4, -.2), \n\t\t\t\tlegend.title = element_text(size = 7),\n\t\t\t\tlegend.text = element_text(size = 7)\n\t\t\t\t\t) +\n\t\tguides(fill = guide_legend(\n\t\t\ttitle = \"Incidents per 100K people\",\n\t\t\tdirection = \"horizontal\",\n\t\t\tkeyheight = unit(1, units = \"mm\"),\n\t\t\tkeywidth = unit(10, units = \"mm\"),\n\t\t\ttitle.position = \"top\",\n\t\t\ttitle.hjust = .5,\n\t\t\tlabel.hjust = .5,\n\t\t\tnrow = 1,\n\t\t\tbyrow = T,\n\t\t\treverse = F,\n\t\t\tlabel.position = \"bottom\"\n\t\t))\n\n\treturn(g)\n}\n```\n:::\n\n\nThe function is ready, now we iterate over the seven crime categories and put the plots together in one image. To magnify it for better viewing, click on it to bring it up in a lightbox.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for generating the plots\"}\n## map over all crime categories\n# create list of crime types\ncrimecats <- unique(crime_2023_2$offence_cat_name)\n\n# create plots, stitch together with patchwork\nwrap_plots(\n\tmap(crimecats, ~dk_crime_map(offence = .x, maptitle = .x)),\n\twidths = 5, heights = 5) +\n\tplot_annotation(\n\t\ttitle = \"Crimes by Type and Province in Denmark, 2023\",\n\t\tsubtitle = \"*Total Crimes per 100K people*\",\n\t\tcaption = \"*Data from Danmarks Statistik. Criminal code = sexual offences + violence + property + other*\",\n\t\ttheme = theme(plot.subtitle = element_markdown(),\n\t\t\t\t\tplot.caption = element_markdown()))\n```\n\n::: {.cell-output-display}\n![](images/prompt_7_4-1.png){width=100%}\n:::\n:::\n\n\nIf this were for an official company or organization report or academic publication there are some tweaks I'd want to make...the color palette, maybe writing a function to make the breaks better, spacing of province labels, maybe making it interactive with tool tips that show province name along with other data. I might do an inset or separate plots for Copenhagen. But for the purpose of the chart challenge I'm happy with it because it meets my goal, which was to build a plot function and iterate it over a vector, rather than copy-paste and edit the plot code.\n\n*created and posted April 29, 2024*\n\n## Prompt #8 - Circular {#prompt8}\n\nBecause prompt 7 took so much time, I'm cheating a bit with this prompt, and reusing two plots I made for the [bicycle rides post](https://www.gregdubrow.io/posts/my-year-of-riding-danishly/).\n\nI wanted to visualise which hours and minutes I started my rides. The `coord_polar()` function turns a bar chart into a circle. Adding the right breaks and limits, and *voila*, first the hours of the day...\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for circular time plots - hour\"}\nstrava_data %>%\n\tfilter(activity_year == 2023) %>%\n\tcount(ride_type, activity_hour) %>%\n\t{. ->> tmp} %>%\n\tggplot(aes(activity_hour, y = n, fill = ride_type)) +\n\tgeom_bar(stat = \"identity\") +\n\tscale_x_continuous(limits = c(0, 24), breaks = seq(0, 24)) +\n\tgeom_text(data = subset(tmp, ride_type == \"Commute/Studieskolen\" & n > 20),\n\t\taes(label= n), color = \"white\", size = 4) +\n\tcoord_polar(start = 0) +\n\ttheme_minimal() +\n\tscale_fill_manual(values = c(\"#0072B2\", \"#E69F00\", \"#CC79A7\"),\n\t\t\t\t\t\t\t\t\t\tlabels = c(\"Commute/<br>Studieskolen\", \"Other\", \"Workout\")) +\n\tlabs(x = \"\", y = \"\",\n\t\t\t title = \"Most Rides During Morning and Evening Commuting Hours\",\n\t\t\t subtitle = \"*Numbers Correspond to Hour of Day on a 24 hr clock*\") +\n\ttheme(legend.text = element_markdown(),\n\t\t\t\taxis.text.y = element_blank(),\n\t\t\t\tlegend.title = element_blank(),\n\t\t\t\tplot.title = element_text(size = 10, hjust = 0.5),\n\t\t\t\tplot.subtitle = element_markdown(hjust = 0.5, size = 9))\nrm(tmp)\n```\n:::\n\n\n![](images/circular_hour.png){fig-alt=\"circular bar plot representing a clock\"}\n\n...and the minutes.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for circular time plots - minute\"}\nactivty_ampm %>%\n\tggplot(aes(activity_min, y = activity_min_n, fill = ampm)) +\n\tgeom_col(position = position_stack(reverse = TRUE)) +\n\tscale_x_continuous(limits = c(-1, 60), breaks = seq(0, 59), labels = seq(0, 59)) +\n\tgeom_text(data = subset(activty_ampm, activity_min_n > 5),\n\t\t\t\t\t\taes(label= activity_min_n), color = \"white\", size = 4, position = position_nudge(y = -1)) +\n\tcoord_polar(start = 0) +\n\ttheme_minimal() +\n\tscale_fill_manual(values = c(\"#E57A77\", \"#7CA1CC\"),\n\t\t\t\t\t\t\t\t\t\tlabels = c(\"AM\", \"PM\")) +\n\tlabs(x = \"\", y = \"\",\n\t\t\t title = \"Most Morning Rides Started Between 12 & 30 Past the Hour <br>\n\t\t\t Evening Rides More Evenly Spaced Through the Hour\",\n\t\t\t subtitle = \"*Numbers Correspond to  Minutes of the Hour*\") +\n\ttheme(legend.text = element_markdown(),\n\t\t\t\taxis.text.y = element_blank(),\n\t\t\t\tlegend.title = element_blank(),\n\t\t\t\tplot.title = element_markdown(size = 10, hjust = 0.5),\n\t\t\t\tplot.subtitle = element_markdown(hjust = 0.5, size = 9))\n\n```\n:::\n\n\n![](images/circular_minute.png){fig-alt=\"circular bar plot representing a clock\"}\n\nThey aren't perfect but they're close enough and visualised what I wanted them to show...what time of the day did I start my bike rides in 2023. For more detail and context, read the [bike rides post](https://www.gregdubrow.io/posts/my-year-of-riding-danishly/).\n\n*created and posted April 29, 2024*\n\n## Prompt #9 - Major/Minor {#prompt9}\n\nThe very first thing that came to mind for this plot prompt was to look at differences in music between major and minor keys. That provided me with the perfect excuse to use the [`spotifyr`](https://github.com/charlie86/spotifyr) package again, as I did in my post [analyzing the music](https://www.gregdubrow.io/posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/) from my band [Gosta Berling](https://gostaberling.bandcamp.com/).\n\nAs I already had the code I needed to authorize API access through the package, I figued it would be easy, and this would be at most a couple of hours of work.\n\nNot so much.\n\nSparing the detail, let's leave it by noting in order to get access to the playlist endpoint I had to redo the access code and that took a while. Then I had to figure out a way to overcome the rate limit for getting audio features. That took some time. But it's all done, so let's get some data and build some charts.\n\nIn my other Spotify-based post I addressed the ethical issues with using the platform, so I won't rehash here except to say it's a great tool for listening to and discovering music, but please, actually buy music from the artists you like or discover on the platform, **especially** independent artists.\n\nAnyway, Spotify grants access to the API and gives you an id and secret code. The code below shows how to store and access the credentials.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for getting playlist data - pt1\"}\nlibrary(tidyverse)\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(spotifyr) # functions to access spotify API\n\n\n# if you need to add spotify API keys\n# usethis::edit_r_environ()\n# as SPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx'\n# as SPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx'\n\n# identify the scopes, or end-points, that you want.\n# read the spotify api documentation to decide on scopes\n# for playlists we need these\nscopes = c(\n\t\"user-library-read\",\n\t\"user-read-recently-played\",\n\t\"playlist-read-private\",\n\t\"playlist-read-collaborative\",\n\t\"user-read-private\")\n\n# uses the scopes and credentials to authorise the session.\nauth <- spotifyr::get_spotify_authorization_code(\n\tSys.getenv(\"SPOTIFY_CLIENT_ID\"),\n\tSys.getenv(\"SPOTIFY_CLIENT_SECRET\"),\n\tscopes)\n\n# pulls a list of your playlists..default is 20, you can get up to 50\nplaylists <- spotifyr::get_user_playlists('dannebrog13', limit = 20,\n\toffset = 0, authorization = auth)\n\n# alternately, go to spotify and get the link to the playlist (click on the ... and select share)\n# https://open.spotify.com/playlist/6mlrhYRlFv3Ji5xwzqFWwQ?si=966ddb877c414599\n\n# The playlist code you need is after \"playlist/\" and goes up to the ?\n\n# set the playlist code as an object\nlikeddid <- \"6mlrhYRlFv3Ji5xwzqFWwQ\"\n```\n:::\n\n\nOk, we have the info we need to get the list of songs in the playlist. To do that I amended code I found in [this post](https://www.rpubs.com/womeimingzi11/how_my_spotify_looks_like), and changed the function options for the `get_playlist_tracks` call.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for getting playlist data - pt2\"}\nall_liked_tracks <-\n\tceiling(get_playlist_tracks(\"6mlrhYRlFv3Ji5xwzqFWwQ\", include_meta_info = TRUE)[['total']] / 50) %>%\n\tseq() %>%\n\tmap(function(x) {\n\t\tget_playlist_tracks(\"6mlrhYRlFv3Ji5xwzqFWwQ\",\n\t\tfields = NULL,\n\t\tlimit = 50, offset = (x - 1) * 50,\n\t\tmarket = NULL,\n\t\tauthorization = get_spotify_access_token(),\n\t\tinclude_meta_info = FALSE)\n\t}) %>% reduce(rbind)\n```\n:::\n\n\nI would recommend saving successful pulls as local `rds` files. You run the risk of hitting your rate limit if you call the API too many times.\n\nNow that we have a list of songs, we want to get the audio features. These are measures that Spotify has attached to tracks to indicate things like \"danceability\", \"energy\", \"valence\" (essentially happiness) & other items. Read the [Spotify developers' guide](https://developer.spotify.com/documentation/web-api/reference/get-audio-features) for more detail.\n\nGetting the audio features for a long list like this one, 1900+ entries, poses a challenge when the rate limit per call is 100 tracks. I was trying to figure out a way to loop over chunks of 100 with `Sys.sleep()` call but couldn't get it to work. Luckily, someone figured out the solution to get around the rate limit that takes hardly any time. I won't copy the code here, you can find it on the package's repo on the [issues tab](https://github.com/charlie86/spotifyr/issues/130#issuecomment-885076286).\n\nWhat I'll show below is the code to merge the track details with the audio features into a nice tidy dataframe. Read the comments in the code to see the process.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for getting playlist data - pt3\"}\n# initial version of df. contains a couple of nested lists that need unpacking. easier to do in stages.\nliked_songs1 <- liked_track_features %>%\n# assign key number values to character values.\n\tmutate(key_name = case_when(key == 0 ~ \"C\", key == 1 ~ \"C#/Db\", key == 2 ~ \"D\",\n\t\t\tkey == 3 ~ \"D#/Eb\", key == 4 ~ \"E\", key == 5 ~ \"F\",\n\t\t\tkey == 6 ~ \"F#/Gb\", key == 7 ~ \"G\", key == 8 ~ \"G#/Ab\",\n\t\t\tkey == 9 ~ \"A\", key == 10 ~ \"A#/Bb\", key == 11 ~ \"B\")) %>%\n\tmutate(mode_name = case_when(mode == 0 ~ \"Minor\", mode == 1 ~ \"Major\")) %>%\n\tmutate(key_mode = paste(key_name, mode_name, sep = \" \")) %>%\n\tselect(track.id = id, key_name:key_mode, time_signature, tempo, duration_ms,\n\t\t\t\t danceability, energy, loudness, speechiness:valence,\n\t\t\t\t key, mode, type, uri, track_href, analysis_url) %>%\n\t# join to playlist track information\n\tleft_join(all_liked_tracks) %>%\n\t#remove a few variables we won't need\n\tselect(-video_thumbnail.url, -track.episode, -added_by.href:-added_by.external_urls.spotify) \n\n# unnest some columns\nliked_songs <- unnest_wider(liked_songs1, track.artists, names_sep = \"_\") %>%\n\tselect(-track.artists_external_urls.spotify, -track.artists_uri, -track.artists_type, -track.artists_href) %>%\n\tmutate(track.artist_names = map_chr(track.artists_name, toString)) %>%\n\tseparate(track.artist_names, paste0('track.artist', c(1:6)), sep = ',', remove = F) %>%\n\tmutate(across(56:60, str_trim)) %>%\n\tselect(track.id:track.artists_id, track.artist_names:track.artist6, everything(), -track.artists_name)\n\n## At this point, save the file as an rds!!\n\n```\n:::\n\n\nBTW, the playlist in question is my \"liked songs\", those I starred or hearted or whatever. To make it a list I could access via API I had to copy from the static list Spotify makes (but isn't in the API) into a user list. You can find it [here](https://open.spotify.com/playlist/6mlrhYRlFv3Ji5xwzqFWwQ?si=966ddb877c414599). There is of course some selection bias in the list...is the track on Spotify, have I either come across it & liked it or remembered to because I wanted a list like this for comfort listening...etc. It's not perfectly representative of the music I like the most, but close enough.\n\nMy most liked songs are from artists like R.E.M., The National, The Hold Steady, Superchunk, Replacements, Paul Kelly, The Feelies, Teenage Fanclub, Yo La Tengo, Robyn Hitchcock...so you get the idea of sounds for when we look at the plots of audio features.\n\nNow it's time to make a chart. I decided to visualise the differences in audio feature values between major and minor key songs. So first I created a dataframe with all the summary stats.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for getting visualisation data ready\"}\nliked_songs_summary <-\nliked_songs %>%\n\trename(duration.ms = duration_ms) %>%\n\t# change loudness to abs value for better scaling in plots\n\tmutate(loudness = abs(loudness)) %>%\n\tgroup_by(mode_name) %>%\n\tsummarise_at(vars(tempo:valence),\n\t\tlist(mean = mean,\n\t\tq25 = ~quantile(., 0.25),\n\t\tmed = median,\n\t\tq75 = ~quantile(., 0.75),\n\t\tmin = min, max = max)) %>%\n \tpivot_longer(-mode_name,\n \t\tnames_to = \"var_measure\",\n \t\tvalues_to = \"value\") %>%\n\tseparate_wider_delim(var_measure, \"_\", names = c(\"var\", \"measure\")) %>%\n\t#mutate(value = round(value, 2)) %>%\n\t# change duration to seconds for easier explanation\n\tmutate(value = ifelse(var == \"duration.ms\", value / 1000, value)) %>%\n\tmutate(value = round(value, 2)) %>%\n\tmutate(var = ifelse(var == \"duration.ms\", \"duration_sec\", var))\n```\n:::\n\n\nNow to make charts. I decided on lollipop charts to show the min and max values at either end, and the mean value along the lollipop stem. I thought I'd be able to write the basic chart as a function or put them all in one faceted cahart, but the audio features have different scales. Most are 0-1, but tempo (beats per minute) gets up to the 200s, loudness (in decibels) goes -60-0.\n\nThen I tried to make a function for the axis scale to respond to the audio feature, but it wasn't working so in the end it was easier to copy and amend the code as needed for the features. First up, we do all the 0-1 scale features as a faceted plot.\n\nClick on the chart to enlarge it.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for audio features plot 1\"}\nlibrary(ggtext) # enhancements for text in ggplot\n\nliked_songs_summary %>%\n\tfilter(var %in% c(\"acousticness\", \"danceability\", \"energy\", \"instrumentalness\",\n\t\t\t\t\t\t\t\t\t \"liveness\", \"speechiness\", \"valence\")) %>%\n\tpivot_wider(names_from = measure, values_from = value) %>%\n\tggplot() +\n\tgeom_segment(aes(x= min, xend=max, y=fct_rev(mode_name), yend=fct_rev(mode_name)), color=\"grey\") +\n\tgeom_point( aes(x=min, y=mode_name), color=\"#4E79A7\", size=3 ) +\n\tgeom_point( aes(x=max, y=mode_name), color=\"#79A74E\", size=3 ) +\n\tgeom_point( aes(x=mean, y=mode_name), color=\"#A74E79\", size=3 ) +\n\tscale_x_continuous(limits = c(0, 1), breaks = scales::pretty_breaks(4)) +\n\tlabs(x = \"\", y = \"\", title = \"Spotify defined audio feature values for Liked Songs playlist by major & minor key tracks\",\n\t\t\t subtitle = \"<span style='color: #4E79A7;'>Min</span> \\n<span style='color: #A74E79;'>Mean</span> \\n<span style='color: #79A74E;'>Max</span>\",\n\t\t\t caption = \"*Data from Spotify API via spotifyr package*\") +\n\ttheme_minimal() +\n\ttheme(panel.grid = element_blank(),\n\t\t\t\tplot.subtitle = element_markdown(size = 12),\n\t\t\t\tplot.caption = element_markdown(),\n\t\t\t\tstrip.text.x = element_text(size = 10 )) +\n\tfacet_wrap(~ var, ncol = 2, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](images/prompt9_5-1.png){width=100%}\n:::\n:::\n\n\nThere's not a ton of difference between major and minor key songs, only for liveness, which tries to measure the presence of an audience, and speechiness, or is the singing more talky or not. I was a bit surprised that valence (Spotify's happiness measure) was the same for both.\n\nLet's now do tempo *(beats per minute)*, duration *(in seconds)*, and loudness *(in decibels, \"the quality of a sound that is the primary psychological correlate of physical strength \\[amplitude\\]\")*.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for audio features plot2-4\"}\n# var == \"tempo\" ~ c(60, 210),\nliked_songs_summary %>%\n\tfilter(var == \"tempo\") %>%\n\tselect(-var) %>%\n\tpivot_wider(names_from = measure, values_from = value) %>%\n\tggplot() +\n\tgeom_segment(aes(x= min, xend=max, y=fct_rev(mode_name), yend=fct_rev(mode_name)), color=\"grey\") +\n\tgeom_point( aes(x=min, y=mode_name), color=\"#4E79A7\", size=3 ) +\n\tgeom_point( aes(x=max, y=mode_name), color=\"#79A74E\", size=3 ) +\n\tgeom_point( aes(x=mean, y=mode_name), color=\"#A74E79\", size=3 ) +\n\tscale_x_continuous(limits = c(0, 220), breaks = scales::pretty_breaks(4)) +\n\tlabs(x = \"\", y = \"\", title = \"Minimal difference in tempo values between major & minor key songs\",\n\t\t\t subtitle = \"*Audio feature: tempo (beats per minute) <br>\n\t\t\t x axis scale reflects min & max values for playlist.*\",\n\t\t\t caption = \"*Data from Spotify API via spotifyr package*\") +\n\tannotate(geom = \"richtext\",\n\t\t\t\t\t label = \"<span style='color: #4E79A7;'>Min</span> \\n<span style='color: #A74E79;'>Mean</span> \\n<span style='color: #79A74E;'>Max</span>\",\n\t\t\t\t\t x = 150, y = 1.5) +\n\ttheme_minimal() +\n\ttheme(panel.grid = element_blank(),\n\t\t\t\tplot.subtitle = element_markdown(size = 12),\n\t\t\t\tplot.caption = element_markdown(),\n\t\t\t\taxis.text.y = element_text(size = 12))\n```\n\n::: {.cell-output-display}\n![](images/prompt9_6-1.png){width=100%}\n:::\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for audio features plot2-4\"}\n\n# var == \"duration\" ~ c(60, 1100))\n#duration <-\nliked_songs_summary %>%\n\tfilter(var == \"duration_sec\") %>%\n\tselect(-var) %>%\n\tpivot_wider(names_from = measure, values_from = value) %>%\n\tggplot() +\n\tgeom_segment(aes(x= min, xend=max, y=fct_rev(mode_name), yend=fct_rev(mode_name)), color=\"grey\") +\n\tgeom_point( aes(x=min, y=mode_name), color=\"#4E79A7\", size=3 ) +\n\tgeom_point( aes(x=max, y=mode_name), color=\"#79A74E\", size=3 ) +\n\tgeom_point( aes(x=mean, y=mode_name), color=\"#A74E79\", size=3 ) +\n\tscale_x_continuous(limits = c(0, 1100), breaks = scales::pretty_breaks(5)) +\n\tlabs(x = \"\", y = \"\", title = \"Minimal difference in average length, though longest song is major key.\",\n\t\t\t subtitle = \"*Audio feature: duration (in seconds)*\",\n\t\t\t caption = \"*Data from Spotify API via spotifyr package*\") +\n\tannotate(geom = \"richtext\",\n\t\t\t\t\t label = \"<span style='color: #4E79A7;'>Min</span> \\n<span style='color: #A74E79;'>Mean</span> \\n<span style='color: #79A74E;'>Max</span>\",\n\t\t\t\t\t x = 700, y = 1.5) +\n\ttheme_minimal() +\n\ttheme(panel.grid = element_blank(),\n\t\t\t\tplot.subtitle = element_markdown(size = 12),\n\t\t\t\tplot.caption = element_markdown(),\n\t\t\t\taxis.text.y = element_text(size = 12))\n```\n\n::: {.cell-output-display}\n![](images/prompt9_6-2.png){width=100%}\n:::\n\n```{.r .cell-code .code-overflow-scroll  code-fold=\"true\" code-summary=\"Show code for audio features plot2-4\"}\n\n# var == \"loudness\" ~ c(0, 30),\n#loud <-\nliked_songs_summary %>%\n\tfilter(var == \"loudness\") %>%\n\tselect(-var) %>%\n\tpivot_wider(names_from = measure, values_from = value) %>%\n\tggplot() +\n\tgeom_segment(aes(x= min, xend=max, y=fct_rev(mode_name), yend=fct_rev(mode_name)), color=\"grey\") +\n\tgeom_point( aes(x=min, y=mode_name), color=\"#4E79A7\", size=3 ) +\n\tgeom_point( aes(x=max, y=mode_name), color=\"#79A74E\", size=3 ) +\n\tgeom_point( aes(x=mean, y=mode_name), color=\"#A74E79\", size=3 ) +\n\tscale_x_continuous(limits = c(0, 60)) +\n\tlabs(x = \"\", y = \"\", title = \"Minimal difference in loudness profile between major & minor key songs\",\n\t\t\t subtitle = \"*Audio feature: loudness. <br>Original values are -60db-0db (decibels). Absolute value use for scaling.*\",\n\t\t\t caption = \"*Data from Spotify API via spotifyr package*\") +\n\tannotate(geom = \"richtext\",\n\t\t\t\t\t label = \"<span style='color: #4E79A7;'>Min</span> \\n<span style='color: #A74E79;'>Mean</span> \\n<span style='color: #79A74E;'>Max</span>\",\n\t\t\t\t\t x = 30, y = 1.5) +\n\ttheme_minimal() +\n\ttheme(panel.grid = element_blank(),\n\t\t\t\tplot.subtitle = element_markdown(size = 12),\n\t\t\t\tplot.caption = element_markdown(),\n\t\t\t\taxis.text.y = element_text(size = 12))\n```\n\n::: {.cell-output-display}\n![](images/prompt9_6-3.png){width=100%}\n:::\n:::\n\n\nSo there you have it. An approach to visualising audio features. There's more to do here, but that will have to wait for an even more in-depth post. This turned out to be longer than I planned.\n\n*p.s. - yes, I know I'm now posting these after the chart challenge is officially over. but they are good exercises for me to keep learning and creating. and i'm enjoying it, so...*\n\n*created and posted May 2, 2024*\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}