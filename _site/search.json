[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gregers kjerulf dubrow",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nExploring Happiness - EDA Part 2\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\nrstats\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nExploring Happiness - Analysis\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\nrstats\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nCall Me By My Name\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\nrstats\n\n\nbabynames\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nExploring Happiness - Part 1…EDA\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\nrstats\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nRandom music for the new routine\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\nrstats\n\n\nmusic\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nMigration from Hugo to Quarto\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\nhowto\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\n…and we’re back. Migration and resurrection\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nSad Songs & Pretty Charts - a Gosta Berling Music Data Visualization\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nmusic\n\n\nspotify\n\n\ngosta berling\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2021\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nInvited Talk at University of San Francisco, February 2020\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nhigher education\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2021\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, February 2, 2021 - HBCU Enrollment\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nhigher education\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2021\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, April 07, 2020 - Le Tour! (Stage 2, charts!)\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nsports\n\n\ncycling\n\n\ntour de france\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, April 07, 2020 - Le Tour! (Stage 1, cleaning the data)\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nsports\n\n\ncycling\n\n\ntour de france\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, November 27, 2018 - Maryland Bridges\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, November 24, 2020 - Hiking Trails in WA State\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "gregers kjerulf dubrow",
    "section": "",
    "text": "I’m a data analyst/data scientist living in København Denmark. I am open to work, be it full-time, contract, or freelance r/statistics and general editing consultation for projects like graduate theses & dissertations or other research projects.\nThough my main subject area expertise is in higher education policy, undergraduate admissions & enrollment management, and student success, the work I have done translates very well to people analytics (employee learning and retention) and business problems like customer churn and retention.\nI work in r for everything from data import & cleaning, to analysis, modeling & visualization. I have also used SAS and excel and have basic working knowledge of SQL & Python and a passing familiarity with Tableau.\nMy favorite part of data work is visualisation and making nice charts & tables. I’m also quite happy to do the data grunt work that is important but less glamorous - cleaning and munging data to get it ready for analysis and machine learning modeling.\nI was born in Denmark and grew up in the US. After many years in San Francisco I lived in Lyon France from May to December 2022 , working as an ESL instructor after earning a CELTA via the ELT Hub.\nIn this space I’ll mostly be posting code-throughs and results from my personal data projects; music, football (the soccer kind), education, #tidytuesday data projects and other work. All views expressed here are my own.\nOver the years I’ve made some music with Gosta Berling, Slowness, Big Still, The Trolleyvox and Idle Wilds. My photography is at Flickr.\nUnless otherwise cited, all photographs used in posts are mine, with a CC BY-NC-ND 4.0 license"
  },
  {
    "objectID": "posts/tidy_tuesday_maryland_bridges/index.html",
    "href": "posts/tidy_tuesday_maryland_bridges/index.html",
    "title": "Tidy Tuesday, November 27, 2018 - Maryland Bridges",
    "section": "",
    "text": "This dataset was posted to the #TidyTuesday repo back in November 2018. I worked on it a bit then, but didn’t properly finish it until March of 2020. With the blog finally set up figured I might as well post it as an entry.\nUpdate for migration from Hugo to Quarto…now that code-fold is native to code chunks, I’ll sometimes use if for long bits of code. Just click the down arrow to show code\n\n# readin in data, create df for plots\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\nFirst let’s read in the file from the raw data file on github\n\ntt_balt_gh <-\nread_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-27/baltimore_bridges.csv\",\n         progress = show_progress())\n\n\n\nOrganize and clean the data for analysis\n\nclean up some date and name issues\na decade-built field, factored for sorting\ntime since last inspection\n\n\n\nshow data cleaning code\n# keeping the comparison date March 21 when I originally did analysis\ntoday <- as.Date(c(\"2020-03-21\"))\n#Sys.Date()\ntoday_yr <- as.numeric(format(today, format=\"%Y\"))\n\ntt_mdbrdf <- as.data.frame(tt_balt_gh) %>%\n  mutate(age = today_yr - yr_built) %>%\n  #  mutate(vehicles_n = as.numeric(str_remove(vehicles, \" vehicles\")))\n  ## not needed, avg_daily_traffic has same info\n  mutate(inspection_yr = inspection_yr + 2000) %>%\n  mutate(county = ifelse(county == \"Baltimore city\", \"Baltimore City\", county)) %>%\n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  mutate(bridge_condition = factor(bridge_condition, levels = c(\"Good\", \"Fair\", \"Poor\"))) %>%\n  mutate(decade_built = case_when(yr_built <= 1899 ~ \"pre 1900\", \n                                  yr_built >= 1900 & yr_built <1910 ~ \"1900-09\",\n                                  yr_built >= 1910 & yr_built <1920 ~ \"1910-19\",\n                                  yr_built >= 1920 & yr_built <1930 ~ \"1920-29\",\n                                  yr_built >= 1930 & yr_built <1940 ~ \"1930-39\",\n                                  yr_built >= 1940 & yr_built <1950 ~ \"1940-49\",\n                                  yr_built >= 1950 & yr_built <1960 ~ \"1950-59\",\n                                  yr_built >= 1960 & yr_built <1970 ~ \"1960-69\",\n                                  yr_built >= 1970 & yr_built <1980 ~ \"1970-79\",\n                                  yr_built >= 1980 & yr_built <1990 ~ \"1980-89\",\n                                  yr_built >= 1990 & yr_built <2000 ~ \"1990-99\",\n                                  yr_built >= 2000 & yr_built <2010 ~ \"2000-09\",\n                                  TRUE ~ \"2010-19\")) %>%\n  mutate(decade_built = factor(decade_built, levels = \n                                 c(\"pre 1900\", \"1900-09\", \"1910-19\", \"1920-29\", \"1930-39\",\n                                   \"1940-49\", \"1950-59\", \"1960-69\", \"1970-79\", \n                                   \"1980-89\", \"1990-99\", \"2000-09\", \"2010-19\"))) %>%\n  mutate(inspect_mmyy = ISOdate(year = inspection_yr, month = inspection_mo, day = \"01\")) %>%\n  mutate(inspect_mmyy = as.Date(inspect_mmyy, \"%m/%d/%y\")) %>%\n  mutate(inspect_days = today - inspect_mmyy) %>%\n  mutate(inspect_daysn = as.numeric(inspect_days)) %>%\n  mutate(inspect_years = inspect_daysn/ 365.25) %>%\n  mutate(inspect_months = inspect_daysn / 30.417)\n\n\n\n\nThe first few charts look at bridges built by decade, the condition of all bridges by county, and how long since last inspection.\n\ntt_mdbrdf %>% \n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  count(decade_built) %>%\n  ggplot(aes(decade_built, n)) +\n  geom_bar(stat = \"identity\", fill = \"navy\") +\n  geom_text(aes(label = n), color = \"white\", vjust = 1.2) +\n  labs(title = \"Peak bridge-building years in Maryland were 1950s to 1990s\" ,\n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nBaltimore City has the lowest percentage of bridges in good condition, Anne Arundel the most. Baltimore City & Harford County seems to have the largest percentage of bridges in poor condition.\n\n\nshow stacked bar chart code\n## percent bridge condition by county\n# need to create df object to do subset label call in bar chart\nbrcondcty <- \ntt_mdbrdf %>%\n  count(county, bridge_condition) %>%\n  group_by(county) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() \n\nggplot(brcondcty, aes(x = county, y = pct, fill = bridge_condition)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(data = subset(brcondcty, bridge_condition != \"Poor\"), \n            aes(label = percent(pct)), position = \"stack\", \n            color= \"#585858\", vjust = 1, size = 3.5) +\n  scale_y_continuous(label = percent_format()) +\n  labs(title = \"Percent bridge condition by county\" , \n        x = \"\", y = \"\", fill = \"Bridge Condition\") +\n  scale_fill_brewer(type = \"seq\", palette = \"Blues\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nGiven the condition percentages in Baltimore County & City and Harford County, it’s no surprise that their bridges are older than in other counties.\n\n\nshow bar chart code\n## median age of bridges by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medage = median(age)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medage)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medage, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median bridge age by county\" , \n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt’s somewhat reassuring then that Baltimore City bridges at least have less time in months since last inspection than do the counties.\n\n\nshow bar chart code\n## median months since last inspection by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medinsp = median(inspect_months)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medinsp)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medinsp, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median months since last inspection, by county\",\n       subtitle = \"as of March 2020\",\n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt might be the outliers pulling the smoothing line straight, but there doesn’t seem to be too much of a relationship between age and time since last inspection.\n\n\nshow scatterplot code\n## age by months since last inspection\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\n\nAnd in fact, removing the outliers shows a slight relationship; the older bridges do seem to get inspected more frequently. In terms of a better visualization, looking at this again, I wonder if some jittering or another type of plot might have been more visually appealing, givne the clustering of most recent inspections.\n\n# same code as above but with outliers removed\ntt_mdbrdf %>%\n  filter(age <150, inspect_months <60) %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(title = \"Months since inspection, outliers removed\", \n       x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\nNot sure if scatter-plot with colors by county is best way to go for this idea. Maybe a tree map?\n\n\nshow scatterplot code\n# same but colors by county\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  labs(title = \"Months since last inspection (from current date)\", \n       x = \"Months since last inspection (from current date)\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.8, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nFunky distributions here…Anne Arundel & Baltimore City have the highest median daily riders, but Howard County’s upper quartile is way out there.\nshowing the code here to illustrate how I like to run the mean & interquartiles in the same code as rendering the plot.\n\n# median & interquartiles of daily riders of bridges by county -\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medtraf = median(avg_daily_traffic),\n            lq = quantile(avg_daily_traffic, 0.25),\n            uq = quantile(avg_daily_traffic, 0.75)) %>%\n  ungroup() %>%\n  ggplot(aes(county, medtraf)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"navy\") +\n  geom_point(size = 3, color = \"orange\", alpha = .8) +\n  geom_text(aes(label = comma(medtraf, digits = 0)), \n            size = 4, color = \"orange\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = comma(uq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = comma(lq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Median & interquartile ranges of average daily riders per bridge, by county\" , \n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nAs with the other scatterplot with colors for county, might need a different way to see relationship between bridge age and daily traffic by county.\n\n\nshow scatterplot code\n## age by avg daily riders by county\ntt_mdbrdf %>%\n  ggplot(aes(avg_daily_traffic, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  scale_x_continuous(labels = comma) +\n  labs(title = \"Average daily traffic per bridge, by county\" , \n        x = \"Average daily traffic\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.75, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nCover image for post (CC BY-SA 4.0) from Wikipedia\nThis post was last updated on 2023-05-14"
  },
  {
    "objectID": "posts/migrating-from-hugo-to-quarto/index.html",
    "href": "posts/migrating-from-hugo-to-quarto/index.html",
    "title": "Migration from Hugo to Quarto",
    "section": "",
    "text": "The Saône & The Rhône converge in Lyon\n\n\nAs I mentioned in the Migration and Resurrection post, some issues with my old Hugo build and Netlify playing nicely with Hugo & Wowchemy led to me deciding to rebuild things in Quarto. I was thinking about it anyway, as Quarto seems robust and even more user-friendly (to me) than blogdown & rmarkdown, which I still love. I just love Quarto more.\nThe basic idea of the blog remains the same - present the code & analysis for topics that interest me. While switching from SAS to r, I learned so much from reading tons of data blogs and watching how-to videos that I wanted to give back to the rstats community by showing and explaining my work. It also helps me as my own reference for code and approach to visualization when I forget how I did a pivot-long or cleaned and set-up data to work well with the visualization I wanted to do.\nI decided to set some ground rules for the old posts:\n\nMinimal text edits…only for typos and occasional clunky language.\nBut no editing the code. The projects I posted showed where I was in using r at the time. Though to be honest, if anything I’ve regressed a bit as I haven’t used r that much since I left my last data job. Besides, the rmarkdown -> quarto migration already entailed enough editing in the YAML and code chunk headers.\nThe only exception to the code edit rule was changing the chunk options to the #| syntax and adding code-fold options to some code chunks. It wasn’t easily doable in blogdown & Hugo when I first launched the site but it’s a native functionality to Quarto, so hoorah!\nRepost the projects with date-stamps from the original posting. I want this to still accurately document my own data analysis/data science progression, even with that long gap between posts.\n\nSo how did I do it?\nFirst I spent a sunny weekend afternoon in May watching two presentations about Quarto.\nThis Welcome to Quarto Workshop from August 2022 led by Thomas Mock.\n\n\n\n\n\n\n\n\n\n\nAfter that it was Isabella Velásquez’s presentation on building a blog with Quarto.\nI also read the extensive Quarto documentation.\nEach of their personal blogs are nicely done in Quarto so I’ll also be poking around their github repos.\nAs I had already working with blogdown & rmarkdown, the transition to Quarto was smooth. Minor grammatical differences in the YAML and code chunks, but nothing that didn’t make sense.\nSetting up the blog is as simple as starting any new project in r. Just go to: File -> New Project -> New Directory -> Quarto blog and fill in the name and directory location.\nAfter setting up the basic design using one of the packaged themes and drafting the landing and About pages, I pushed the new files to github and hoped that Netlify would play nicely and render the new site.\nOn the first commit, which wiped out all of my old content and replaced with the new files, Netlify did its thing but I got the dreaded Error 404, site not found. With a little digging I found out that I had to go the Build & Deploy -> Continuous Deployment -> Build Settings section and add _site to the Publish Directory box like this:\n\n\n\n\n\nDid that, did another git commit and voila, up and running.\nNext step was to spend a sunny Sunday afternoon redoing my old rmd files to qmd, and navigating the differences in YAML and code chunk options.\n\n\n\ncode chunk options in rmd\n\n\n\n\n\ncode chunk options in qmd\n\n\nI also like the intelligent completion bits in Quarto \nand using ctrl + space to see all the parameters for the option you’re setting.\nquick aside…used veed.io for convert a screen-capture movie to animated gif…quick and easy\nGoing forward I’ll probably tweak the design now and then as I learn a bit more customization and functionality in Quarto and learn CSS and other styling tools for things like wrapping text around images and other tweaks and enhancements. But for now the site looks good and it’s time to get back to adding new data posts.\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/blog-migration-and-resurrection/index.html",
    "href": "posts/blog-migration-and-resurrection/index.html",
    "title": "…and we’re back. Migration and resurrection",
    "section": "",
    "text": "So much has happened since my last post here in February 2021. The blog was set up during the early stages of COVID lockdown when all of a sudden I had more time on my hands.\nSoon after that last post my wife and I made the decision to move to Europe in early 2022, so started preparations for that - arrange for a shipping container, learn French, apply for a CELTA program.\nThe world started to repoen a bit in late summer 2021, but then my wife broke her ankle. So all of a sudden I was doing lots of home health care & keeping the house running on top of work. The wasn’t much brain space for independent data projects & keeping the blog running.\nThen in late April 2022 we moved. The first stop was Lyon France\n\n\n\n\n\nwhere I earned the CELTA via the ELT Hub.\nI had expected teaching gigs to trickle in and I’d do independent data work while also spending time learning French and exploring Lyon and more of France. But the gigs came in abundance, and they were all new preps. So free time was at a minimum.\nThen in December 2022 we moved again. A job at CIEE in Denmark\n\n\n\n\n\ncame up so we accelerated the ultimate end goal of moving here - back to where I was born and still have lots of family and friends. Now that we’re settled in our first non-sublet in almost a year and I’ve gotten settled at work, there’s finally some time to get my head back into data work. I was partly inspired by serving as a mentor in the DDSA mentoring program. I also started using r at work to automate a few mundane tasks and to do a little analysis on our students.\nI’ll write about the migration process in a bit more detail in another post, but the tl/dr as to why is because in trying to do a simple update to my old main picture (me in a mask, early in lockdown) the build was failing in Netlify. So rather than try and diagnose the Hugo/Wowchemy issues (which turned out to be the Ubuntu deploy image…an easy reset), I saw that Posit’s new Quarto platform is very robust and a bit easier to publish with, so I decided to just rebuild the entire site.\nTo read about the migration details, go here. All of the old posts are up, time-stamped with the original posting dates. And hopefully soon, new posts as I work through a fairly long list of project ideas and try to get back to working on #tidytuesday datasets.\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/tidy-tuesday-maryland-bridges/index.html",
    "href": "posts/tidy-tuesday-maryland-bridges/index.html",
    "title": "Tidy Tuesday, November 27, 2018 - Maryland Bridges",
    "section": "",
    "text": "The Francis Scott Key Bridge in Baltimore\n\n\nThis dataset was posted to the #TidyTuesday repo back in November 2018. I worked on it a bit then, but didn’t properly finish it until March of 2020. With the blog finally set up figured I might as well post it as an entry.\nUpdate for migration from Hugo to Quarto…now that code-fold is native to code chunks, I’ll sometimes use if for long bits of code. Just click the down arrow to show code\n\n# readin in data, create df for plots\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\nFirst let’s read in the file from the raw data file on github\n\ntt_balt_gh <-\nread_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-27/baltimore_bridges.csv\",\n         progress = show_progress())\n\n\n\nOrganize and clean the data for analysis\n\nclean up some date and name issues\na decade-built field, factored for sorting\ntime since last inspection\n\n\n\nshow data cleaning code\n# keeping the comparison date March 21 when I originally did analysis\ntoday <- as.Date(c(\"2020-03-21\"))\n#Sys.Date()\ntoday_yr <- as.numeric(format(today, format=\"%Y\"))\n\ntt_mdbrdf <- as.data.frame(tt_balt_gh) %>%\n  mutate(age = today_yr - yr_built) %>%\n  #  mutate(vehicles_n = as.numeric(str_remove(vehicles, \" vehicles\")))\n  ## not needed, avg_daily_traffic has same info\n  mutate(inspection_yr = inspection_yr + 2000) %>%\n  mutate(county = ifelse(county == \"Baltimore city\", \"Baltimore City\", county)) %>%\n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  mutate(bridge_condition = factor(bridge_condition, levels = c(\"Good\", \"Fair\", \"Poor\"))) %>%\n  mutate(decade_built = case_when(yr_built <= 1899 ~ \"pre 1900\", \n                                  yr_built >= 1900 & yr_built <1910 ~ \"1900-09\",\n                                  yr_built >= 1910 & yr_built <1920 ~ \"1910-19\",\n                                  yr_built >= 1920 & yr_built <1930 ~ \"1920-29\",\n                                  yr_built >= 1930 & yr_built <1940 ~ \"1930-39\",\n                                  yr_built >= 1940 & yr_built <1950 ~ \"1940-49\",\n                                  yr_built >= 1950 & yr_built <1960 ~ \"1950-59\",\n                                  yr_built >= 1960 & yr_built <1970 ~ \"1960-69\",\n                                  yr_built >= 1970 & yr_built <1980 ~ \"1970-79\",\n                                  yr_built >= 1980 & yr_built <1990 ~ \"1980-89\",\n                                  yr_built >= 1990 & yr_built <2000 ~ \"1990-99\",\n                                  yr_built >= 2000 & yr_built <2010 ~ \"2000-09\",\n                                  TRUE ~ \"2010-19\")) %>%\n  mutate(decade_built = factor(decade_built, levels = \n                                 c(\"pre 1900\", \"1900-09\", \"1910-19\", \"1920-29\", \"1930-39\",\n                                   \"1940-49\", \"1950-59\", \"1960-69\", \"1970-79\", \n                                   \"1980-89\", \"1990-99\", \"2000-09\", \"2010-19\"))) %>%\n  mutate(inspect_mmyy = ISOdate(year = inspection_yr, month = inspection_mo, day = \"01\")) %>%\n  mutate(inspect_mmyy = as.Date(inspect_mmyy, \"%m/%d/%y\")) %>%\n  mutate(inspect_days = today - inspect_mmyy) %>%\n  mutate(inspect_daysn = as.numeric(inspect_days)) %>%\n  mutate(inspect_years = inspect_daysn/ 365.25) %>%\n  mutate(inspect_months = inspect_daysn / 30.417)\n\n\n\n\nThe first few charts look at bridges built by decade, the condition of all bridges by county, and how long since last inspection.\n\ntt_mdbrdf %>% \n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  count(decade_built) %>%\n  ggplot(aes(decade_built, n)) +\n  geom_bar(stat = \"identity\", fill = \"navy\") +\n  geom_text(aes(label = n), color = \"white\", vjust = 1.2) +\n  labs(title = \"Peak bridge-building years in Maryland were 1950s to 1990s\" ,\n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nBaltimore City has the lowest percentage of bridges in good condition, Anne Arundel the most. Baltimore City & Harford County seems to have the largest percentage of bridges in poor condition.\n\n\nshow stacked bar chart code\n## percent bridge condition by county\n# need to create df object to do subset label call in bar chart\nbrcondcty <- \ntt_mdbrdf %>%\n  count(county, bridge_condition) %>%\n  group_by(county) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() \n\nggplot(brcondcty, aes(x = county, y = pct, fill = bridge_condition)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(data = subset(brcondcty, bridge_condition != \"Poor\"), \n            aes(label = percent(pct)), position = \"stack\", \n            color= \"#585858\", vjust = 1, size = 3.5) +\n  scale_y_continuous(label = percent_format()) +\n  labs(title = \"Percent bridge condition by county\" , \n        x = \"\", y = \"\", fill = \"Bridge Condition\") +\n  scale_fill_brewer(type = \"seq\", palette = \"Blues\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nGiven the condition percentages in Baltimore County & City and Harford County, it’s no surprise that their bridges are older than in other counties.\n\n\nshow bar chart code\n## median age of bridges by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medage = median(age)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medage)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medage, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median bridge age by county\" , \n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt’s somewhat reassuring then that Baltimore City bridges at least have less time in months since last inspection than do the counties.\n\n\nshow bar chart code\n## median months since last inspection by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medinsp = median(inspect_months)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medinsp)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medinsp, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median months since last inspection, by county\",\n       subtitle = \"as of March 2020\",\n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt might be the outliers pulling the smoothing line straight, but there doesn’t seem to be too much of a relationship between age and time since last inspection.\n\n\nshow scatterplot code\n## age by months since last inspection\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\n\nAnd in fact, removing the outliers shows a slight relationship; the older bridges do seem to get inspected more frequently. In terms of a better visualization, looking at this again, I wonder if some jittering or another type of plot might have been more visually appealing, givne the clustering of most recent inspections.\n\n# same code as above but with outliers removed\ntt_mdbrdf %>%\n  filter(age <150, inspect_months <60) %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(title = \"Months since inspection, outliers removed\", \n       x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\nNot sure if scatter-plot with colors by county is best way to go for this idea. Maybe a tree map?\n\n\nshow scatterplot code\n# same but colors by county\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  labs(title = \"Months since last inspection (from current date)\", \n       x = \"Months since last inspection (from current date)\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.8, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nFunky distributions here…Anne Arundel & Baltimore City have the highest median daily riders, but Howard County’s upper quartile is way out there.\nshowing the code here to illustrate how I like to run the mean & interquartiles in the same code as rendering the plot.\n\n# median & interquartiles of daily riders of bridges by county -\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medtraf = median(avg_daily_traffic),\n            lq = quantile(avg_daily_traffic, 0.25),\n            uq = quantile(avg_daily_traffic, 0.75)) %>%\n  ungroup() %>%\n  ggplot(aes(county, medtraf)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"navy\") +\n  geom_point(size = 3, color = \"orange\", alpha = .8) +\n  geom_text(aes(label = comma(medtraf, digits = 0)), \n            size = 4, color = \"orange\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = comma(uq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = comma(lq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Median & interquartile ranges of average daily riders per bridge, by county\" , \n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nAs with the other scatterplot with colors for county, might need a different way to see relationship between bridge age and daily traffic by county.\n\n\nshow scatterplot code\n## age by avg daily riders by county\ntt_mdbrdf %>%\n  ggplot(aes(avg_daily_traffic, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  scale_x_continuous(labels = comma) +\n  labs(title = \"Average daily traffic per bridge, by county\" , \n        x = \"Average daily traffic\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.75, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nCover image for post (CC BY-SA 4.0) from Wikipedia\nThis post was last updated on 2023-05-14"
  },
  {
    "objectID": "blog-migration-and-resurrection/index.html",
    "href": "blog-migration-and-resurrection/index.html",
    "title": "…and we’re back. Migration and resurrection",
    "section": "",
    "text": "Lots of things happening since last post in February 2021. The blog was set up during the early stages of COVID lockdown when all of a sudden I had more time on my hands.\nBut since that last post my wife and I made the decision to move to Europe in early 2022 and started preparations for that. The world started to repoen a bit in late summer 2021, and then my wife broke her ankle. So all of a sudden I was doing lots of home health care & keeping the house running on top of work. The wasn’t much brain space for independent data projects & keeping the blog running.\nThen in April 2022 we moved. The first stop was Lyon France\n\n\n\n\n\nwhere I earned a CELTA via the ELT Hub.\nI had expected teaching gigs to trickle in and I’d do independent data work while also spending time learning French and exploring Lyon and more of France. But the gigs came in abundance, and they were all new preps. So free time was at a minimum.\nThen in December 2022 we moved again. A job at CIEE in Denmark\n\n\n\n\n\ncame up and we accelerated the ultimate end goal of moving here - back to where I was born and still have lots of family and friends. Now that we’re settled in our first non-sublet in almost a year and I’ve gotten settled at work, there’s finally some time to get my head back into data work. I was partly inspired by serving as a mentor in the DDSA mentoring program. I also started using r at work to automate a few mundane tasks and to do a little analysis on our students.\nI’ll write about the migration process in a bit more detail in another post, but the tl/dr as to why is because in trying to do a simple update to my old main picture (me in a mask, early in lockdown) the build was failing in Netlify. So rather than try and diagnose the Hugo/Wowchemy issues (which turned out to be the Ubuntu deploy image…an easy reset), I saw that Posit’s new Quarto platform is very robust and a bit easier to publish with, so I decided to just rebuild the entire site.\nTo read about the migration details, go here. All of the old posts are up, time-stamped with the original posting dates. And hopefully soon, new posts as I work through a fairly long list of project ideas."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "© Copyright Gregers Kjerulf Dubrow\nThis is my personal website. All views are mine, and nothing I post here should be taken as an endorsement by my employer or any organizations of which I am a member.\nContent on this site is provided under a Creative Commons (CC-BY) 4.0 license. You may reuse this content as long as you indicate cite me as the author and provide a link back to the original material. Source code of the site is provided under the MIT license and may be reused without restriction."
  },
  {
    "objectID": "posts/tidy-tuesday-feb-02-2021-hbcu/index.html",
    "href": "posts/tidy-tuesday-feb-02-2021-hbcu/index.html",
    "title": "Tidy Tuesday, February 2, 2021 - HBCU Enrollment",
    "section": "",
    "text": "When the the Tidy Tuesday HBCU dataset was posted I got excited because here was something right in my scope of knowledge; enrollment in higher education. It was a great first Tidy Tuesday set for Black History Month. To keep this from being a journal-length piece, I won’t put on my history of higher ed teacher hat & get into how HBCUs came to be. Here’s a good overview.\nOne of my strengths as a data analyst is a brain that’s constantly asking questions of a dataset:\n* what’s in there?\n* what relationships exist between variables?\n* what I can add to the dataset to glean more insight?\nFor Tidy Tuesday though that slows things down – it’s mostly meant to be a quick turnaround thing where you share results to Twitter. So my deep-dive habits mean I’m usually a bit behind getting Tidy Tuesday analysis done the week the data are posted. My goal for this analysis is to show:\n* changes over time in the racial and gender make-up of students at HBCUs\n* changes over time in overall enrollment at HBCUs by sector (public 4yr, private 4y, etc), relative to non-HBCUs\n* changes over time in tuition & fees by sector, HBCUs vs non-HBCUs\nThe Tidy Tuesday data has some of what I need - it comes from Data.World via IPEDS (essentially the US Department of Education’s higher ed data library) . I’m supplementing it with data from the Delta Cost Project, which aggrgated years worth of data from IPEDS.\nSo let’s dive into the data.\nFirst we load the packages we’ll be using…\n\n# load packages\nlibrary(tidytuesdayR) # load the tidy tuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot\nlibrary(ggrepel) # helper functions for ggplot\n\n…then lets make a couple of things we’ll use a lot going forward.\n\n# create notin operator to help with cleaning & analysis\n`%notin%` &lt;- negate(`%in%`)\n\n# function for year-over-year percent changes\npctchange &lt;- function(x) {\n  (x - lag(x)) / lag(x)\n}\n\nNow we’ll load the sets from TidyTuesday and then the Delta Cost Project files.\nI’ll load the Tidy Tuesday data objects via the tidytuesdayR package and clean up the data. This week’s data came in a few separate files. For my analysis I need a dataframe of HBCU enrollments with Black and non-Black students. So I need to combine the two HBCU enrollment files (one each for Black students and all students) into one and subtract Black from All to get non-Black. I’ll show code for loading the Black student set in, cleaning it up a bit, and skip to the subtraction & joins. For the full code experience, head to my Tidy Tuesday repo\n\n\nshow tt_hbcu_black data load & cleaning code\ntt_hbcu_load &lt;- tt_load(\"2021-02-02\")\n\ntt_hbcu_black &lt;- as_tibble(tt_hbcu_load$hbcu_black) %&gt;%\n  clean_names() %&gt;%\n  mutate(year = as.character(year)) %&gt;%\n  mutate(ethnicity = \"Black\") %&gt;%\n  select(year, ethnicity, enroll_n = total_enrollment, women = females, men = males, \n         four_year_all = x4_year, two_year_all = x2_year,\n         total_public, four_year_pub = x4_year_public, two_year_pub = x2_year_public, \n         total_private, four_year_pri = x4_year_private, two_year_pri = x2_year_private) \n\n\n\n\nshow tt_hbcu_all data load & cleaning code\ntt_hbcu_all &lt;- as_tibble(tt_hbcu_load$hbcu_all) %&gt;%\n  clean_names() %&gt;%\n  mutate(ethnicity = \"All\") %&gt;%\n  mutate(year = as.character(year)) %&gt;%\n  select(year, ethnicity, enroll_n = total_enrollment, women = females, men = males, \n         four_year_all = x4_year, two_year_all = x2_year,\n         total_public, four_year_pub = x4_year_public, two_year_pub = x2_year_public, \n         total_private, four_year_pri = x4_year_private, two_year_pri = x2_year_private)  \n\n\nLet’s join tt_hbcu_black with tt_hbcu_all and create the non-Black ethnicity group. There is probably a better way to do the subtractions programmatically with a function but my r skills aren’t there yet. The final dataframes I’ll use for plots are tt_hbcu_enr_eth_sex & tt_hbcu_enr_eth_sect. Because of the way the source data was arrayed I could do ethnicty by gender and ethnicity by sector but not all three vectors. So, two sets…\n\n\nshow dataframe join code\ntt_hbcu_notblack = as.data.frame(tt_hbcu_all) %&gt;%\n  select(-ethnicity) %&gt;%\n    # renaming multiple columns in one line of code!\n  rename_with(~ paste(.x, \"all\", sep = \"_\"), .cols = (2:12)) %&gt;%\n  bind_cols(tt_hbcu_black) %&gt;%\n  select(-year...13, -ethnicity) %&gt;%\n  rename(year = year...1) %&gt;%\n  # the subtractions by column to get non-black. again, might be a better way to do it w/ a function.\n  mutate(enroll_n_nb = enroll_n_all - enroll_n) %&gt;%\n  mutate(women_nb = women_all - women) %&gt;%\n  mutate(men_nb = men_all - men) %&gt;%\n  mutate(four_year_all_nb = four_year_all_all - four_year_all) %&gt;%\n  mutate(two_year_all_nb = two_year_all_all - two_year_all) %&gt;%\n  mutate(total_public_nb = total_public_all - total_public) %&gt;%\n  mutate(four_year_pub_nb = four_year_pub_all - four_year_pub) %&gt;%\n  mutate(two_year_pub_nb = two_year_pub_all - two_year_pub) %&gt;%\n  mutate(total_private_nb = total_private_all - total_private) %&gt;%\n  mutate(four_year_pri_nb = four_year_pri_all - four_year_pri) %&gt;%\n  mutate(two_year_pri_nb = two_year_pri_all - two_year_pri) %&gt;%\n  mutate(ethnicity = \"Not Black\") %&gt;%\n  select(year, ethnicity, enroll_n_nb:two_year_pri_nb) %&gt;%\n  rename_with(~ str_remove(.x, \"_nb\"), .cols = (3:13))\n\n# create final dataframes, turn wide to long\n# note sex pct is by eth group\ntt_hbcu_enr_eth_sex = as.data.frame(rbind(tt_hbcu_all, tt_hbcu_black)) %&gt;%\n  rbind(tt_hbcu_notblack) %&gt;%\n  select(-four_year_all:-two_year_pri) %&gt;%\n  arrange(year, ethnicity) %&gt;% \n  group_by(year) %&gt;%\n   mutate(enroll_eth_pct = enroll_n / first(enroll_n)) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(cols = women:men,\n               names_to = \"sex\",\n               values_to = \"sex_n\") %&gt;%\n  arrange(year, ethnicity) %&gt;%\n  group_by(year, ethnicity) %&gt;%\n  mutate(enroll_sex_pct = sex_n / sum(sex_n)) %&gt;%\n  ungroup() %&gt;%\n  select(year, ethnicity, enroll_n, enroll_eth_pct, sex, sex_n, enroll_sex_pct) %&gt;%\n  arrange(year, ethnicity, sex)\n\n# note pct_sect_eth is by eth group by year, pct_eth_sect is pct eth w/in sector\ntt_hbcu_enr_eth_sect = rbind(tt_hbcu_all, tt_hbcu_black) %&gt;%\n  rbind(tt_hbcu_notblack) %&gt;%\n  select(-women, -men) %&gt;%\n  arrange(year, ethnicity) %&gt;%\n  pivot_longer(cols = four_year_all:two_year_pri,\n               names_to = \"sector\", \n               values_to = \"sector_n\") %&gt;%\n  arrange(year, ethnicity) %&gt;%\n  mutate(pct_sect_eth = sector_n / enroll_n) %&gt;%\n  arrange(year, sector) %&gt;%\n  group_by(year, sector) %&gt;%\n  mutate(pct_eth_sect = sector_n / (sum(sector_n) /2)) %&gt;%\n  ungroup()\n\n\nOk, the Tidy Tuesday data is sorted, so let’s use the haven package load in the Delta Cost Project (DCP) data they provided as SAS files.\nThey’ve split their data into two files, one from 1987 to 1999 and one from 2000 to 2015. There’s a ton of data in the set, but all I want for this analysis is enrollments and tuition - I’ll pull those fields from each set, then rbind together into a dataframe called tuitenr_8715_agg for year-over-year by-sector analysis. Unfortunately while DCP has enrollment by ethnicity, they only do it for total enrollment - grad and undergrad combined - so I can’t do some of the HBCU vs non-HBCU comparisons. There are other sources which I’ll describe at the end of the post.\nOk, loading the 2000 to 2015 data…\n\n\nshow DCP 2000 - 2015 load code\ndelta0015all &lt;- (haven::read_sas(\"~/Data/ipeds/delta_public_release_00_15.sas7bdat\", NULL))\ndelta0015_tuitenr &lt;- delta0015all %&gt;%\n  filter(between(sector_revised, 1, 6)) %&gt;%\n  mutate(sector_desc = case_when(sector_revised == 1 ~  \"Public 4yr\",\n                                 sector_revised == 2    ~ \"Private nonprofit 4yr\",\n                                 sector_revised == 3    ~ \"Private for-profit 4yr\",\n                                 sector_revised == 4    ~ \"Public 2yr\",\n                                 sector_revised == 5    ~ \"Private nonprofit 2yr\",\n                                 sector_revised == 6    ~ \"Private for-profit 2yr\")) %&gt;%\n # mutate(total_undergraduates = ifelse(is.na(total_undergraduates), 0, total_undergraduates)) %&gt;%\n  select(unitid, instname, sector_revised, sector_desc, hbcu,\n         year = academicyear, tuition_fee_ug_in = tuitionfee02_tf, \n         tuition_fee_ug_oos = tuitionfee03_tf, \n         total_undergraduates)\n\n\nNow let’s load the 1987 to 1999 data…\n\n\nshow DCP 1987 - 1999 load code\ndelta8799all &lt;- (haven::read_sas(\"~/Data/ipeds/delta_public_release_87_99.sas7bdat\", NULL))\n\ndelta8799_tuitenr &lt;- as_tibble(delta8799all) %&gt;%\n  filter(between(sector_revised, 1, 6)) %&gt;%\n  mutate(sector_desc = case_when(sector_revised == 1 ~  \"Public 4yr\",\n                                 sector_revised == 2    ~ \"Private nonprofit 4yr\",\n                                 sector_revised == 3    ~ \"Private for-profit 4yr\",\n                                 sector_revised == 4    ~ \"Public 2yr\",\n                                 sector_revised == 5    ~ \"Private nonprofit 2yr\",\n                                 sector_revised == 6    ~ \"Private for-profit 2yr\")) %&gt;%\n # mutate(total_undergraduates = ifelse(is.na(total_undergraduates), 0, total_undergraduates)) %&gt;%\n  select(unitid, instname, sector_revised, sector_desc, hbcu,\n         year = academicyear, tuition_fee_ug_in = tuitionfee02_tf, \n         tuition_fee_ug_oos = tuitionfee03_tf, \n         total_undergraduates)\n\n\n…and let’s join and munge the DCP files…\n\n\nshow DCP join & clean code\ntuitenr_8715 &lt;- rbind(delta0015_tuitenr, delta8799_tuitenr)\n\ntuitenr_8715_agg &lt;-\n  tuitenr_8715 %&gt;%\n  # filter(!sector_desc == \"Private for-profit 2yr\" &\n  #          !sector_desc == \"Private for-profit 4yr\") %&gt;%\n#  filter(!is.na(tuition_fee_ug_in)) %&gt;%\n  group_by(year, sector_desc, hbcu) %&gt;%\n  summarise(enr_tot = sum(total_undergraduates, na.rm = TRUE),\n            mean_tuit_in = mean(tuition_fee_ug_in, na.rm = TRUE),\n            mean_tuit_oos = mean(tuition_fee_ug_oos, na.rm = TRUE)\n            ) %&gt;%\n  mutate(level = ifelse(str_detect(sector_desc, \"2yr\"), \"2yr\", \"4yr\")) %&gt;%\n#  summarise(mean_tuit_in = mean(tuition_fee_ug_in, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  mutate(hbcu_f = ifelse(hbcu == 1, \"HBCU\", \"Not HBCU\")) %&gt;%\n  mutate(hbcu_f = factor(hbcu_f, levels = c(\"HBCU\", \"Not HBCU\"))) \n\n\nBecause I’ll be doing some year-over-year percent change analysis, I’ll create a separate dataframe from tuitenr_8715_agg (the DCP data) so I can compare HBCU to non-HBCU changes by year.\nRemember the pctchange function I created when I loaded the packages? Here it’s put to good use. I’ll only show code for one of the frames - I created frames for each sector, HBCU and non-HBCU. The final data set will rbind 10 frames into tuit_8715_pctchgh. BTW, this is another example of when I wish I were a bit better at purrr/mapping and functions so I could have done it with less code.\nSince the data provided starts in 1987, I’ll filter that year out of the dataframes.\n\ntuit_8715_pctchgh1 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 1, sector_desc == \"Public 4yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\n\n\nshow the remaining sector files code\ntuit_8715_pctchgh2 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 1, sector_desc == \"Private nonprofit 4yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh3 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 1, sector_desc == \"Public 2yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh4 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 1, sector_desc == \"Private nonprofit 2yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh5 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Public 4yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh6 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Private nonprofit 4yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh7 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Public 2yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh8 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Private nonprofit 2yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh9 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Private for-profit 4yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh10 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Private for-profit 2yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh &lt;- rbind(tuit_8715_pctchgh1, tuit_8715_pctchgh2) %&gt;%\n  rbind(tuit_8715_pctchgh3) %&gt;%\n  rbind(tuit_8715_pctchgh4) %&gt;%\n  rbind(tuit_8715_pctchgh5) %&gt;%\n  rbind(tuit_8715_pctchgh6) %&gt;%\n  rbind(tuit_8715_pctchgh7) %&gt;%\n  rbind(tuit_8715_pctchgh8) %&gt;%\n  rbind(tuit_8715_pctchgh9) %&gt;%\n  rbind(tuit_8715_pctchgh10) \n\n\nOk, the data is in place, let’s see what it tells us! I’ll fold the code for the more basic charts, click on the arrow to see it. You’ll note that this analysis is mostly line graphs. That’a because I’m mostly doing trend analysis, and I like line graphs for time series.\nFirst up we’ll use the Tidy Tuesday data to look at HBCU enrollment by ethnicity (Black and non-Black) from 1990 to 2015. What do we see? Overall, periods of increase and decrease in total enrollment. But interestingly we see that non-Black enrollment kept increasing, even as Black enrollment started to drop. Of course Black students make up a vast majority of HBCU enrollment, so their trends drive overall numbers. It is worth noting though that the changes started in the 2010s, coinciding with changes in how NCES counted ethncity. It’s beyond the scope of this quick (ha) analysis, but worth digging deeper to see the effect of changes on the ethnic mix of HBCUs.\n\n\nshow the enrollment by ethnicity line charts code\n## tt hbcu data, black men v women over time\ntt_hbcu_enr_eth_sex %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  ggplot(aes(year, enroll_n, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format(), breaks = scales::pretty_breaks()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  facet_grid(ethnicity ~ ., scales = \"free_y\") +\n  labs(title = \"Black Enrollment at HBCUs Rising & Falling Since 1990\",\n       subtitle = \"Non-Black enrollment slowly & streadily increasing\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Total UG Enrollment\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity line charts code\n\ntt_hbcu_enr_eth_sex %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  filter(ethnicity == \"Black\") %&gt;%\n  distinct(year, ethnicity, .keep_all = T) %&gt;%\n  ggplot(aes(year, enroll_eth_pct, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1.25) +\n  scale_y_continuous(limits = c(.5, 1), \n                     labels = scales::percent_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Slight Decrease Over Time in Percentage of Black Students Enrolled at HBCUs\",\n       subtitle = \"Might be due to changes in how ethnicity is coded by US Dept of Ed\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Pct Black students\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), \n        plot.title = element_text(size = 11),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nNext let’s look at enrollment by ethnicity & sex. HBCU enrollment mirrors that of higher ed enrollment in general - since the early 1980s women outnumber men. There are differences by sector (4-year, 2-year, public & private) and field of study but the oveall trend has been consistent. We do see that for non-Black students at HBCUs the percentage of women is a bit less than that of Black students.\n\n\nshow the enrollment by ethnicity & sex line charts code\ntt_hbcu_enr_eth_sex %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  ggplot(aes(year, sex_n, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  # scale_y_continuous(limits = c(0, 250000),\n  #                    breaks = c(0, 50000, 100000, 150000, 200000, 250000),\n  #                    labels = scales::comma_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Black Women Enroll in Higher Numbers Than Men at HBCUs\",\n       subtitle = \"Same Pattern as Overall Undergrad Enrollments in the US\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Total UG Enrollment\") +\n  facet_grid(ethnicity ~ sex, scales = \"free_y\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity & sex line charts code\n\n# sex - women as pct\ntt_hbcu_enr_eth_sex %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  filter(sex == \"women\") %&gt;%\n  ggplot(aes(year, enroll_sex_pct, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(limits = c(.5, .8), \n                     labels = scales::percent_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Women a Greater Percentage of those Enrolled at HBCUs\",\n       subtitle = \"Slightly higher among Black students than not Black\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Pct Women enrolled\") +\n  facet_grid(ethnicity ~ ., scales = \"free_y\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nThe next thing I want to look at is enrollment by sector for HBCUs. In this case we think of sectors along two axes - public and private, and 2-year (community/junior colleges) vs 4-year (baccalaureate granting). What these first two sector charts show us is that most HBCU enrollment is in publics and in 4-year schools. Given the land-grant history of most HBCUs, that makes sense.\nI am leaving in the code for the first chart to highlight how one can a) have two different colored lines within the same chart in a faceted visualization (the scale_color_manual call) and b) use the amazing ggtext package to get two different colors of text in the subtitle line, so it does the work of a legend. The &lt;span style = &lt;/span&gt; and the element_markdown call in the theme specification is how you make it happen.\nThe third chart takes a slightly different view of the data, looking at percent of enrollment by sector within each of the ethnic categories we have in the data, Black and not-Black. What the first two charts showed is confirmed here - that Black students in HBCUs are mostly in public 4-years, and that trend has held steady since at least 1990.\n\ntt_hbcu_enr_eth_sect %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %&gt;%\n  filter(sector %in% c(\"total_private\", \"total_public\")) %&gt;%\n  ggplot(aes(year, pct_sect_eth, group = sector, color = sector)) +\n  geom_line(size = 1) +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"Public HBCUs Enroll Greater Pct of Students \",\n       subtitle = \"Greater Pct Black students in Private HBCU than non-Black students; \n       &lt;span style = 'color:#56B4E9;'&gt;Blue = Public&lt;/span&gt; \n       &lt;span style = 'color:#E69F00;'&gt;Orange = Private&lt;/span&gt;\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by Publc & Private HBCUs\") +\n  facet_grid(ethnicity ~ .) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_markdown(size = 8, face = \"italic\"),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\n\nshow the enrollment by ethnicity & sector charts code\ntt_hbcu_enr_eth_sect %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %&gt;%\n  filter(sector %in% c(\"four_year_all\", \"two_year_all\")) %&gt;%\n  ggplot(aes(year, pct_sect_eth, group = sector, color = sector)) +\n  geom_line(size = 1) +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"4-year HBCUs Enroll ~ 90% of all Students\",\n       subtitle = \"Black students more likely to be in 4-year HBCU than non-Black students; \n       &lt;span style = 'color:#56B4E9;'&gt;Blue = 4-year&lt;/span&gt; \n       &lt;span style = 'color:#E69F00;'&gt;Orange = 2-year&lt;/span&gt;\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by 4yr & 2yr HBCUs\") +\n  facet_grid(ethnicity ~ .) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_markdown(size = 8, face = \"italic\"),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity & sector charts code\n\ntt_hbcu_enr_eth_sect %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %&gt;%\n  filter(sector %in% c(\"four_year_pub\", \"four_year_pri\", \"two_year_pri\", \"two_year_pub\")) %&gt;%\n  mutate(sector = str_replace(sector, \"four_year_pub\", \"4 Year Public\")) %&gt;%\n  mutate(sector = str_replace(sector, \"two_year_pub\", \"2 Year Public\")) %&gt;%\n  mutate(sector = str_replace(sector, \"four_year_pri\", \"4 Year Private\")) %&gt;%\n  mutate(sector = str_replace(sector, \"two_year_pri\", \"2 Year Private\")) %&gt;%\n#  ggplot(aes(year, enroll_sect_pct, group = sector, color = sector)) +\n  ggplot(aes(year, pct_sect_eth, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"Black Students most likely to be in 4-year HBCUs\",\n       subtitle = \"Non-black students moving to 2-year HBCUs from 4-yr; \n       &lt;span style = 'color:#E69F00;'&gt;Percents sum to 100% across ethnic groups&lt;/span&gt;\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by Sector\") +\n  facet_grid(ethnicity ~ sector) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        axis.text.x = element_text(size = 6),\n        plot.subtitle = element_markdown(face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nSo we’ve done what we can with the Tidy Tuesday data, time to compare HBCUs to non-HBCUs using the Delta Cost Project data. We’ll stay with sector analysis and look at enrollments by sector. For the chart output I wanted, I used Thomas Lin Pederson’s patchwork package. I’ll fold the code that creates the component charts and just show the patchwork call.\n\n\nshow the enrollment by sector charts code\nenr19902015hbcu &lt;-\ntuitenr_8715_agg %&gt;%\n  filter(hbcu == 1) %&gt;% \n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(title = \"HBCU Enrollment Across Sector 1990-2015\",\n       #caption = \"Source: Delta Cost Project\",\n    x = \"\", y = \"Total UG enrollment\") +\n  facet_wrap(~ sector_desc, scales = \"free\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n                axis.text.y = element_text(size = 7), \n        #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\nenr19902015nothbcu &lt;-\n  tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2) %&gt;%\n#  filter(level == \"2yr\") %&gt;% \n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#56B4E9\", group = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(title = \"non-HBCU Enrollment Across Sector 1990-2015\",\n       caption = \"Source: Delta Cost Project\",\n       x = \"\", y = \"Total UG enrollment\") +\n  facet_wrap(~ sector_desc, scales = \"free\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#56B4E9\"),\n                axis.text.y = element_text(size = 7), \n        #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\nenr19902015all &lt;- enr19902015hbcu / enr19902015nothbcu\nenr19902015all\n\n\n\n\nFor a change of pace let’s see a slope graph of percent change in tuition by sector from 1990 to 2015. I found a helpful code-thru of a similar-ish slope graph here and used that as my guide. I used patchwork for this too, and because in this instance I used patchwork for title and other adornments, I’ll show code for all steps.\nTwo observations stand out - first, HBCUs are less expensive by sector than non-HBCUs. But for both HBCUs and non-HBCUs, the price gap between private 4-year schools and the other sectors widened during this time period.\nBut these are sticker prices and don’t include institutional aid, or “tuition discount”. If a college offers you a $3000 institutional grant (not a federal or state grant or loan), the money is essentially foregone revenue for the school, or a discount to the stated tuition price.\n\n\nshow the slope graph code\ntuitsect_hbcu &lt;-\n    tuitenr_8715_agg %&gt;%\n    filter(hbcu == 1 & (year == 1990 | year == 2015)) %&gt;%\n    #  filter(level == \"2yr\") %&gt;% \n    ggplot(aes(year, mean_tuit_in, group = sector_desc)) +\n    geom_line(aes(color = sector_desc)) +\n    geom_point(aes(color = sector_desc)) +\n    scale_color_manual(values = c(\"Public 4yr\" = \"#999999\", \n                                                                \"Public 2yr\" = \"#E69F00\", \n                                                                \"Private nonprofit 2yr\" = \"#56B4E9\", \n                                                                \"Private nonprofit 4yr\" = \"#009E73\")) +\n    geom_text_repel(data = tuitenr_8715_agg %&gt;%\n                                        filter(hbcu == 1 & year == 2015),\n                                    aes(label = sector_desc), hjust = \"left\", nudge_x = .5,\n                                    direction = \"y\", size = 4) +\n    annotate(\"text\", x = 1991, y = 12000, label = \"HBCUs\", size = 5, fontface = \"italic\") +\n    scale_x_continuous(breaks = c(1990, 2015)) +\n    scale_y_continuous(labels = scales::dollar_format(),\n                                         limits = c(0, 15000),\n                                         breaks = c(0, 2500, 5000, 7500, 10000, 12500, 15000)) +\n    labs(x = \"\", y = \"Average Tuition & Fees\") +\n    theme_light() +\n    theme(panel.grid.major = element_blank(), panel.grid.major.x = element_blank(), \n                panel.grid.minor = element_blank(), panel.grid.minor.y = element_blank(),\n                panel.background = element_blank(), \n                strip.background = element_rect(fill = \"#56B4E9\"),\n                panel.border = element_blank(), legend.position = \"none\",  \n                axis.text.y = element_text(size = 8), \n                axis.ticks = element_blank(), axis.text.x = element_blank(), \n                axis.title.y = element_text(hjust = -.07),\n                #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n                plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\ntuitsect_nonhbcu &lt;-\n    tuitenr_8715_agg %&gt;%\n    filter(hbcu == 2 & (year == 1990 | year == 2015)) %&gt;%\n    #  filter(level == \"2yr\") %&gt;% \n    ggplot(aes(year, mean_tuit_in, group = sector_desc)) +\n    geom_line(aes(color = sector_desc)) +\n    geom_point(aes(color = sector_desc)) +\n    scale_color_manual(values = c(\"Public 4yr\" = \"#999999\", \n                                                                \"Public 2yr\" = \"#E69F00\", \n                                                                \"Private nonprofit 2yr\" = \"#56B4E9\", \n                                                                \"Private nonprofit 4yr\" = \"#009E73\",\n                                                                \"Private for-profit 2yr\" = \"#0072B2\", \n                                                                \"Private for-profit 4yr\" = \"#CC79A7\")) +\n    geom_text_repel(data = tuitenr_8715_agg %&gt;%\n                                        filter(hbcu == 2 & year == 2015),\n                                    aes(label = sector_desc), hjust = \"left\", nudge_x = .5, \n                                    direction = \"y\", size = 3) +\n    annotate(\"text\", x = 1992, y = 20000, label = \"not HBCUs\", size = 5, fontface = \"italic\") +\n    scale_x_continuous(breaks = c(1990, 2015)) +\n    scale_y_continuous(labels = scales::dollar_format(),\n                                         limits = c(0, 25200),\n                                         breaks = c(0, 5000, 10000, 15000, 20000, 25000)) +\n    labs(x = \"\", y = \"\") +\n    theme_light() +\n    theme(panel.grid.major = element_blank(), panel.grid.major.x = element_blank(), \n                panel.grid.minor = element_blank(), panel.grid.minor.y = element_blank(),\n                panel.background = element_blank(), \n                strip.background = element_rect(fill = \"#56B4E9\"),\n                panel.border     = element_blank(),  axis.text.y = element_text(size = 8), \n                legend.position = \"none\", axis.ticks       = element_blank(),\n                #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n                plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\ntuitsec &lt;- tuitsect_hbcu / tuitsect_nonhbcu  + plot_annotation(\n    title = 'Tuition & Fees at HBCUs Lower by Sector than non-HBCUs',\n    subtitle = 'Private non-profit 4-yr tuition increased more than other sectors',\n    caption = \"Source: Delta Cost Project\", \n    theme = theme(plot.subtitle = element_text(face = \"italic\", size = 9)))\ntuitsec\n\n\n\n\nFinally, let’s check out some year-over-year percent changes in enrollment & tuition, using the line graphs I love so much. Code is basic ggplot, so not worth showing here. If there’s anything of interest it was in adding a geom_hline call to add the light gray reference line at 0%.\nAs we’d seen in eariler charts, enrollment at HBCUs ping-ponged up and down. There was a bit less volatility in non-HBCUS, and in general enrollments didn’t decrease from 2000 onward.\nAt both HBCUs and non-HBCUs, public & private 4-years raised tuition & fees every year. The publics had a couple of instances of sharper increases, likely in response to recession-hit state budgets. The 2-year schools, especially 2-year HBCUs, had wider up & down swings in tuition.\n\n\nshow the faceted line graph code\ntuit_8715_pctchgh %&gt;%\n  mutate(hbcu_f = case_when(hbcu == 1 ~ \"HBCU\", TRUE ~ \"Not HBCU\")) %&gt;%\n  #  filter(hbcu == 1 & year &gt;= 1990) %&gt;% \n  filter(year &gt;= 1990) %&gt;%\n  filter(sector_desc %in% c(\"Public 4yr\", \"Private nonprofit 4yr\", \"Public 2yr\")) %&gt;%\n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  geom_hline(yintercept=0, color = \"gray\") +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(title = \"Year-over-year Percent Changes to UG Enrollment\",\n       subtitle = \"HBCU sectors had wider positive & negative swings\", \n       caption = \"Source: Delta Cost Project\", \n       x = \"\", y = \"Pct Chnage in-state tuition & fees\") +\n  facet_grid(hbcu_f ~ sector_desc) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the faceted line graph code\n\ntuit_8715_pctchgh %&gt;%\n  mutate(hbcu_f = case_when(hbcu == 1 ~ \"HBCU\", TRUE ~ \"Not HBCU\")) %&gt;%\n  #  filter(hbcu == 1 & year &gt;= 1990) %&gt;% \n  filter(year &gt;= 1990) %&gt;%\n  filter(sector_desc %in% c(\"Public 4yr\", \"Private nonprofit 4yr\", \"Public 2yr\")) %&gt;%\n  ggplot(aes(year, mean_tuit_in)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  geom_hline(yintercept=0, color = \"gray\") +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(title = \"Year-over-year Percent Changes to Tuition & Fees\",\n       subtitle = \"More volatility in 2-year sector, steady increases in every sector\", \n       caption = \"Source: Delta Cost Project\", \n       x = \"\", y = \"Pct Chnage in-state tuition & fees\") +\n  facet_grid(hbcu_f ~ sector_desc) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nThis was designed to be broad-scope, more-or-less exploratory analysis. Just looking for general trends, what might be worth a deeper dive. Like with the percent changes in tuition or enrollment…what was happening in the years with the wider + or - swings? The 2001 recession? The 2009/10 economic collapse? Which specific schools had more success with enrollment? Which HBCUs are the most & least expensive?\nAnd what other questions one could answer with the data I used here? Well for one, the tuition discount rates by sectors and HBCU/not HBCU would be an interesting thing to look at. Did HBCUs need to more aggressively discount tution to meet enrollment targets? The Delta Cost Project set includes a tuition discount variable. There’s a regression analysis in there somewhere, as well as other predictive analysis.\nI’d also want to look at which ethnic groups made up the growth in non-Black enrollments starting in 2010. Or was it that the changes to how ethnicity was recorded meant that students who used to be classified as Black become Hispanic or multi-ethnic. I know from my time in undergrad admissions at UC Berkeley that this accounted for a decline in the federal count of African-American students. However, when we compared the numbers with the UC definitions, we didn’t see a decline.\nTo get those numbers I could download IPEDS data directly from NCES or use r tools like the Urban Insitute’s educationdata package which is an API wrapper to that scrapes NCES and other websites for data.\nThere is a ton of higher education data out there, and it’s never been easier to get at scale than now. Trust me, as someone who has done individual downloads, ordered CD-ROMs, even used the late-great WebCASPAR, if you have an education policy or outcomes related question, there is publicly available data for analysis.\nCover image for post from GWU CCAS Graduate Blog\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/tidy-tuesday-nov-11-2020-hiking-trails-in-wash-state/index.html",
    "href": "posts/tidy-tuesday-nov-11-2020-hiking-trails-in-wash-state/index.html",
    "title": "Tidy Tuesday, November 24, 2020 - Hiking Trails in WA State",
    "section": "",
    "text": "Trail-head lake in the Cascade Mountains.\n\n\n\nDiving deep into the Tidy Tuesday pool…\nEvery week I’m so impressed by the amount of beautiful, creative and informative analyses and data viz efforts that spring from #TidyTuesday, the weekly data project that evolved out of the R4DS Learning Community. And every week I say to myself, “I should really give it a go, grab the data and post some results”. Then life intervenes, work gets crazy, and before I know it…\nWell, around Thanksgiving, with some slack time and some inspiration, I finally got around to it. A resulting benefit was it got me motivated to finally get around to getting this blog set up. My github repo has a folder for any tidytuesday projects I get around to.\nHere’s the code and resulting charts & tables.\n\n# load packages\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\n\nFirst let’s read in the file using the tidytuesdayR package. We’ll also look at the raw data\n\ntt_watrail <- tt_load(\"2020-11-24\")\n#> \n#>  Downloading file 1 of 1: `hike_data.rds`\n\nglimpse(tt_watrail$hike_data)\n#> Rows: 1,958\n#> Columns: 8\n#> $ name        <chr> \"Lake Hills Greenbelt\", \"Snow Lake\", \"Skookum Flats\", \"Ten…\n#> $ location    <chr> \"Puget Sound and Islands -- Seattle-Tacoma Area\", \"Snoqual…\n#> $ length      <chr> \"2.3 miles, roundtrip\", \"7.2 miles, roundtrip\", \"7.8 miles…\n#> $ gain        <chr> \"50\", \"1800\", \"300\", \"1585\", \"500\", \"500\", \"425\", \"450\", \"…\n#> $ highpoint   <chr> \"330.0\", \"4400.0\", \"2550.0\", \"2370.0\", \"1000.0\", \"2200.0\",…\n#> $ rating      <chr> \"3.67\", \"4.16\", \"3.68\", \"3.92\", \"4.14\", \"3.14\", \"5.00\", \"2…\n#> $ features    <list> <\"Dogs allowed on leash\", \"Wildlife\", \"Good for kids\", \"L…\n#> $ description <chr> \"Hike through a pastoral area first settled and farmed in …\n\n\n\nThere are a few things we want to do with the data for the working dataframe:\n\ncreate columns for miles, direction, type from length\ncreate specific location columns frolm location\nchange rating, gain and highpoint to numeric\ncreate a rating group\nchange features to character vector, also unnest; makes the resulting df long. we’ll use distinct when we only need 1 obs per trail\n\n\ntt_watraildf <- tt_watrail$hike_data %>%\n  mutate(length_miles = parse_number(length)) %>%\n  mutate(across(gain:rating, as.numeric)) %>%\n  mutate(rating_grp = case_when(rating == 0 ~ \"0\",\n                                rating >0 & rating < 2 ~ \"1\",\n                                rating >=2 & rating < 3 ~ \"2\",\n                                rating >=3 & rating < 4 ~ \"3\",\n                                rating >=4 & rating < 5 ~ \"4\",\n                                rating == 5 ~ \"5\")) %>%\n  mutate(trail_type = case_when(grepl(\"roundtrip\", length) ~ \"Round trip\",\n                          grepl(\"one-way\", length) ~ \"One Way\",\n                          grepl(\"of trails\", length) ~ \"Trails\")) %>% \n  mutate(location_split = location) %>%\n  separate(location_split, c(\"location_region\",\"location_specific\"), sep = ' -- ') %>%\n  mutate(features = lapply(features, sort, na.last = TRUE)) %>%\n  mutate(feature_v = sapply(features,FUN = function(x) if (all(is.na(x))) NA else paste(x,collapse = \", \"))) %>%\n  mutate(feature_v = str_trim(feature_v)) %>%\n  mutate(features_unnest = features) %>%\n  unnest(cols = c(features_unnest), keep_empty = TRUE) %>% \n  mutate(feature_v = ifelse(is.na(feature_v), \"none\", feature_v)) %>%\n  mutate(features_unnest = ifelse(is.na(features_unnest), \"none\", features_unnest)) %>%\n  mutate(feature_init = case_when(features_unnest == \"Dogs allowed on leash\" ~ \"DA\",\n                                  features_unnest == \"Dogs not allowed\" ~ \"DN\",\n                                  features_unnest == \"Wildlife\" ~ \"Wl\",\n                                  features_unnest == \"Good for kids\" ~ \"GK\",\n                                  features_unnest == \"Lakes\" ~ \"Lk\",\n                                  features_unnest == \"Fall foliage\" ~ \"FF\",\n                                  features_unnest == \"Ridges/passes\" ~ \"RP\",\n                                  features_unnest == \"Established campsites\" ~ \"EC\",\n                                  features_unnest == \"Mountain views\" ~ \"MV\",\n                                  features_unnest == \"Old growth\" ~ \"OG\",\n                                  features_unnest == \"Waterfalls\" ~ \"Wf\",\n                                  features_unnest == \"Wildflowers/Meadows\" ~ \"WM\",\n                                  features_unnest == \"Rivers\" ~ \"Ri\",\n                                  features_unnest == \"Coast\" ~ \"Co\",\n                                  features_unnest == \"Summits\" ~ \"Su\")) %>%\n  mutate(feature_init = ifelse(is.na(feature_init), \"none\", feature_init)) %>%\n  mutate(feature_type = if_else(feature_init %in% c(\"DA\",\"DN\",\"GK\"), \"Companion\", \"Feature\")) %>%\n  mutate(feature_type = ifelse(feature_init == \"none\", \"none\", feature_type)) %>%\n  group_by(name) %>%\n  mutate(feature_n = n()) %>%\n  ungroup() %>%\n  mutate(feature_n = ifelse(feature_init == \"none\", 0, feature_n)) %>%\n  select(name, location_region, location_specific, trail_type, length_miles, \n         gain, highpoint, rating, rating_grp, features, feature_v, features_unnest, \n         feature_init, feature_type, feature_n, description, location, length)\n\n\n\nTo get a sense of what the data look like, I’ll run some historgrams and scatterplots to see how things cluster, if there are outliers or anything else especially noticable.\nUsing log10 for the length scale to even out the spread. The patchwork package stitches the plots together in a neat panel.\n\nhist_length <-\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles)) +\n  geom_histogram(alpha = 0.8) +\n  scale_x_log10() +\n  labs(x = \"Length (miles), log10\")\n\nhist_gain <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(gain)) +\n  geom_histogram(alpha = 0.8) +\n  scale_x_log10()\n\nhist_high <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(highpoint)) +\n  geom_histogram(alpha = 0.8) \n\nhist_rate <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(rating)) +\n  geom_histogram(alpha = 0.8) \n\n(hist_length | hist_gain) /\n  (hist_high | hist_rate)\n\n\n\n\nFor the scatterplots, I plotted length by gain, faceting by ratings groups and then by region. We do have to be careful with ratings, as they are user-generated and some trails have very few votes. Log10 used again for length.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles, gain)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"Length (miles) log10\", y = \"Total Gain\",\n       title = \"Length v Gain, by Rating Group\") +\n  facet_wrap(vars(rating_grp))\n\n\n\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles, gain)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"Length (miles) log 10\", y = \"Total Gain\",\n       title = \"Length v Gain, by Region\") +\n  facet_wrap(vars(location_region))\n\n\n\n\nThe outliers in terms of gain & length clustered in a few regions, so I wanted to see which they were. Not a surprise they clustered in the Cascades & Rainier.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  filter(gain > 15000) %>%\n  filter(length_miles > 90) %>%\n  select(location_region, name, length_miles, gain) %>%\n  arrange(name) \n#> # A tibble: 5 × 4\n#>   location_region      name                                   length_miles  gain\n#>   <chr>                <chr>                                         <dbl> <dbl>\n#> 1 Southwest Washington Pacific Crest Trail (PCT) Section H -…         148. 27996\n#> 2 South Cascades       Pacific Crest Trail (PCT) Section I -…          99  17771\n#> 3 Central Cascades     Pacific Crest Trail (PCT) Section K -…         117  26351\n#> 4 North Cascades       Pacific Northwest Trail - Pasayten Tr…         119  21071\n#> 5 Mount Rainier Area   Wonderland Trail                                93  22000\n\n\n\nNow that we see how the length, gain, highpoint & ratings spread out, I want build a table to see the averages by region.\nI’ve been wanting to take a deeper dive into gt & reactable. I’ve got some basic gt calls down, but for this excercise I wanted to learn how to conditionally format columns based on value. So inspired by Thomas Mock’s gt primer, a basic table with heatmap-like formatting for some columns. See his explainer for details on the code, and for more features than I’m including.\n\n# create by region averages df\nbyregion <-  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  group_by(location_region) %>%\n  summarise(n_region = n(),\n            avglength = mean(length_miles),\n            avgrating = mean(rating),\n            avggain = mean(gain),\n            avghigh = mean(highpoint),\n            minhigh = min(highpoint),\n            maxhigh = max(highpoint)) %>%\n  mutate_at(vars(avglength:avgrating), round, 2) %>%\n  mutate_at(vars(avggain:avghigh), round, 0) \n\nbyregion %>%\n  gt() %>%\n  fmt_number(columns = vars(avggain, avghigh, minhigh, maxhigh), decimals = 0, use_seps = TRUE) %>%\n  # sets the columns and palette to format cell color by value range\n  data_color(\n    columns = vars(avglength, avgrating, avggain, avghigh, minhigh, maxhigh),\n    colors = scales::col_numeric(\n      palette = c(\"#ffffff\", \"#f2fbd2\", \"#c9ecb4\", \"#93d3ab\", \"#35b0ab\"),\n      domain = NULL)) %>%\n#  tab_style calls add border boxes first to column labels, then body cells\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_column_labels(\n        columns = gt::everything()\n        ))) %>%\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_body(\n        rows = gt::everything()\n      ))) %>%\n    cols_align(columns = TRUE, align = \"center\") %>%\n  cols_align(columns = \"location_region\", align = \"left\") %>%\n  cols_width(vars(location_region) ~ px(150),\n             vars(n_region) ~ px(60),\n             starts_with(\"avg\") ~ px(80),\n             ends_with(\"high\") ~ px(90)\n             ) %>%\n  tab_header(title = \"Regional Averages\",\n             subtitle = md(\"_North Cascades have longest trails,\n                           all mountain areas have lots of gain and highest points_\")) %>%\n  cols_label(location_region = \"Region\", n_region = \"N\", avglength = \"Avg Length (miles)\",\n            avgrating = \"Avg Rating\", avggain = \"Avg Gain (ft)\",avghigh = \"Avg high point\",\n             minhigh = \"Lowest high point\", maxhigh = \"Max high point\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Regional Averages\n    \n    \n      North Cascades have longest trails,\nall mountain areas have lots of gain and highest points\n    \n    \n      Region\n      N\n      Avg Length (miles)\n      Avg Rating\n      Avg Gain (ft)\n      Avg high point\n      Lowest high point\n      Max high point\n    \n  \n  \n    Central Cascades\n219\n9.53\n3.04\n2,276\n4,752\n600\n9,511\n    Central Washington\n79\n5.71\n2.78\n823\n2,260\n240\n6,876\n    Eastern Washington\n142\n9.33\n2.15\n1,592\n4,410\n300\n7,310\n    Issaquah Alps\n76\n5.03\n2.53\n984\n1,518\n250\n3,004\n    Mount Rainier Area\n193\n8.19\n3.35\n1,881\n5,222\n800\n10,080\n    North Cascades\n292\n11.24\n3.08\n2,535\n5,111\n125\n9,200\n    Olympic Peninsula\n209\n8.13\n3.32\n1,572\n2,821\n20\n6,988\n    Puget Sound and Islands\n190\n4.25\n2.80\n452\n573\n10\n3,750\n    Snoqualmie Region\n216\n8.71\n3.15\n2,198\n4,467\n450\n9,416\n    South Cascades\n188\n8.44\n3.07\n1,641\n4,732\n922\n12,276\n    Southwest Washington\n120\n6.58\n2.69\n1,171\n1,774\n20\n7,800\n  \n  \n  \n\n\n\n\n\n\nNow let’s look at the effect of trail features on rating.\nFirst we’ll look at average rating by feature, then fit a model. First, a scatter-plot of number of features listed for a trail with user rating. Looks like at a certain point, it’s diminshing returns on trail features in terms of effect on rating.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(feature_n, rating)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"# of features on a trail\", y = \"User rating\",\n       title = \"Features and Rating by Trail Region\") +\n  facet_wrap(vars(location_region))\n\n\n\n\nHere’s a table similar to the one for averages by region. I used the unnested features, so trails will be represented more than once. Dog-free trails do get the highest ratings, but it’s likely because they also tend to have highest high points, so offer views, are challenging, and so get good ratings.\n\nbyfeature <- \ntt_watraildf %>%\n  group_by(features_unnest) %>%\n  summarise(n_feature = n(),\n            avgrating = mean(rating),\n            avglength = mean(length_miles),\n            avggain = mean(gain),\n            avghigh = mean(highpoint),\n            minhigh = min(highpoint),\n            maxhigh = max(highpoint)) %>%\n  mutate_at(vars(avglength:avgrating), round, 2) %>%\n  mutate_at(vars(avggain:avghigh), round, 0) %>%\n  arrange(desc(avgrating))\n\n## create table\nbyfeature %>%\n  gt() %>%\n  fmt_number(columns = vars(n_feature, avggain, avghigh, minhigh, maxhigh), decimals = 0, use_seps = TRUE) %>%\n  # sets the columns and palette to format cell color by value range\n  data_color(\n    columns = vars(avglength, avgrating, avggain, avghigh, minhigh, maxhigh),\n    colors = scales::col_numeric(\n      palette = c(\"#ffffff\", \"#f2fbd2\", \"#c9ecb4\", \"#93d3ab\", \"#35b0ab\"),\n      domain = NULL)) %>%\n  # tab_style calls add border boxes first to column labels, then body cells\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_column_labels(\n        columns = gt::everything()\n      ))) %>%\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_body(\n        rows = gt::everything()\n      ))) %>%\n  tab_header(title = \"Averages by Feature\",\n             subtitle = md(\"_Dog-free trails with waterfalls & high peaks earn high ratings_\")) %>%\n  cols_align(columns = TRUE, align = \"center\") %>%\n  cols_align(columns = \"features_unnest\", align = \"left\") %>%\n  cols_width(vars(features_unnest) ~ px(150),\n             vars(n_feature) ~ px(60),\n             starts_with(\"avg\") ~ px(80),\n             ends_with(\"high\") ~ px(90)\n             ) %>%\n  cols_label(features_unnest = \"Feature\", n_feature = \"N\", avglength = \"Avg Length (miles)\",\n             avgrating = \"Avg Rating\", avggain = \"Avg Gain (ft)\",avghigh = \"Avg high point\",\n             minhigh = \"Lowest high point\", maxhigh = \"Max high point\") \n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Averages by Feature\n    \n    \n      Dog-free trails with waterfalls & high peaks earn high ratings\n    \n    \n      Feature\n      N\n      Avg Rating\n      Avg Length (miles)\n      Avg Gain (ft)\n      Avg high point\n      Lowest high point\n      Max high point\n    \n  \n  \n    Dogs not allowed\n255\n3.52\n9.28\n1,921\n4,173\n10\n10,080\n    Waterfalls\n282\n3.46\n9.64\n1,938\n3,648\n150\n10,080\n    Established campsites\n396\n3.40\n12.80\n2,380\n4,487\n25\n12,276\n    Ridges/passes\n496\n3.25\n12.24\n2,864\n5,575\n400\n9,511\n    Lakes\n583\n3.19\n9.92\n1,988\n4,231\n20\n9,511\n    Old growth\n534\n3.16\n9.00\n1,746\n3,364\n25\n8,096\n    Mountain views\n1,175\n3.13\n9.72\n2,201\n4,621\n20\n12,276\n    Summits\n454\n3.11\n10.40\n2,854\n5,250\n200\n12,276\n    Wildflowers/Meadows\n952\n3.10\n9.26\n1,967\n4,243\n10\n9,511\n    Rivers\n547\n3.05\n9.76\n1,731\n3,205\n10\n12,276\n    Good for kids\n694\n3.01\n4.63\n569\n2,080\n10\n8,245\n    Wildlife\n747\n3.00\n8.84\n1,541\n3,241\n10\n10,080\n    Dogs allowed on leash\n1,045\n2.94\n7.04\n1,379\n3,183\n20\n12,276\n    Fall foliage\n508\n2.94\n8.28\n1,618\n3,334\n20\n9,249\n    Coast\n106\n2.89\n4.12\n351\n433\n10\n6,454\n    none\n68\n2.38\n7.26\n1,758\n3,849\n60\n8,970\n  \n  \n  \n\n\n\n\n\n\nAnd finally a quick model to see what might affect a trail rating.\nIt’s a simple linear model using length, gain, highpoint, & number of features to predict rating. The elevation of the highest point and number of features are both significant. I’d need to do more digging to see what the power of the estimate is on the rating. It’s also slightly counter-intuitive given that we saw in the charts that length, elevation and gain seem to positively affect rating. But then the model only accounts for 4% of varaince, so it’s not telling us much.\n\n# creat df with distinct observations for each trail \ntt_watraildf_dist <- tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) \n\nwtmodel1 <- lm(rating ~ length_miles + gain + highpoint + feature_n, data = tt_watraildf_dist)\nsummary(wtmodel1)\n#> \n#> Call:\n#> lm(formula = rating ~ length_miles + gain + highpoint + feature_n, \n#>     data = tt_watraildf_dist)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.6984 -0.3776  0.3716  0.9284  2.4565 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.205e+00  8.942e-02  24.663  < 2e-16 ***\n#> length_miles -6.565e-03  5.678e-03  -1.156    0.248    \n#> gain         -3.590e-05  3.100e-05  -1.158    0.247    \n#> highpoint     8.318e-05  1.742e-05   4.775 1.93e-06 ***\n#> feature_n     1.272e-01  1.484e-02   8.576  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.398 on 1919 degrees of freedom\n#> Multiple R-squared:  0.0488, Adjusted R-squared:  0.04682 \n#> F-statistic: 24.61 on 4 and 1919 DF,  p-value: < 2.2e-16\n\nThere’s plenty more to do with the set, and some responses I’ve seen on Twitter have been creative…network graphs, better models…but I was able to brush up on gt, learned how to unnest and keep obs where the list was empty. So a successful #tudytuesday.\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/index.html",
    "href": "posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/index.html",
    "title": "Sad Songs & Pretty Charts - a Gosta Berling Music Data Visualization",
    "section": "",
    "text": "For this post, I thought I’d focus on music analytics, given that music and data science/analysis are two things I’ve spent most of my waking hours doing for a number of years now.\nOver the years I’ve made a lot of music in a number of different projects. For most of my time living in the Bay Area I’ve played with some friends in a band called Gosta Berling. We’ve released two EPs and a full album (click on the album covers to give listen)\n  \nOur sound could be called melancholy mood-pop. We like melody, but we were raised on brooding post-punk so a minor key vibe is definitely present. The Spotify API has musical features including danceability, energy, and valence (what they call ‘happiness’). I used Charlie Thompson’s spotifyr package to see how we score. spotifyr has a bunch of functions designed to make it easier to navigate Spotify’s JSON data structure.\nOne quick thing…I’m using Spotify data so in effect validating Spotify. While I appreciate the availability of the data for projects like this, Spotify needs to do much better by way of paying artists. We don’t have tons of streams, but as you can see from this account report… \n…artists get f$ck-all per stream. So sure, use Spotify, it’s a great tool for discovering new music. And while artists pressure them to pay more per stream, you can help by purchasing music from artists you like. The pandemic has killed touring income, so sales are all that many artists have to support themselves. Help them out, buy the music you like. Especially if they’re on Bandcamp and you buy 1st Fridays, when Bandcamp waives their revenue share, meaning the artist gets every penny. Did I mention you can buy our music on Bandcamp? :)\nAnyway, soapbox off…first thing, let’s load the packages we’ll be using:\n\n# load packages\nlibrary(spotifyr) # pull data from spotify\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot text\n\nlibrary(httr)\nlibrary(stringr) # work with string data\nlibrary(lubridate) # work with dates\nlibrary(GGally) # correlation plots\nlibrary(PerformanceAnalytics) # correlation plots\nlibrary(corrr)  # correlation plots\n\nTo get access the Spotify data, you need a developer key. Charlie’s explained how to do it on the package page, so I won’t repeat that here. To set up the keys in your .Renviron, run usethis::edit_r_environ() and add (where the xs are your codes):\n\nSPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx'\nSPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx'\n\n# or do\nSys.setenv(SPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx')\n\nThis call sets your access token for the data session\nIf you run into redirect issues, see this stackoverflow thread, specifically this comment\nFirst thing is to search the artist data for audio features. I’m pulling in everything into a dataframe. Now initially I had a filter for artist = 'Gosta Berling'. But that was also pulling in data from a Swedish prog-metal band Gösta Berling’s Saga. So I needed to filter on our artist ID and pull in albums and singles (for some reason our EPs were listed as singles, but whatever)\n\n# gets full range of information for tracks from artist\ngosta_audio1 &lt;- get_artist_audio_features(artist = \"4Vb2yqJJthJTAZxKz4Aryn\", include_groups = c(\"album\", \"single\"))\n\nOh…why more than one band with Gosta Berling in their name? Well, The Saga of Gösta Berling was Greta Garbo’s first feature-length film, and based on an old Swedish novel Gösta Berling’s Saga about a drunkard outcast priest seeking redemption. When, like our band, you’re a bunch of movie nerds, and a couple of you are especially obsessed with silent films & old Hollywood, you name your band Gosta Berling. And so does a Swedish band…anyway…more about the data.\nThe code here gets a dataframe for each record. I also needed to add album titles. Next steps were to merge the album dataframes together, extract the song IDs and pass them to the get_track_features() function as a list.\n\n# get album tracks, add album name could merge on other df, easier to quick fix this way\ntravel &lt;- get_album_tracks(id = \"0vBs7ZtBj3ROrRyac3M47q\")\ntravel$album &lt;- \"Travel\"\nsweetheart &lt;- get_album_tracks(id = \"0dJBaJ3VFxOtdG5L9yzALJ\")\nsweetheart$album &lt;- \"Everybody's Sweetheart\"\nwinterland  &lt;- get_album_tracks(id = \"6CMekiY6lCIuBZpzFDInpf\")\nwinterland$album &lt;- \"Winterland\"\n\n# merge album files, output track ids to use for audio features\ngbtracks &lt;- data.table::rbindlist(list(sweetheart, travel, winterland))\n#copy result from console to paste as vector below\ngbtrackids &lt;- dput(as.character(gbtracks$id)) \n\ngosta_audio2 &lt;- \n  get_track_audio_features(c(\"2SotrXjkvjTZf05XSMKGyp\", \"07cTJ65GZ4Lvr6b1CtgPll\", \"4ooz79IN3la97See8IMNRL\", \"7pgCh68iFO0LNUNKWTFFIP\", \"4ZCesDRgGWKEXwq8iKw5FB\", \"4ZdH5B3tijHjWiwyOErgtf\", \"5GWKeBYgOsv3PKutDIQoet\", \"0XXWRsY6URe2Vx7Bxs6k06\", \"0t3AGVXHyF3dEYuhvAYuNz\", \"4ObsuwrVLKUq5aF8whrFqk\", \"0PnjWfIPwsqBtllMILjzxB\", \n\"7uQtlGsKxXOzsSapKTZRFU\", \"3kQuG44stzA3pQf7g61Ipt\", \n\"0YH9wkimhRhCmstNZyxPgO\", \"7rEbjyNO0dTEK6x8HkLqAz\", \"4VgEAtVQtkwIHzKMOROk6X\", \"5R9M4s6QZljNPVVzxoy98h\", \"1FNtHQ0juoKg2yCf9u4VSg\", \"5NWmfmupE7FEJ9O1e9vizu\"),\nauthorization = get_spotify_access_token())\n\nThis gets a dataframe with most of what I want…just a few tweaks needed. First, since they weren’t pulled from the get_track_audio_features() call, I used the track id, name, and album track number from the gbtracks dataframe. Also, because the song key returned as only the numeric value, I created the letter name and mode (major or minor), and ordered the columns.\n\n# get track number and name, merge from gbtracks -\n# need b/c these fields not returned from get_track_audio_features()\ngbtrack2 &lt;- gbtracks %&gt;%\n  select(id, name, album, track_number) %&gt;%\n  rename(track_name = name)\n\n# merge to complete df. add names for key and mode\ngosta_audio &lt;- left_join(gosta_audio2, gbtrack2) %&gt;%\n  mutate(key_name = case_when(key == 0 ~ \"C\", key == 2 ~ \"D\", key == 4 ~ \"E\", key == 5 ~ \"F\",\n                              key == 7 ~ \"G\", key == 9 ~ \"A\", key == 11 ~ \"B\")) %&gt;%\n  mutate(mode_name = case_when(mode == 0 ~ \"Minor\", mode == 1 ~ \"Major\")) %&gt;%\n  mutate(key_mode = paste(key_name, mode_name, sep = \" \")) %&gt;%\n  rename(track_id = id) %&gt;%\n  select(album, track_name, track_number, key_mode, time_signature, duration_ms, \n         danceability, energy, loudness, tempo, valence, \n         acousticness, instrumentalness, liveness, speechiness,\n         key_name, mode_name, key, mode)\n\nOk, we’ve got a nice tidy dataframe, let’s do some analysis & visualization!\nSpotify’s developer pages have good explanations of the data. Some notes from spotify here about elements:\n\nMost of the audio features are 0-1, 1 being highest. e.g. higher speechiness = higher ratio of words::music. Valence is “happiness”, where higher = happier.\nLoundess in dB, tempo is BPM.\n\nSo let’s look at a quick summary of the audio features for our songs.\n\n#&gt;   duration_ms      danceability        energy          loudness      \n#&gt;  Min.   : 75933   Min.   :0.2500   Min.   :0.0476   Min.   :-21.350  \n#&gt;  1st Qu.:295380   1st Qu.:0.3545   1st Qu.:0.3260   1st Qu.:-12.031  \n#&gt;  Median :350053   Median :0.3920   Median :0.5190   Median : -9.943  \n#&gt;  Mean   :334762   Mean   :0.4105   Mean   :0.5233   Mean   :-10.705  \n#&gt;  3rd Qu.:385634   3rd Qu.:0.4820   3rd Qu.:0.7160   3rd Qu.: -7.537  \n#&gt;  Max.   :522760   Max.   :0.5730   Max.   :0.9360   Max.   : -6.014  \n#&gt;      tempo           valence        acousticness     instrumentalness \n#&gt;  Min.   : 82.15   Min.   :0.0349   Min.   :0.00371   Min.   :0.00881  \n#&gt;  1st Qu.:116.51   1st Qu.:0.1620   1st Qu.:0.12920   1st Qu.:0.50800  \n#&gt;  Median :141.83   Median :0.2940   Median :0.39300   Median :0.69800  \n#&gt;  Mean   :131.06   Mean   :0.3105   Mean   :0.41332   Mean   :0.62883  \n#&gt;  3rd Qu.:149.98   3rd Qu.:0.4405   3rd Qu.:0.63750   3rd Qu.:0.84450  \n#&gt;  Max.   :166.01   Max.   :0.6960   Max.   :0.88600   Max.   :0.94400  \n#&gt;     liveness       speechiness     \n#&gt;  Min.   :0.0703   Min.   :0.02540  \n#&gt;  1st Qu.:0.1020   1st Qu.:0.02810  \n#&gt;  Median :0.1160   Median :0.03060  \n#&gt;  Mean   :0.1333   Mean   :0.03699  \n#&gt;  3rd Qu.:0.1265   3rd Qu.:0.03865  \n#&gt;  Max.   :0.3300   Max.   :0.11600\n\nFirst I wanted to look at basic correlations for the values. There are a number of ways to run and visualize correlations in r…a few examples follow. First thing I needed to do was a subset of the gosta_audio df for easier calls with the various correlation packages.\nLet’s try correlations in base r. You get the coefficients in the console or you can output to a dataframe to hard-code the visualization.\n\ncor(gbcorr)\n#&gt;                  duration_ms danceability      energy    loudness      tempo\n#&gt; duration_ms       1.00000000   0.03575546 -0.09957649  0.16485951 -0.1589364\n#&gt; danceability      0.03575546   1.00000000 -0.10466026  0.09671649 -0.2719148\n#&gt; energy           -0.09957649  -0.10466026  1.00000000  0.85748849  0.5140085\n#&gt; loudness          0.16485951   0.09671649  0.85748849  1.00000000  0.4952005\n#&gt; tempo            -0.15893636  -0.27191484  0.51400852  0.49520052  1.0000000\n#&gt; valence          -0.04414383  -0.10232090  0.72025346  0.48053791  0.5519247\n#&gt; acousticness     -0.19009855   0.11222116 -0.74742026 -0.65043898 -0.3612391\n#&gt; instrumentalness  0.12784620   0.06977532 -0.53088295 -0.49709651 -0.4411810\n#&gt; liveness         -0.30987073  -0.25213421  0.49374017  0.30054882  0.5316901\n#&gt; speechiness      -0.30678610  -0.31639826  0.45449667  0.27298422  0.4217976\n#&gt;                      valence acousticness instrumentalness   liveness\n#&gt; duration_ms      -0.04414383   -0.1900986       0.12784620 -0.3098707\n#&gt; danceability     -0.10232090    0.1122212       0.06977532 -0.2521342\n#&gt; energy            0.72025346   -0.7474203      -0.53088295  0.4937402\n#&gt; loudness          0.48053791   -0.6504390      -0.49709651  0.3005488\n#&gt; tempo             0.55192475   -0.3612391      -0.44118097  0.5316901\n#&gt; valence           1.00000000   -0.7793878      -0.29646550  0.4743309\n#&gt; acousticness     -0.77938779    1.0000000       0.39266796 -0.3261889\n#&gt; instrumentalness -0.29646550    0.3926680       1.00000000 -0.3406087\n#&gt; liveness          0.47433091   -0.3261889      -0.34060869  1.0000000\n#&gt; speechiness       0.41684028   -0.3150009      -0.56643572  0.7459700\n#&gt;                  speechiness\n#&gt; duration_ms       -0.3067861\n#&gt; danceability      -0.3163983\n#&gt; energy             0.4544967\n#&gt; loudness           0.2729842\n#&gt; tempo              0.4217976\n#&gt; valence            0.4168403\n#&gt; acousticness      -0.3150009\n#&gt; instrumentalness  -0.5664357\n#&gt; liveness           0.7459700\n#&gt; speechiness        1.0000000\ngbcorrs1 &lt;- as.data.frame(cor(gbcorr))\n\nOr you could let some packages do the viz work for you. First, the GGally package, which returns a nice matrix visualization that shows which fields are most postively and negatively correlated.\n\nggcorr(gbcorr, label = TRUE)\n\n\n\n\nWe see here some strong postive associations with energy::loundess returning a .9 coefficient, and liveness::speechiness and energy::valence each returning at .7 coefficient. The energy::acousticness and loudness::acousticness combinations each return a -.7 coefficient, showing a negative relationship between those music features.\nWith the corrr package I tried a couple of approaches. First a basic matrix that prints to the console, and doesn’t look much different than base r.\n\ngbcorr %&gt;%\n  correlate(use = \"pairwise.complete.obs\", method = \"spearman\")\n#&gt; # A tibble: 10 × 11\n#&gt;    term     duration_ms danceability energy loudness  tempo valence acousticness\n#&gt;    &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 duratio…     NA            0.0799 -0.319   -0.189 -0.439  -0.191      -0.0737\n#&gt;  2 danceab…      0.0799      NA      -0.269   -0.124 -0.323  -0.209       0.128 \n#&gt;  3 energy       -0.319       -0.269  NA        0.872  0.658   0.761      -0.725 \n#&gt;  4 loudness     -0.189       -0.124   0.872   NA      0.574   0.458      -0.595 \n#&gt;  5 tempo        -0.439       -0.323   0.658    0.574 NA       0.665      -0.479 \n#&gt;  6 valence      -0.191       -0.209   0.761    0.458  0.665  NA          -0.770 \n#&gt;  7 acousti…     -0.0737       0.128  -0.725   -0.595 -0.479  -0.770      NA     \n#&gt;  8 instrum…      0.135        0.0333 -0.447   -0.586 -0.416  -0.177       0.339 \n#&gt;  9 liveness     -0.319       -0.321   0.319    0.144  0.479   0.488      -0.103 \n#&gt; 10 speechi…     -0.331       -0.715   0.382    0.283  0.640   0.396      -0.209 \n#&gt; # ℹ 3 more variables: instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, speechiness &lt;dbl&gt;\n\nNext, I used their rplot call and then rendered a network graph using the network_plot() call.\n\ngbcorrs2 &lt;- correlate(gbcorr)\nrplot(gbcorrs2)\n\n\n\n   # network graph\ncorrelate(gbcorr) %&gt;% \n  network_plot(min_cor=0.5)\n\n\n\n\nAnd finally the `performance analytics’ package, which was the first of the packages to include significance levels in the default output.\n\n\n\n\n\nGiven the correlations, I was interested in exploring the relationships a bit more. So I ran a few scatterplots, with song titles as data labels, and dots colored by album name (using primary color from the cover) to see also if any of the albums clustered at all along either axis. The ggrepel package is used to move the labels off of the dots.\nThere is a bit of a relationship between the Energy score and Valence - so our more energetic songs are our happiest songs. Another interesting way to explore this would be to do some sentiment analysis on the lyics and see if there’s a relationship between energy, valence and using words considered to be more positive in nature. That’s a project on my to-do list.\n\ngosta_audio %&gt;%\n  ggplot(aes(energy, valence, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) +\n  ylim(0, 1) +\n  xlim(0, 1) +\n  theme_minimal() +\n  labs(x = \"energy\", y = \"valence (happiness)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nNext I wondered if there’s a relationship between song tempo (beats per minute) & happiness. Our average BPM is 131, which isn’t too far the the mid-range of songs on Spotify. The histogram below used to be on the Spotify API page but they don’t seem to have it up anywhere anymore, so found it via the Wayback Machine\nSo let’s see the resulting scatterplot…\n\n\nshow tempo x valence scatterpplot code\ngosta_audio %&gt;%\n  ggplot(aes(tempo, valence, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) + \n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"tempo (bpm)\", y = \"valence (happiness)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\nIt’s not until we get to about the 130 BPM range is it that our songs start to get to even a .25 valence (happiness) score, and from there the relationship between tempo & happiness really kicks in.\nFinally, tempo and energy…\n\n\nshow tempo x energy scatterpplot code\ngosta_audio %&gt;%\n  ggplot(aes(tempo, energy, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) +\n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"tempo (bpm)\", y = \"energy\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\nSo yes, most of our songs are in the bottom half of the happy scale. And there does seem to be a bit of a relationship between tempo, energy and happiness and of course a relationship between tempo and energy. Going forward, I’d love to explore our song lyrics via text analysis, especially sentiment analysis to see if the songs Spotify classified as our most sad (low valence) had lyrics that were less positive.\nSo if you like slightly melancholy mood-pop that’s in the 130 +/- BPM range (think The National, Radiohead), I think you’ll like us.\nThanks for reading. And again, give us a listen, and maybe buy some music if you like. :)\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/invited-talk-usf-feb2020/index.html",
    "href": "posts/invited-talk-usf-feb2020/index.html",
    "title": "Invited Talk at University of San Francisco, February 2020",
    "section": "",
    "text": "In early 2020 (back in the days of in-person gatherings) I was invited to give a talk at two budget forums at the University of San Francisco. The general theme was looking at the landscape of enrollments in higher education, with a specific focus on liberal arts colleges, especially Jesuit colleges. Because I collected data from a variety of sources and did much of the work in r, I thought it would make for a good data blog post. Plus, like the Tidy Tuesday HBCU enrollment post, it’s about higher education, which has been my area of professional expertise for a while now.\nI structured the talk around these general questions:\n\nWhat are the major trends affecting the higher education landscape in the US today, particularly traditional liberal arts colleges?\n\nChanging demographics impacting enrollments\nAffordability\n\nWhat social/political threats are on the horizon that colleges should start addressing?\n\nThe talk was divided into four segments:\n\nThe high school graduation picture in California (USF gets most of its students from CA)\nHistorical enrollment at USF, and compared to other Jesuit colleges\nCollege Costs and Affordability\n\nWhat social/political threats are on the horizon that colleges should start addressing?\n\nThe data, code and resulting presentation are in this github repo. What I plan to do in this post is in effect annotate some of the code to explain how I put everything together. And of course to show some charts & tables.\nFirst up is pulling in a few decades of high school graduation and enrollment data and wrangling it all to show historical enrollment and projections through 2029. The full code for that is at the github repo in the file 01_hs enrollment data.R. So what did I do?\nFirst, loaded some packages:\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot\nlibrary(rCAEDDATA) # aggregated CA HS data\nlibrary(readr) # read in flat files\nlibrary(janitor) # data munging/cleaning utilities\n\nThe rCAEDDATA package was put together by my former SFSU IR colleague David Ranzolin. Though the last update was October 2017, it still contains a trove of data, including public HS graduation numbers from 1993 to 2016. That said, I didn’t actually use the package functions, just downloaded the data included with the package. For later years I manually downloaded files from the CA Department of Education’s data pages: 2017 here, and the later years here. The 2017 file has the same structure as the files in David’s package. Later years need a bit of restructuring:\n\ncahsgrad18 &lt;- read.delim(\"data/cahsgrad18.txt\", stringsAsFactors=FALSE) %&gt;%\n    clean_names() %&gt;% \n    filter(reporting_category == \"TA\") %&gt;%\n    filter(aggregate_level == \"S\") %&gt;%\n    filter(dass == \"All\") %&gt;%\n    filter(charter_school == \"All\") %&gt;%\n    mutate(YEAR = \"2018\") %&gt;%\n    mutate(YEAR = factor(YEAR)) %&gt;%\n    mutate_at(vars(ends_with(\"_code\")), as.character) %&gt;%\n    mutate(county_code = ifelse(nchar(county_code) == 1, \n                str_pad(county_code, 2, \"left\", \"0\"), county_code)) %&gt;%\n    mutate(CDS_CODE = paste(county_code, district_code, school_code, sep = \"\")) %&gt;%\n    mutate(GRADS = as.integer(ifelse(regular_hs_diploma_graduates_count == \"*\", \n                0, regular_hs_diploma_graduates_count))) %&gt;%\n    mutate(UC_GRADS = as.integer(ifelse(met_uc_csu_grad_req_s_count == \"*\", \n                0, met_uc_csu_grad_req_s_count))) %&gt;%\n    select(CDS_CODE, GRADS, UC_GRADS, YEAR) %&gt;%\n    group_by(YEAR) %&gt;%\n    summarise(total_grads = sum(GRADS),\n                        uccsu = sum(UC_GRADS),\n                        notuccsu = total_grads - uccsu)\n\nNext, some projected HS graduation data from the CA Department of Finance. I did a quick transposing of the “HS Grads Table” tab in excel and used that to read into r. You can see the file I used at the github repo’s data folder.\n\ngrproj_to2028 &lt;- readxl::read_excel(\"data/capublic_k12_enrollproj_to2028.xlsx\",\n                                sheet = \"hsgrads-tr\") %&gt;%\n    filter(year != \"2017-18\") %&gt;%\n    mutate(yearend = str_sub(year, 6, 7)) %&gt;%\n    mutate(YEAR = paste(\"20\", yearend, sep = \"\")) %&gt;%\n    #mutate(YEAR = factor(year_ch)) %&gt;%\n    mutate(uccsu = as.integer(NA)) %&gt;%\n    mutate(notuccsu = as.integer(NA)) %&gt;%\n    mutate(notuccsu = as.integer(NA)) %&gt;%\n    mutate(type = \"Projected\") %&gt;%\n    select(YEAR, total_grads = total, uccsu, notuccsu, type) %&gt;%\n    # amend 2018-19 with actual results from\n    # https://dq.cde.ca.gov/dataquest/dqcensus/CohRateLevels.aspx?cds=00&agglevel=state&year=2018-19\n    mutate(total_grads = ifelse(YEAR == \"2019\", 417496, total_grads)) %&gt;%\n    mutate(uccsu = ifelse(YEAR == \"2019\", 210980, uccsu)) %&gt;%\n    mutate(notuccsu = ifelse(YEAR == \"2019\", total_grads - uccsu, notuccsu)) %&gt;%\n    mutate(type = ifelse(YEAR == \"2019\", \"Actual\", type))\n\nThe projected HS grad stats didn’t have values for UC/CSU grads, so I needed to impute that as part of the merging of the actual & projected files. I also calculated year-over-year percent changes for a few fields. (though I did it manually not with a function like in the Tidy Tuesday HBCU post). The code below gets us the cahsgrads_1993_2028 dataframe.\n\ncahsgrads_1993_2028 &lt;- rbind(cahsgrad93to18_tot, grproj_to2028) %&gt;%\n    mutate(pctucgrads = uccsu / total_grads) %&gt;%\n    arrange(YEAR) %&gt;%\n    # add projected uccsu grads based on constant 2017-18 to 2018-19 increase 0.0061437\n    mutate(pctucgrads = ifelse(YEAR &gt;= \"2020\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2021\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2022\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2023\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2024\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2025\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2026\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2027\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2028\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2029\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(uccsu = ifelse(type == \"Projected\", round(pctucgrads * total_grads, 0), uccsu)) %&gt;%\n    mutate(notuccsu = ifelse(type == \"Projected\", round(total_grads -uccsu, 0), notuccsu)) %&gt;%\n    mutate(gr_tot_change = (total_grads - lag(total_grads))) %&gt;%\n    mutate(gr_tot_pct_change = (total_grads/lag(total_grads)- 1)) %&gt;%\n    mutate(gr_uc_change = (uccsu - lag(uccsu))) %&gt;%\n    mutate(gr_uc_pct_change = (uccsu/lag(uccsu) - 1)) %&gt;%\n    mutate(gr_notuc_change = (notuccsu - lag(notuccsu))) %&gt;%\n    mutate(gr_notuc_pct_change = (notuccsu/lag(notuccsu) - 1)) %&gt;%\n    select(YEAR, total_grads, uccsu, notuccsu, type, pctucgrads, type, everything())\n\ncahsgrads_1993_2028 &lt;- cahsgrads_1993_2028 %&gt;%\n    mutate(pctucgrads = ifelse(year_ch &gt;= \"9293\", uccsu / total_grads, pctucgrads))\n\nNow that we have the data, let’s make the chart I presented to the group, showing actual & projected high school grads in California, breaking out the UC/CSU eligible grads.\nWhat does the chart tell us? Well…\n\n67% increase in grads from 1993 to 2018\nUC/CSU eligibility 33% in 1993, 50% in 2018\nGrads expected to peak in 2023, then decline slightly\n\nBut these assumptions were all pre-COVID - I gave the talks in early February of 2020. Given factors such as migration patterns within the state, people moving out of CA, parents moving their kids to private schools, etc., the actual graduation picture is sure to change.\n\n\nshow the enrollment charts code\ncahsgrads_1993_2028 %&gt;%\n    select(YEAR, uccsu, notuccsu) %&gt;%\n    pivot_longer(-YEAR, names_to = \"ucelig\", values_to = \"n\") %&gt;%\n    ggplot(aes(YEAR, n, fill = rev(ucelig))) +\n    geom_bar(stat = \"identity\", color = \"black\") +\n    geom_segment(aes(x = 27.5, y = 0, xend = 27.5, yend = 500000),\n                             size = 2, color = \"grey\") +\n    scale_y_continuous(labels = scales::comma, limits = c(0, 500000)) + \n    scale_x_discrete(breaks = c(\"9293\", \"9798\", \"0203\", \"0708\", \"1213\", \"1718\", \"2223\", \"2829\")) +\n    scale_fill_manual(values = c(\"#1295D8\", \"white\"),\n                                        labels = c(\"UC CSU Eligible\", \"Not UC/CSU Elig\")) +\n    labs(x = \"\", y = \"\", caption = \"Sources: Actual: CA Dept of Education. Projections: CA Dept of Finance\",\n             fill = \"UC/CSU Eligible?\") +\n    annotate(\"text\", x = 28, y = 500000, label = \"Projected\",\n                     size = 6, fontface = \"italic\", hjust = -.25) +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(),   panel.grid.minor = element_blank(),\n                legend.position = c(.2, .9),\n                legend.title=element_text(size=12), legend.text=element_text(size=12),\n                axis.text.x = element_text(size = 7))\n\n\n\n\n\nNext I presented some data on USF enrollment and compared USF to their Jesuit college peers. For that I used two sources: data from the Delta Cost Project (DCP), and since DCP stops at 2015, downloaded some files directly from IPEDS and read in the CSVs. I could also have used r tools like the Urban Insitute’s educationdata package an API wrapper to that scrapes NCES and other websites for data. (I also referenced that package in my HBCU post).\nThe import code isn’t all that challenging - read in the CSV, select the fields I needed, do a bit of basic cleaning. So no need to show it. You can see it in the github repo - go to the file ’02_ipeds_enroll.R`. Though if anything’s worth highlighting it’s the need to create an object of IPEDS unitids for Jesuit colleges so I could group them during analysis. The Jesuit colleges include names you know: Georgetown, Gonazaga, Boston College, the Loyolas (Chicago, LA, New Orleans, Baltimore), etc…\n\njesids &lt;- c(\"164924\", \"181002\", \"186432\", \"122931\", \"169716\", \"159656\", \"127918\", \"192323\", \n                    \"163046\", \"122612\", \"236595\", \"239105\", \"203368\", \"179159\", \"215770\", \n                        \"215929\", \"131496\", \"166124\", \"102234\", \"117946\", \"206622\", \"102234\", \n                        \"166124\", \"117946\", \"206622\", \"235316\", \"129242\")\n\nFirst up is USF enrollment from Fall 1987 to Fall 2018 (the latest year that IPEDS had available as of February 2020). The ggplot code is mostly basic, so I’ve folded it…click the arrow to show the code. It’s mostly worth checking out for this neat solution to a crowded x axis - the every_nth function to count every n value, and apply it to the breaks - in this case I set it to n=3. I’d tried scales::pretty_breaks() but it didn’t work. I also used ggrepel to move the labels a bit.\nWhat’s the enrollment picture at USF? Well, this chart tells us that:\n\nUndergraduate enrollment have increased by 60% since 1987.\nGraduate enrollments hovering around 3,500 for a number of years, and up to +/- 4,000 since 2016.\nRatio of undergraduate::graduate enrollments steady over time, generally +/- 2% points from 60%.\n\nHow does this relate to the high school graduation trends in CA & nearby states?\n\nWith 63% of new students coming from California, high school enrollments here will have most impact.\nWestern Interstate Commission for Higher Education (WICHE) projects HS grads from all western states to peak in 2024 at 862,000, then decline for a few years, rebounding again around 2032. (Knocking at the College Door, https://knocking.wiche.edu)\nDuring 1980s the Gen X population drop mitigated by increased college-going rates – what will happen this time?\n\n\n\nshow the enrollment charts code\nevery_nth = function(n) {\n  return(function(x) {x[c(TRUE, rep(FALSE, n - 1))]})\n}\n\nplot_usfenr_ug &lt;-\n    ipeds_fallenroll_8718 %&gt;%\n    filter(UNITID == \"122612\", level == \"Undergraduate\") %&gt;%\n    select(year, level, tot_enr) %&gt;%\n    ggplot(aes(year, tot_enr)) +\n    geom_bar(stat = \"identity\", fill = \"#00543C\") +\n    # geom_text(aes(label = scales::comma(round(tot_enr), accuracy = 1)), \n    #                   color = \"#919194\", vjust = -.75, size = 3) +\n    geom_text_repel(data = ipeds_fallenroll_8718 %&gt;%\n                                        filter(UNITID == \"122612\", level == \"Undergraduate\",\n                                        year %in% c(\"Fall 1987\", \"Fall 1990\", \"Fall 1990\", \n                                        \"Fall 1993\", \"Fall 1996\", \"Fall 1999\",\n                                        \"Fall 2002\", \"Fall 2005\", \"Fall 2008\", \"Fall 2011\", \n                                        \"Fall 2014\", \"Fall 2017\", \"Fall 2018\")),\n                                    aes(label = scales::comma(round(tot_enr), \n                                            accuracy = 1)), nudge_y = 400,\n                                        min.segment.length = 0,\n                                        size = 3, color = \"#919194\") +\n    scale_x_discrete(breaks = every_nth(n = 3)) +\n    scale_y_continuous(label = scales::comma, limits = c(0, 7000),\n                                         breaks = c(0, 1750, 3500, 5250, 7000)) +\n    labs(x = \"\", y = \"\") +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(),   panel.grid.minor = element_blank(),\n                axis.text.y = element_text(size = 10))\n\nplot_usfenr_gr &lt;-\n    ipeds_fallenroll_8718 %&gt;%\n    filter(UNITID == \"122612\", level == \"Graduate\") %&gt;%\n    select(year, level, tot_enr) %&gt;%\n    ggplot(aes(year, tot_enr)) +\n    geom_bar(stat = \"identity\", fill = \"#FDBB30\") +\n    geom_text_repel(data = ipeds_fallenroll_8718 %&gt;%\n                                        filter(UNITID == \"122612\", level == \"Graduate\",\n                                     year %in% c(\"Fall 1987\", \"Fall 1990\", \"Fall 1990\", \n                                   \"Fall 1993\", \"Fall 1996\", \"Fall 1999\",\n                                   \"Fall 2002\", \"Fall 2005\", \"Fall 2008\", \"Fall 2011\", \n                                   \"Fall 2014\", \"Fall 2017\", \"Fall 2018\")),\n                                    aes(label = scales::comma(round(tot_enr), \n                                                            accuracy = 1)), nudge_y = 200,\n                                    min.segment.length = 0,\n                                    size = 3, color = \"#919194\") +\n    scale_x_discrete(breaks = every_nth(n = 3)) +\n    scale_y_continuous(label = scales::comma, limits = c(0, 4500),\n                                         breaks = c(0, 1500, 3000, 4500)) +\n    labs(x = \"\", y = \"\") +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(),   panel.grid.minor = element_blank(),\n                axis.text.y = element_text(size = 10))\n\nplot_usfenr_all &lt;- plot_usfenr_ug + plot_usfenr_gr +\n    plot_layout(ncol = 1) + plot_annotation(\n    title = 'Fall Enrollment at USF: 1987 - 2018',\n    subtitle = \"&lt;span style = 'color:#00543C;'&gt;Green = Undergraduate&lt;/span&gt;, \n    &lt;span style = 'color:#FDBB30;'&gt;Gold = Graduate&lt;/span&gt;\",\n    caption = \"Sources: Delta Cost Project & IPEDS\", \n    theme = theme(plot.subtitle = element_markdown(face = \"italic\", size = 9)))\nplot_usfenr_all\n\n\n\n\n\nQuick note about this plot…I was getting an annoying Error in grid.Call(C_textBounds, as.graphicsAnnot(x\\(label), x\\)x, x$y, : polygon edge not found message trying to run these plots. For the presentation I was using the Calibri font, with this call in the theme() section: text = element_text(family = \"Calibri\"). I removed that & the error went away. But this after trying everything from reinstalling Quartz, shutting down all browser windows, running `dev.off()’ in the console…got rid of the special font & no error.\nAnyway…back to the charts.\nI wanted to show USF undergraduate enrollment indexed over time relative to their Jesuit college peers. The final version of the chart is below, complete with annotations I added in power point. There are ways to do similar annotations on r; I used power point because I could do it quicker, with less fuzting around with annotation placement after rendering and saving the image.\nWe see that USF is in the upper quarter of total enrollment growth. Gonzaga & St. Louis University, two schools well-known thanks to success in the NCAA men’s basketball tournament, showed significant growth in the period. Here you might say “but wait, Georgetown has had NCAA success”, and I’d reply “yes, but their success started before 1987, so within this period didn’t grow as much as Gonzaga & St. Louis”.\n\n\n\nJesuit College Enrollment\n\n\nSo how did I make this chart? How did we get the green line for USF, with all else in gray?\nFirst I created a dataframe of the Jesuit colleges, and indexed changes in enrollment to 1.\n\nenrollindex_jes &lt;-\nipeds_fallenroll_8718 %&gt;%\n    filter(jescoll == 1, level == \"Undergraduate\") %&gt;%\n  mutate(enr_pct_change2 = enr_pct_change / 100) %&gt;%\n    mutate(enr_pct_change2 = ifelse(year == \"Fall 1987\", 1, enr_pct_change2)) %&gt;%\n    arrange(UNITID, year) %&gt;%\n    group_by(UNITID) %&gt;%\n    mutate(index_enr_inst = 1) %&gt;%\n    mutate(index_enr_inst = ifelse(year &gt;= \"Fall 1988\", cumsum(enr_pct_change2),\n                                                                 index_enr_inst)) %&gt;%\n    ungroup() %&gt;%\n    ## fix loyola NO b/c of enroll drop after katrina\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year == \"Fall 2006\"), \n                                                    0.833399497, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2007\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2008\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2009\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2010\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2011\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2012\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2013\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2014\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2015\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2016\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2017\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2018\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    select(UNITID, inst_name, year, tot_enr, enr_change, enr_pct_change, \n                 enr_pct_change2, index_enr_inst)\n\nWhy all the manual fixes to Loyola in New Orleans? Well, look at the chart again. See the dip in enrollment around 2005? What might have tanked enrollment at a New Orleans-based college in 2005? Oh right…Hurricane Katrina. To smooth out the drop, I reindexed from the 2006 enrollment point, and added to the index sum after that. For some reason I couldn’t identify, the usual lag from prior year wasn’t working so I just did it manually.\nAs for the plot…to get the green line, I first plotted everything but USF in grey, then plotted USF in green. Saved the plot, then added annotations in power point.\n\nggplot(enrollindex_jes, aes(year, index_enr_inst, group = UNITID)) +\n    geom_line(data = subset(enrollindex_jes, UNITID != \"122612\"), color = \"grey\") +\n    geom_line(data = subset(enrollindex_jes, UNITID == \"122612\"), \n        color = \"#00543C\", size = 1) +\n    scale_y_continuous(limits = c(-.5, 2),\n        breaks = c(-.5, 0, .5, 1, 1.5, 2)) +\n    labs(x = \"\", y = \"\") +\n    theme_minimal() +\n    theme(#text = element_text(family = \"Calibri\"),\n                panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n                axis.text.y = element_text(size = 14))\n\nI had planned to use the Delta Cost/IPEDS tuition & fees data for some charts in the section on affordability, but it worked out better to pull data from the College Board.\nAgain, you can access the presentation in the github repo. If you have questions or comments about the code or the content, you can find me on LinkedIn or Twitter by clicking on the icon links at the bottom of the post or on the main page. Or send me an email if you already know how to find me that way."
  },
  {
    "objectID": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html",
    "href": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html",
    "title": "Tidy Tuesday, April 07, 2020 - Le Tour! (Stage 1, cleaning the data)",
    "section": "",
    "text": "| \n\nA mountain of data about the Tour de France…\nHaving looked at hiking trails in Washington state and bridges in Maryland I poked around the #TidyTuesday repo and saw this trove of data from back in April on The Tour de France. I love this race, I cycle for exercise, and I love the Kraftwerk album, so of course I had to dig in.\nSo I don’t bury the lede, this is a two-part post. Why? Because there was a lot of data munging & cleaning needed to get the data into shape for whart I wanted to do. So this post is all about what I needed to do on that end. The analysis post will come soon. Also, I’m trying to work out how to do a code show/hide thing in hugo academic so bear with me that the code takes up lots of pixels.\n(*update - migrating to ‘Quarto’ means a native code-fold feature…hooray!)\nSo let’s dig in…first we’ll load packages and create a ’%notin% operator…\n\n# load packages\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(tdf) # to get original stage results file\n\n# create notin operator to help with cleaning & analysis\n`%notin%` &lt;- negate(`%in%`)\n\nThere’s a ton of data here, sourced from the tdf package from Alastair Rushworth and (Thomas Camminady’s data set) (https://github.com/camminady/LeTourDataSet), via Kaggle\nThere are three distinct sets to work thru, each going back to the first run of the race in 1903:\n* A dataframe of overall (General Classification, or Yellow Jersey / maillot jaune) winners from 1903 to 2019 comes from the Tidy Tuesday frame.\n* A dataframe with stage winners for races 1903 to 2017, also in the Tidy Tuesday set, sourced from Kaggle.\n* A frame of overall stage results, sourced from the tdf pacakge due to issues with date conversion in the data included in the Tidy Tuesday set.\nThe stage winner set needs a bit of mungung…I created a stage_results_id column similar to the one in the stage results set. But it needs leading zeros for stages 1-9 so it sorts properly.\nI then got it in my head I wanted results through 2020, so I grabbed them from wikipedia; but the hard way, with copy-paste since my scraping skills aren’t there & I just wanted it done. Data is uploaded to my github repo if you want to use it. (yes, it’s in an excel file…)\n\n\nShow tdf data cleaning pt1\n# load main file from tt repo\ntt_tdf &lt;- tidytuesdayR::tt_load('2020-04-07')\n\n\n\n    Downloading file 1 of 3: `stage_data.csv`\n    Downloading file 2 of 3: `tdf_stages.csv`\n    Downloading file 3 of 3: `tdf_winners.csv`\n\n\nShow tdf data cleaning pt1\n# create race winners set. comes from tdf package. includes up to 2019\ntdf_winners &lt;- as_tibble(tt_tdf$tdf_winners)\n\n# create stage winner set. in tt file, comes from kaggle, includes up to 2017\ntdf_stagewin1 &lt;- tt_tdf$tdf_stages %&gt;%\n  mutate_if(is.character, str_trim)\n  \n# pulled 2018 - 2020 from wikipedia\n# read in excel - need to separate route field to Origin & Destination\ntdf_stagewin2 &lt;- readxl::read_excel(\"data/tdf_stagewinners_2018-20.xlsx\") %&gt;%\n  mutate(Stage = as.character(Stage)) %&gt;%\n  mutate(Date = lubridate::as_date(Date)) %&gt;% \n  separate(Course, c(\"Origin\", \"Destination\"), \"to\", extra = \"merge\") %&gt;%\n  mutate_if(is.character, str_trim) %&gt;%\n  select(Stage, Date, Distance, Origin, Destination, Type, Winner, Winner_Country = Winner_country)\n\n# join with rbind (since I made sure to put 2018-2020 data in same shape as tt set)\n# clean up a bit\ntdf_stagewin &lt;- rbind(tdf_stagewin1, tdf_stagewin2) %&gt;%\n  mutate(race_year = lubridate::year(Date)) %&gt;% \n  mutate(Stage = ifelse(Stage == \"P\", \"0\", Stage)) %&gt;%\n  mutate(stage_ltr = case_when(str_detect(Stage, \"a\") ~ \"a\",\n                               str_detect(Stage, \"b\") ~ \"b\",\n                               str_detect(Stage, \"c\") ~ \"c\",\n                               TRUE ~ \"\")) %&gt;%\n  mutate(stage_num = str_remove_all(Stage, \"[abc]\")) %&gt;%\n  mutate(stage_num = stringr::str_pad(stage_num, 2, side = \"left\", pad = 0)) %&gt;% \n  mutate(stage_results_id = paste0(\"stage-\", stage_num, stage_ltr)) %&gt;%\n  mutate(split_stage = ifelse(stage_ltr %in% c(\"a\", \"b\", \"c\"), \"yes\", \"no\")) %&gt;%\n  \n  # extract first and last names from winner field\n  mutate(winner_first = str_match(Winner, \"(^.+)\\\\s\")[, 2]) %&gt;%\n  mutate(winner_last= gsub(\".* \", \"\", Winner)) %&gt;%\n\n  # clean up stage types, collapse into fewer groups\n  mutate(stage_type = case_when(Type %in% c(\"Flat cobblestone stage\", \"Flat stage\", \"Flat\",\n                                            \"Flat Stage\", \"Hilly stage\", \"Plain stage\", \n                                            \"Plain stage with cobblestones\") \n                                ~ \"Flat / Plain / Hilly\",\n                                Type %in% c(\"High mountain stage\", \"Medium mountain stage\",\n                                            \"Mountain stage\", \"Mountain Stage\", \"Stage with mountain\",\n                                            \"Stage with mountain(s)\", \"Transition stage\")\n                                ~ \"Mountain\",\n                                Type %in% c(\"Individual time trial\", \"Mountain time trial\") \n                                ~ \"Time Trail - Indiv\",\n                                Type == \"Team time trial\" ~ \"Time Trail - Team\",\n                                TRUE ~ \"Other\")) %&gt;% \n  mutate_if(is.character, str_trim) %&gt;%\n  arrange(desc(race_year), stage_results_id) %&gt;%\n  select(race_year, stage_results_id, stage_date = Date, stage_type, Type, split_stage,\n         Origin, Destination, Distance, Winner, winner_first, winner_last,\n         Winner_Country, everything())\n\n# take a look at this awesome dataset\nglimpse(tdf_stagewin)\n\n\nRows: 2,299\nColumns: 16\n$ race_year        &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020,…\n$ stage_results_id &lt;chr&gt; \"stage-01\", \"stage-02\", \"stage-03\", \"stage-04\", \"stag…\n$ stage_date       &lt;date&gt; 2020-08-29, 2020-08-30, 2020-08-31, 2020-09-01, 2020…\n$ stage_type       &lt;chr&gt; \"Flat / Plain / Hilly\", \"Mountain\", \"Flat / Plain / H…\n$ Type             &lt;chr&gt; \"Flat stage\", \"Medium mountain stage\", \"Flat stage\", …\n$ split_stage      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ Origin           &lt;chr&gt; \"Nice\", \"Nice\", \"Nice\", \"Sisteron\", \"Gap\", \"Le Teil\",…\n$ Destination      &lt;chr&gt; \"Nice\", \"Nice\", \"Sisteron\", \"Orcières-Merlette\", \"Pri…\n$ Distance         &lt;dbl&gt; 156.0, 186.0, 198.0, 160.5, 183.0, 191.0, 168.0, 141.…\n$ Winner           &lt;chr&gt; \"Alexander Kristoff\", \"Julian Alaphilippe\", \"Caleb Ew…\n$ winner_first     &lt;chr&gt; \"Alexander\", \"Julian\", \"Caleb\", \"Primož\", \"Wout van\",…\n$ winner_last      &lt;chr&gt; \"Kristoff\", \"Alaphilippe\", \"Ewan\", \"Roglič\", \"Aert\", …\n$ Winner_Country   &lt;chr&gt; \"NOR\", \"FRA\", \"AUS\", \"SLO\", \"BEL\", \"KAZ\", \"BEL\", \"FRA…\n$ Stage            &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"1…\n$ stage_ltr        &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ stage_num        &lt;chr&gt; \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\",…\n\n\nStage data in CSV from the tidy tuesday repository seems to have truncated the times, leaving only the seconds in a character field. To get complete results we need to pull from tdf package using the cleaning script from the Tidy Tuesday page. Some operations will take a while, so best to run as a background job if you want to do something else while it runs. Or go get a cup of coffee.\nIn terms of cleaning:\n* The stage_results_id & rank fields needs leading zeros.\n* The rank field needs a bit of clean-up to fix the 1000s codes.\n* Since rider names were last-first, I wanted to separate out first and last, and also make a field with the full name, but first name in front. Stackoverlflow was my regex friend here.\n* Other minor fixes\nIn the process of cleaning and comparing to the stage winners set, I noticed there were some problems in years where individual stages were split into 2 or 3 legs (A, B & C). Either while it was scraped or combined, the A leg results ended up repeating to the B leg, and in some cases the C leg wasn’t reported. I put it in as an issue in the github repo. But that shouldn’t take away from what’s an amazing dataset to work with. In the analysis section I’ll work around the problems with those stages.\n\n\nShow tdf data cleaning pt2\nall_years &lt;- tdf::editions %&gt;%\n  unnest_longer(stage_results) %&gt;%\n  mutate(stage_results = map(stage_results, ~ mutate(.x, rank = as.character(rank)))) %&gt;%\n  unnest_longer(stage_results)\n\nstage_all &lt;- all_years %&gt;%\n  select(stage_results) %&gt;%\n  flatten_df()\n\ncombo_df &lt;- bind_cols(all_years, stage_all) %&gt;%\n  select(-stage_results)\n\ntdf_stagedata &lt;- as_tibble(combo_df %&gt;%\n  select(edition, start_date,stage_results_id:last_col()) %&gt;%\n  mutate(race_year = lubridate::year(start_date)) %&gt;%\n  rename(age = age...25) %&gt;%\n\n  # to add leading 0 to stage, extract num, create letter, add 0s to num, paste\n  mutate(stage_num = str_replace(stage_results_id, \"stage-\", \"\")) %&gt;%\n  mutate(stage_ltr = case_when(str_detect(stage_num, \"a\") ~ \"a\",\n                               str_detect(stage_num, \"b\") ~ \"b\",\n                               TRUE ~ \"\"))) %&gt;%\n  mutate(stage_num = str_remove_all(stage_num, \"[ab]\")) %&gt;%\n  mutate(stage_num = stringr::str_pad(stage_num, 2, side = \"left\", pad = 0)) %&gt;%\n  mutate(stage_results_id2 = paste0(\"stage-\", stage_num, stage_ltr)) %&gt;%\n  mutate(split_stage = ifelse(stage_ltr %in% c(\"a\", \"b\"), \"yes\", \"no\")) %&gt;%\n\n  # fix 1000s rank. change to DNF\n  mutate(rank = ifelse(rank %in% c(\"1003\", \"1005\", \"1006\"), \"DNF\", rank)) %&gt;%\n  mutate(rank2 = ifelse(rank %notin% c(\"DF\", \"DNF\", \"DNS\", \"DSQ\",\"NQ\",\"OTL\"),\n                        stringr::str_pad(rank, 3, side = \"left\", pad = 0), rank)) %&gt;%\n\n  # extract first and last names from rider field\n  mutate(rider_last = str_match(rider, \"(^.+)\\\\s\")[, 2]) %&gt;%\n  mutate(rider_first= gsub(\".* \", \"\", rider)) %&gt;%\n  mutate(rider_firstlast = paste0(rider_first, \" \", rider_last)) %&gt;%\n  select(-stage_results_id, -start_date, ) %&gt;%\n\n  # fix 1967 & 1968\n  mutate(stage_results_id2 = ifelse((race_year %in% c(1967, 1968) & stage_results_id2 == \"stage-00\"),\n         \"stage-01a\", stage_results_id2)) %&gt;%\n  mutate(stage_results_id2 = ifelse((race_year %in% c(1967, 1968) & stage_results_id2 == \"stage-01\"),\n         \"stage-01b\", stage_results_id2)) %&gt;%\n  mutate(split_stage = ifelse((race_year %in% c(1967, 1968) &\n                                 stage_results_id2 %in% c(\"stage-01a\", \"stage-01b\")),\n                              \"yes\", split_stage)) %&gt;%\n\n  select(edition, race_year, stage_results_id = stage_results_id2, split_stage,\n         rider, rider_first, rider_last, rider_firstlast, rank2,\n         time, elapsed, points, bib_number, team, age, everything())\n\nsaveRDS(tdf_stagedata, \"data/tdf_stagedata.rds\")\n\n\n\ntdf_stagedata &lt;- readRDS(\"data/tdf_stagedata.rds\")\nglimpse(tdf_stagedata)\n\nRows: 255,752\nColumns: 18\n$ edition          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ race_year        &lt;dbl&gt; 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903,…\n$ stage_results_id &lt;chr&gt; \"stage-01\", \"stage-01\", \"stage-01\", \"stage-01\", \"stag…\n$ split_stage      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ rider            &lt;chr&gt; \"Garin Maurice\", \"Pagie Émile\", \"Georget Léon\", \"Auge…\n$ rider_first      &lt;chr&gt; \"Maurice\", \"Émile\", \"Léon\", \"Fernand\", \"Jean\", \"Marce…\n$ rider_last       &lt;chr&gt; \"Garin\", \"Pagie\", \"Georget\", \"Augereau\", \"Fischer\", \"…\n$ rider_firstlast  &lt;chr&gt; \"Maurice Garin\", \"Émile Pagie\", \"Léon Georget\", \"Fern…\n$ rank2            &lt;chr&gt; \"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008…\n$ time             &lt;Period&gt; 17H 45M 13S, 55S, 34M 59S, 1H 2M 48S, 1H 4M 53S, 1…\n$ elapsed          &lt;Period&gt; 17H 45M 13S, 17H 46M 8S, 18H 20M 12S, 18H 48M 1S, …\n$ points           &lt;int&gt; 100, 70, 50, 40, 32, 26, 22, 18, 14, 10, 8, 6, 4, 2, …\n$ bib_number       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ team             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ age              &lt;int&gt; 32, 32, 23, 20, 36, 37, 25, 33, NA, 22, 26, 28, 21, 2…\n$ rank             &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"1…\n$ stage_num        &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n$ stage_ltr        &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n\n\nPoking around the Kaggle site referenced above I found these datasets of final results for all riders in all races since 1903. A few different fields than in the tidy tuesday winners set.\nNow this is a ton of data to work with, and I won’t use it all. Figured I’d include the code to get it all in case you get inspired to grab it and take a look.\nOk…that’s it for cleaning & prepping…charts and tables in Stage 2.\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/tidy-tuesday-apr-07-2020-LeTour-stage2/index.html",
    "href": "posts/tidy-tuesday-apr-07-2020-LeTour-stage2/index.html",
    "title": "Tidy Tuesday, April 07, 2020 - Le Tour! (Stage 2, charts!)",
    "section": "",
    "text": "Back in the saddle for Stage 2 of the Tour de France data ride\nStage 1 ended up being all about wrangling and cleaning the #TidyTuesday Tour de France data. When I first dug into the data I wasn’t sure what I wanted to visualize. It wasn’t until I spent some time living with the data, seeing what was there and looking at the #tidytuesday TdF submissions on Twitter so I didn’t repeat what was done that I decided I wanted to look at results by stage, specifically the gaps between the winners of each stage and the times recorded for the next-best group and the last rider(s) across the line. Charlie Gallagher took a similar approach at the data, using overall race results for the GC riders.\nA quick but important aside - in the Tour, as in most (all?) UCI races, while each rider is accorded a place - 1, 2, 3, etc… - times are calculated by identifiable groups crossing the line. So let’s say you are 2nd to 15th in the 1st group (of 15 total riders) that crosses with barely any daylight between riders; you each get the same time as the winner. But only 1 rider wins the stage. In any stage, there could be only 2 or 3 identifiable time groups, or there could be many groups. Depends on the stage type and other factors - crashes, where in the race the stage took place, etc…\nWhat this means for my project here is I needed to wrangle data so that I was able to identify two time groups apart from the winner; the next best group and the last group. Each group could have more than 1 rider. Download and clean the stage results data and you’ll see what I mean.\nSo let’s look at some code and charts.\nAt the end of Stage 1 we had a number of data frames. I’m joining two for this analysis, one with stage winners (which has important stage characteristic data) and a set of all riders in every stage from 1903 to 2019. We’ll first load the packages we need…\n\n# load packages\nlibrary(tidyverse) # to do tidyverse things\nlibrary(lubridate) # to do things with dates & times\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # to stitch together plots\n\n# create notin operator to help with cleaning & analysis\n`%notin%` <- negate(`%in%`)\n\nThen join the sets. For the purposes of this post I’ll just load an RDS I created (it’s not uploaded to the repo, sorry, but you can recreate it with the code.\n\ntdf_stageall <- merge(tdf_stagedata, tdf_stagewin, by.x = c(\"race_year\", \"stage_results_id\"),\n                      by.y = c(\"race_year\", \"stage_results_id\"), all = T)\n\n\ntdf_stageall <- readRDS(\"data/tdf_stageall.rds\")\nglimpse(tdf_stageall)\n\nRows: 255,807\nColumns: 32\n$ race_year        <dbl> 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903,…\n$ stage_results_id <chr> \"stage-01\", \"stage-01\", \"stage-01\", \"stage-01\", \"stag…\n$ edition          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ split_stage.x    <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ rider            <chr> \"Garin Maurice\", \"Pagie Émile\", \"Georget Léon\", \"Auge…\n$ rider_first      <chr> \"Maurice\", \"Émile\", \"Léon\", \"Fernand\", \"Jean\", \"Marce…\n$ rider_last       <chr> \"Garin\", \"Pagie\", \"Georget\", \"Augereau\", \"Fischer\", \"…\n$ rider_firstlast  <chr> \"Maurice Garin\", \"Émile Pagie\", \"Léon Georget\", \"Fern…\n$ rank2            <chr> \"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008…\n$ time             <Period> 17H 45M 13S, 55S, 34M 59S, 1H 2M 48S, 1H 4M 53S, 1…\n$ elapsed          <Period> 17H 45M 13S, 17H 46M 8S, 18H 20M 12S, 18H 48M 1S, …\n$ points           <int> 100, 70, 50, 40, 32, 26, 22, 18, 14, 10, 8, 6, 4, 2, …\n$ bib_number       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ team             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ age              <int> 32, 32, 23, 20, 36, 37, 25, 33, NA, 22, 26, 28, 21, 2…\n$ rank             <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"1…\n$ stage_num.x      <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n$ stage_ltr.x      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ stage_date       <date> 1903-07-01, 1903-07-01, 1903-07-01, 1903-07-01, 1903…\n$ stage_type       <chr> \"Flat / Plain / Hilly\", \"Flat / Plain / Hilly\", \"Flat…\n$ Type             <chr> \"Plain stage\", \"Plain stage\", \"Plain stage\", \"Plain s…\n$ split_stage.y    <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ Origin           <chr> \"Paris\", \"Paris\", \"Paris\", \"Paris\", \"Paris\", \"Paris\",…\n$ Destination      <chr> \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon…\n$ Distance         <dbl> 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467…\n$ Winner           <chr> \"Maurice Garin\", \"Maurice Garin\", \"Maurice Garin\", \"M…\n$ winner_first     <chr> \"Maurice\", \"Maurice\", \"Maurice\", \"Maurice\", \"Maurice\"…\n$ winner_last      <chr> \"Garin\", \"Garin\", \"Garin\", \"Garin\", \"Garin\", \"Garin\",…\n$ Winner_Country   <chr> \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA…\n$ Stage            <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ stage_ltr.y      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ stage_num.y      <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n\n\nThis set has many columns that we’ll build off of to use in analysis going forward. To get the changes in gaps by stage types, we’ll build another set. Because we want to look both at changes in stage types and gaps between winners and the field, the trick here is to sort out for each stage in each race year who the winners are (easy), who has the slowest time (mostly easy) and who has the 2nd best record time.\nThat last item it tough because of the time & rank method I described above. The script below is commented to show why I did what I did. Much of the code comes from looking at the data and seeing errors, issues, etc. Not including that code here. Also, much of my ability to spot errors comes from knowledge about the race, how it’s timed, some history. Domain knowledge helps a lot when cleaning & analyzing data.\n\nstage_gap <-\ntdf_stageall %>%\n  arrange(race_year, stage_results_id, rank2) %>%\n  #  delete 1995 stage 16 - neutralized due to death in stage 15, all times the same\n  mutate(out = ifelse((race_year == 1995 & stage_results_id == \"stage-16\"),\n                       \"drop\", \"keep\")) %>%\n  filter(out != \"drop\") %>%\n  # delete  missing times\n  filter(!is.na(time)) %>%\n  # remove non-finishers/starters, change outside time limit rank to numeric to keep in set\n  filter(rank %notin% c(\"DF\", \"DNF\", \"DNS\", \"DSQ\", \"NQ\")) %>%\n  filter(!is.na(rank)) %>%\n\n  # OTLs are ejected from the race because they finished outside a time limit. But we need them in the set.\n  mutate(rank_clean = case_when(rank == \"OTL\" ~ \"999\",\n                           TRUE ~ rank)) %>% \n  # sortable rank field\n  mutate(rank_n = as.integer(rank_clean)) %>%\n  # creates total time in minutes as numeric, round it to 2 digits\n  mutate(time_minutes = ifelse(!is.na(elapsed),\n                              day(elapsed)*1440 + hour(elapsed)*60 + minute(elapsed) + second(elapsed)/60,\n                               NA)) %>%\n  mutate(time_minutes = round(time_minutes, 2)) %>%\n  \n  # create field for difference from winner\n  group_by(race_year, stage_results_id) %>% \n  arrange(race_year, stage_results_id, time_minutes, rank2) %>%\n\n  mutate(time_diff = time_minutes - min(time_minutes)) %>%\n  mutate(time_diff_secs = time_diff*60) %>%\n  mutate(time_diff = round(time_diff, 2)) %>%\n  mutate(time_diff_secs = round(time_diff_secs, 0)) %>%\n  mutate(time_diff_period = seconds_to_period(time_diff_secs)) %>%\n  mutate(rank_mins = rank(time_minutes, ties.method = \"first\")) %>%\n  # create rank field to use to select winner, next best, last\n  mutate(compare_grp = case_when(rank_n == 1 ~ \"Winner\",\n                                 (rank_n > 1 & time_diff_secs > 0 & rank_mins != max(rank_mins))\n                                 ~ \"Next best2\",\n                                  rank_mins == max(rank_mins) ~ \"Last\",\n                                 TRUE ~ \"Other\")) %>%\n  ungroup() %>%\n  group_by(race_year, stage_results_id, compare_grp) %>% \n  arrange(race_year, stage_results_id, rank_mins) %>%\n  mutate(compare_grp = ifelse((compare_grp == \"Next best2\" & rank_mins == min(rank_mins)),\n                               \"Next best\", compare_grp)) %>%\n  mutate(compare_grp = ifelse(compare_grp == \"Next best2\", \"Other\", compare_grp)) %>%\n  ungroup() %>%\n  mutate(compare_grp = factor(compare_grp, levels = c(\"Winner\", \"Next best\", \"Last\", \"Other\"))) %>%\n  # create race decade field\n  mutate(race_decade = floor(race_year / 10) * 10) %>%\n  mutate(race_decade = as.character(paste0(race_decade, \"s\"))) %>%\n  # keep only winner, next, last\n  filter(compare_grp != \"Other\") %>%\n  select(race_year, race_decade, stage_results_id, stage_type, rider_firstlast, bib_number, Winner_Country,\n         rank, rank_clean, rank_n, time, elapsed, time_minutes, time_diff, time_diff_secs, time_diff_period, \n         rank_mins, compare_grp) \n\nOk, finally, let’s see what this data looks like. First a chart to show averages and quartile ranges for the gaps by stage type. Create a data object with the values, then the plots. Faceting by stage type didn’t work because the y axis ranges were very different. So we’ll use patchwork to stitch them together in one plot. The medians are the red dots, interquartile ranges at either end of the line, and means are in black. I included both means & medians because the spread for some stage types was so great.\n\n\nShow stage gap charts code\ngapranges <- stage_gap %>%\n  filter(compare_grp != \"Winner\") %>%\n  filter(stage_type %notin% c(\"Other\", \"Time Trial - Team\")) %>%\n  group_by(stage_type, compare_grp) %>%\n  summarise(num = n(), \n            lq = quantile(time_diff_secs, 0.25),\n            medgap = median(time_diff_secs),\n            uq = quantile(time_diff_secs, 0.75),\n            lq_tp = (seconds_to_period(quantile(time_diff_secs, 0.25))),\n            medgap_tp = (seconds_to_period(median(time_diff_secs))),\n            uq_tp = (seconds_to_period(quantile(time_diff_secs, 0.75))),\n            avggap = round(mean(time_diff_secs),2),\n            avggap_tp = round(seconds_to_period(mean(time_diff_secs)), 2))\n\ngapplot1 <-\ngapranges %>%\n  filter(compare_grp == \"Next best\") %>%\n  ggplot(aes(stage_type, medgap, color = avggap)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"#0055A4\") +\n  geom_point(size = 2, color = \"#EF4135\") +\n  geom_point(aes(y = avggap), size = 2, color = \"black\", alpha = .8) +\n  geom_text(aes(label = medgap_tp), \n            size = 3, color = \"#EF4135\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = uq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = lq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.2) +\n  geom_text(aes(label = avggap_tp, y = avggap_tp),\n            size = 3, color = \"black\", alpha = .8, hjust = -.1) +\n  labs(title = \"Time Gap from Stage Winner to Next Best Time\",\n       subtitle = \"Median & Inter-quartile Ranges (avg in black)\",\n       y = \"Time Gap from Winner\", x = \"Stage Type\") +\n  theme_light() +\n  theme(plot.title = element_text(color = \"#0055A4\", size = 9),\n        plot.subtitle = element_text(face = \"italic\", color = \"#EF4135\",\n                                     size = 8),\n        axis.title.x = element_text(color = \"#0055A4\"),\n        axis.title.y = element_text(color = \"#0055A4\"), \n        axis.text.x = element_text(color = \"#0055A4\"),\n        axis.text.y=element_blank())\n\ngapplot2 <-\ngapranges %>%\n  filter(compare_grp == \"Last\") %>%\n  ggplot(aes(stage_type, medgap, color = avggap)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"#0055A4\") +\n  geom_point(size = 2, color = \"#EF4135\") +\n  geom_point(aes(y = avggap), size = 2, color = \"black\", alpha = .8) +\n  geom_text(aes(label = medgap_tp), \n            size = 3, color = \"#EF4135\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = uq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = lq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.1) +\n  geom_text(aes(label = avggap_tp, y = avggap_tp),\n            size = 3, color = \"black\", alpha = .8, hjust = -.1) +\n  labs(title = \"Time Gap from Stage Winner to Slowest Time\",\n       subtitle = \"Median & Inter-quartile Ranges (avg in black)\",\n       y = \"\", x = \"Stage Type\") +\n  theme_light() +\n  theme(plot.title = element_text(color = \"#0055A4\", size = 9),\n        plot.subtitle = element_text(face = \"italic\", color = \"#EF4135\",\n                                     size = 8),\n        axis.title.x = element_text(color = \"#0055A4\", size = 9),\n        axis.text.x = element_text(color = \"#0055A4\", size = 9),\n        axis.text.y=element_blank())\n\ngapplot1 + gapplot2 +\n  plot_annotation(title = \"Tour de France Stages, 1903 to 2019\",\n                  theme = theme(plot.title = \n                                  element_text(color = \"#0055A4\", size = 10)))\n\n\n\n\n\nWhat do these charts tell us? Well unsurprisingly mountain stages tend to have longer gaps between winners and the rest of the field than do flat/plain/hilly stages. Time trials are usually on flat or hilly stages, so they behave more like all other flat/plain/hilly stages. Even looking at the median to smooth for outliers, half of the last men in on mountain stages came in under 36 minutes, half over 36 minutes. The last 25% of mountain-stage riders came in an hour or more after the winner.\nHow has this changed over time? Well let’s facet out by degree decade.\nFirst thing that needs doing is to build a dataframe for analysis - it will have medians my race year and stage type. But for the chart we want to have a decade field. Turns out this was a bit complicated in order to get the chart I wanted. You can see in the code comments why I did what I did.\n\n\nShow df build code\ngaprangesyrdec <- \nstage_gap %>%\n  filter(compare_grp != \"Winner\") %>%\n  filter(stage_type %notin% c(\"Other\", \"Time Trial - Team\")) %>%\n  group_by(stage_type, compare_grp, race_year) %>%\n  summarise(num = n(), \n            lq = quantile(time_diff_secs, 0.25),\n            medgap = median(time_diff_secs),\n            uq = quantile(time_diff_secs, 0.75),\n            lq_tp = (seconds_to_period(quantile(time_diff_secs, 0.25))),\n            medgap_tp = (seconds_to_period(median(time_diff_secs))),\n            uq_tp = (seconds_to_period(quantile(time_diff_secs, 0.75))),\n            avggap = round(mean(time_diff_secs),2),\n            avggap_tp = round(seconds_to_period(mean(time_diff_secs)), 2)) %>%\n  ungroup() %>%\n  # need to hard code in rows so x axis & faceting works in by decade charts\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1915, .before = 13) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1916, .before = 14) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1917, .before = 15) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1918, .before = 16) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1915, .before = 123) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1916, .before = 124) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1917, .before = 125) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1918, .before = 126) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1915, .before = 233) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1916, .before = 234) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1917, .before = 235) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1918, .before = 236) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1915, .before = 343) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1916, .before = 344) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1917, .before = 345) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1918, .before = 346) %>%\n\n    # need field for x axis when faciting by decade\n  mutate(year_n = str_sub(race_year,4,4)) %>%\n  # create race decade field\n  mutate(race_decade = floor(race_year / 10) * 10) %>%\n  mutate(race_decade = as.character(paste0(race_decade, \"s\"))) %>%\n#  mutate(race_decade = ifelse(race_year %in%))\n  arrange(stage_type, compare_grp, race_year) %>%\n  select(stage_type, compare_grp, race_year, year_n, race_decade, everything())\n\n\nNow that we have a dataframe to work from, let’s make a chart. But to do that we have to make a few charts and then put them together with the patchwork package.\nFirst up is changes in the mountain stages and the median gaps between winner and next best recorded time. I grouped into three decade sets. Note that because of changes in the gaps over time, the y axes are a bit different in the early decades of the race. Also note at how I was able to get hours:seconds:minutes to show up on the y axis. The x axis digits are that way because race year would repeat in each facet, so I had to create a proxy year.\n\n\nShow mountain stage gap charts code\n# mountain winner to next best\nplot_dec_mtnb1 <-\ngaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n#  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Mountain\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\" , size = 8),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtnb2 <-\ngaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 420), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\" , size = 8),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtnb3 <-\ngaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 420), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\" , size = 8),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtnb1 / plot_dec_mtnb2 / plot_dec_mtnb3 +\n  plot_annotation(title = \"Gaps Between Winner & Next Best Times are Narrowing\",\n                  subtitle = \"Median gap on mountain stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nWhat does this chart tell us? As you look at it, keep in mind the y axis is different in the 1900s - 1930s chart because in the early years of the race the gaps were much wider.\nMost obviously, and not surprisingly, the gaps between winner and next best time shrank as the race professionalized and sports science got better. There are of course outliers here and there in the last few decades, but the course changes year-to-year, and some years the race organizers have made some years more difficult than other in the mountains.\nWe also see the effect of war. The two world wars not only interrupted the race in those years, but especially in the years immediately after WWII the gaps were larger than in the late 1930s. We can imagine what the war did to the pool of riders. The sport needed time to recover, for riders to train and get back to full fitness.\nOk, now let’s look at the changes in the mountains from the winners to the time for the last rider(s). The only change from the last set of charts is filter(compare_grp == \"Last\")\n\n\nShow mountain stage gap charts code\n# mountain winner to last\nplot_dec_mtla1 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n  #  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Last\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtla2 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 5400), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtla3 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 5400), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtla1 / plot_dec_mtla2 / plot_dec_mtla3  +\n  plot_annotation(title = \"Gaps Between Winner & Last Rider Times Mostly Stable Since 1950s\",\n                  subtitle = \"Median gap on mountain stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nWhat do we see here? Well first, notice that the gaps in the 1900s to 1930s were huge, especially before the 1930s. By the 1930s the gaps was usually around 30-40 minutes, similar to post-WWII years. But in the early years of the race, the last man in sometimes wouldn’t arrive until 10+ hours after the winner!\nBut since then the gaps are mostly around 30+ minutes. And again, I adjusted to include racers who finish outside of the time-stage cut off, and are thus eliminated from the race overall.\nOk, last two charts in this series…this time we’ll look at the flat & hilly stages. The only code changes are to the filters: filter(compare_grp == \"Next best\") or filter(compare_grp == \"Last\") and filter(stage_type == \"Flat / Plain / Hilly\").\n\n\nShow flat/hilly stage gap charts code\n# flat/hilly next best\nplot_dec_flnb1 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n  #  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Mountain\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flnb2 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 300), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flnb3 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 300), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 7),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flnb1 / plot_dec_flnb2 / plot_dec_flnb3 +\n  plot_annotation(title = \"Gaps Between Winner & Next Best Times Mostly < 1 Minute Since 1970s\",\n                  subtitle = \"Median gap on flat & hilly stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nPerhaps the most surprising thing in the Flat/Hilly stage gaps between winners & next best is that the gaps were similar to mountain stages. But then from watching the race all these years I remember that the climbers finish in groups fairly near to each other, even if the mountain stages are so hard.\nNo surprise of course that for many decades now the gaps have been around or under a minute. After the bunch sprints, the next group of riders, those not contesting the win, are right behind that pack.\n\n\nShow flat/hilly stage gap charts code\n### flat / hilly winner to last\nplot_dec_flla1 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n  #  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Mountain\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flla2 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 2340), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flla3 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 2340), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flla1 / plot_dec_flla2 / plot_dec_flla3 +\n  plot_annotation(title = \"Gaps Between Winner & Last Rider Times Very Tight by 1970s, Stabilized to ~ 10 min since\",\n                  subtitle = \"Median gap on flat & hilly stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nThe gap from winner to last was much less than winner-to-last in mountains, which isn’t a surprise. The sprinters tend to suffer in the Alps, Pyrenees and other mountain stages. As long as they come in under the time threshold, they are likely to be well behind on the day. But on flat stages, the only thing that keeps a rider more than a few minutes back is a spill, flat tire, or just having a bad day.\nNow it’s worth noting that I did not normalize for stage distance or elevation gain (for mountain stages) in terms of comparing year to year. I went with the assumption that since I was grouping multiple stages into a year, that even over time this would normalize itself. If this were a more serious analysis I’d do it.\nAnother extension of this analysis would be a model to predict time gaps. Then I’d include stage distance & gain, rider height/weight, and other factors.\nSome shout-outs are in order. First of course to the #tidytuesday crew. For the data here:\n* Alastair Rushworth and his tdf package\n* Thomas Camminady and his Le Tour dataset\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/Random music choice for the new routine/index.html",
    "href": "posts/Random music choice for the new routine/index.html",
    "title": "Random music for the new routine",
    "section": "",
    "text": "Yes, I have a lot of music"
  },
  {
    "objectID": "posts/random-music-choice-for-the-new-routine/index.html",
    "href": "posts/random-music-choice-for-the-new-routine/index.html",
    "title": "Random music for the new routine",
    "section": "",
    "text": "Yes, I have a lot of music.\n\n\n\n\n\nTo know exactly how much, a few years ago I painstakingly catalogued everything into a spreadsheet, and every time I get something new I add to the list. As of October 2023 I have over 2160 records and CDs…more than 900 vinyl albums (LP & EP), more than 1000 CD albums (LP & EP), 100+ 7” singles plus music in other formats.\nAnd yes I had it shipped from San Francisco to Lyon, then Lyon to Copenhagen. Related, my wife is a patient and understanding woman.\nWe know about the paradox of choice, right? Why you can’t choose something to watch from the thousands of things available on the streaming services to which you subscribe?\nYeah, same here.\nThis is a problem I want to solve even more now that I’ll be home more often, as my contract at CIEE has ended and I’m spending time re-skilling and upskilling my data knowledge…getting reacquainted with r, learning some Python and Tableau, and hopefully posting more regularly here, all while looking for my next job (hint-hint, if you need a data analyst/data scientist or know someone who does, let me know).\nI need things to listen to and sometimes I want to randomize the choice so I don’t spend 10 minutes dithering about it and choosing one of the same 20 records I listen to by default.\nSo anyway, what does a data nerd do? Write a quick script to randomize the choice. It’s fairly simple, and I have a couple of ground rules:\n\nWhat comes up must be played. If that means Hüsker Dü Land Speed Record at 8:30am, so be it. If that means Wilco’s Sky Blue Sky in the afternoon when I need a pick-me-up, so be it.\nOnce played, it can’t be played again via the randomizer. This of course doesn’t preclude playing it when I want to hear it.\n\nSo how does it work? Well, let’s see the code…it’s not genius-level but it does the trick.\nFirst we load the packages….\n\n# load packages\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\nNow we load the music library…\n\n# read in music library\nmusiclib &lt;- readxl::read_excel(\"~/Documents/music catalogue.xlsx\") %&gt;%\n    clean_names() %&gt;%\n    rename(format = format_vinyl_cd, type = type_lp_ep_7_single_12)\n#&gt; rename: renamed 2 variables (type, format)\nglimpse(musiclib)\n#&gt; Rows: 2,166\n#&gt; Columns: 11\n#&gt; $ artist        &lt;chr&gt; \"'Til Tuesday\", \"[The] Caseworker\", \"10,000 Maniacs\", \"1…\n#&gt; $ title         &lt;chr&gt; \"Welcome Home\", \"These Weeks Should Be Remembered\", \"Bli…\n#&gt; $ type          &lt;chr&gt; \"LP\", \"LP\", \"LP\", \"LP\", \"LP\", \"LP\", \"LP\", \"LP\", \"LP\", \"L…\n#&gt; $ format        &lt;chr&gt; \"CD\", \"CD\", \"CD\", \"CD\", \"CD\", \"CD\", \"vinyl\", \"vinyl\", \"C…\n#&gt; $ year_issue    &lt;dbl&gt; 1986, 2003, 1989, 1987, 1993, 1995, 2019, 1985, 1992, 19…\n#&gt; $ year_original &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ label_1       &lt;chr&gt; \"Epic Records\", \"Manifesto Records\", \"Elektra Records\", …\n#&gt; $ label_2       &lt;chr&gt; NA, NA, NA, NA, NA, \"Fifty Seven Records\", NA, NA, \"Poly…\n#&gt; $ label_3       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ notes         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ added         &lt;dttm&gt; 2020-12-14, 2020-05-25, 2020-12-14, 2020-12-14, 2020-12…\n\n…and here we load the exclusion list (it will get appended to later)\n\n# input played list\nmusicplayed &lt;- readxl::read_excel(\"~/Documents/musicplayed.xlsx\") %&gt;%\n    clean_names()\n#glimpse(musicplayed)\n\nThen we merge with the library, filter out what’s been played, and get something new to listen to. In this case I’m filtering to just get full LPs or EPs (no singles) and on vinyl or CD (I have some rando flexidiscs).\nAfter which I create a new df to output and append to the played list and overwrite the played list.\n\n# merge with played file,\n# exclude played\n# if desired, filter on format (vinyl, CD, either) and type (LP, EP, etc)\n# output new music to play\n# select variables for output to played file\nplaywhat &lt;- musiclib %&gt;%\n    merge(musicplayed, by = c(\"artist\", \"title\", \"format\", \"type\"), all = T) %&gt;%\n    mutate(played = ifelse(is.na(played), 0, played)) %&gt;%\n    filter(played == 0) %&gt;%\n    filter(format %in% c(\"vinyl\", \"CD\")) %&gt;%  \n    filter(type %in% c(\"LP\", \"EP\")) %&gt;%\n    sample_n(1) %&gt;%\n    select(artist, title, format, type) %&gt;%\n    mutate(date_played = Sys.Date()) %&gt;%\n    mutate(played = 1)\n\nview(playwhat)\n\n# add new music played to what has been played\nplayed2 &lt;- playwhat %&gt;%\n    rbind(musicplayed)\n\n## output that to excel file\nwritexl::write_xlsx(played2, \"~/Documents/musicplayed.xlsx\")\n\nBy some cosmic coincidence, I ran the program this morning to get something to listen to while writing this post, and what came up was Searching for Ray by Copenhagen band El Ray. And I bought this album back in 2018 at Rte 66, an amazing store here in Copenhagen.\nIt turned out to be the perfect music to listen to while getting this post together (and debugging some weird glitch where quarto couldn’t locate r)…I listened to it twice.\n\n\n\n\n\nThey play fun surf-rock…check them out.\nSo that’s it, the data nerd’s way to get around the paradox of choice and work through the tons of music I have on hand."
  },
  {
    "objectID": "posts/exploring-happiness/index.html",
    "href": "posts/exploring-happiness/index.html",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "",
    "text": "What makes us happy?\nA subjective question to be sure. The things that make me happy might not do much for you, and what brings you happiness might not work for me. To each their own? Or are there some basic things that boost our collective happiness, things most of us agree are good.\nThere have been attempts to answer the question in a universal way, by focusing on broad quality of life measures and activities. A perfect question for social scientists to dig into, and so they have.\nAmong the most referenced measures is the World Happiness Report. A yearly multi-chapter report published by Sustainable Development Solutions Network, using responses from the Gallup World Poll.\nFor this post (and at least one, maybe more to come) I want to dig into the data that has been made available. Every year the WHR research team releases data for chapter two, which has composite scores by country based on the ladder score question and some others. They add logged GDP and other data in the full chapter report. GDP is made available in the chapter data for release.\nThe Data  The Chapter 2 data has been consistently offered for download for years now. There are two datasets:\n\nData for Figure 1 includes the three three-year rolling average of the happiness ladder question (a 0-10 scale, described in the statistical appendix) along with related measures, aggregated by country. We also get the ladder score of a hypothetical dystopian country.\nData for Table 1 has the output of the OLS regression model to predict each country’s ladder score.\n\nThe Figure 1 data also includes OLS output in form of the percent of each country’s happiness score that could be attributed to the component variables. Another column in the Figure 1 set includes a column with the dystopia score plus the country’s residual of the actual and predicted ladder scores. In the data loading code below you’ll that added a column separating out the residual.\nBoth the report’s statisitcal appendix (downloads a pdf) and on-line version of Chapter 2 explain everything in more detail so I won’t repeat it here.\nWorking with the data  I’ll be using r for all aspects of the work; importing the data, cleaning, analysing, and visualising. So let’s go…\nFor this post, I want to focus on Exploratory Data Analysis (EDA). It’s the part of the analytical process where you get a broad overview of the data…look for things that need cleaning, look for distributions and relationships. In the past I’d build my own charts and tables, and that took quite a lot of time and mental energy.\nThankfully there are packages to speed up the work. So to get a quick look at this first set of WHR data, I’ll test-drive the EDA packages DataExplorer by Boxuan Cui, and Roland Krasser’s explorer.\nTo start with let’s load packages to import and clean the data. These are the three packages I use for almost every analysis in r.\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\nTo get the WHR data into RStudio you can go two ways. First is to download the sheet to your local machine and read in:\n\n# read in WHR data, fix country names for later merges\nwhr23_fig2_1a &lt;- readxl::read_excel(\"yourfilepath/DataForFigure2.1WHR2023.xls\") %&gt;%\n    clean_names() %&gt;%\n    as_tibble() %&gt;%\n    mutate(residual = dystopia_residual - ladder_score_in_dystopia) %&gt;%\n    select(-residual_is_dystopia_minus_dystopia_plus_residual) %&gt;%\n    mutate(whr_year = 2023) %&gt;%\n    mutate(country_name = case_when(\n        country_name == \"Czechia\" ~ \"Czech Republic\",\n        country_name == \"State of Palestine\" ~ \"Palestinian Territories\",\n        country_name ==  \"Turkiye\" ~ \"Turkey\",\n        TRUE ~ country_name))\n\nYou could also use curl to download straight from the WHR page:\n\nlibrary(readxl)\nurl1 &lt;- \"https://happiness-report.s3.amazonaws.com/2023/DataForFigure2.1WHR2023.xls\"\ndestfile1 &lt;- \"DataForFigure2_1WHR2023.xls\"\ncurl::curl_download(url1, destfile1)\nwhr23_fig2_1a &lt;- readxl::read_excel(destfile1) \n## %&gt;% (and then the same cleaning steps shown above)\n\nThe data from the WHR does not include the world region for each country, something I will want for further analysis. I’m not sure what the source is for the region grouping they are using. I found a file with a region column on Kaggle for the 2021 survey, so downloaded that and merged on country name.\n\n# read in kaggle file with region names\nctreg &lt;- readr::read_csv(\"yourfilepath/world-happiness-report-2021.csv\") %&gt;%\n    as_tibble() %&gt;%\n    clean_names() %&gt;%\n    select (country_name, region = regional_indicator)\n\n# join to whr23 on country, add missing region for Congo (Kinshasa)\nwhr23_fig2_1 &lt;- whr23_fig2_1a %&gt;%\n    merge(ctreg, all = TRUE) %&gt;%\n    as_tibble() %&gt;%\n    select(country_name, region, whr_year, everything()) %&gt;%\n    mutate(region = ifelse(\n        country_name == \"Congo (Kinshasa)\", \"Sub-Saharan Africa\", region))\n\nAnother way to do it is to hard code them. I had to go back to the 2018 report to find a list.\nRegardless, we now have a dataset, so let’s explore it.\nThe DataExplorer package  Let’s start with DataExplorer. The create_report() function runs the full set of native reports and outputs to a directory of your choosing with a filename of your choosing. But for a review I want to go through a few of the individual report elements.\nintroduce() outputs a table showing rows, columns and other information. If you want to see this information in chart form, plot_intro() and plot_missing() do that.\n\n## DataExplorer for EDA\nlibrary(DataExplorer) # EDA tools\n\n# summary of completes, missings\nintroduce(whr23_fig2_1)\n#&gt; # A tibble: 1 × 9\n#&gt;    rows columns discrete_columns continuous_columns all_missing_columns\n#&gt;   &lt;int&gt;   &lt;int&gt;            &lt;int&gt;              &lt;int&gt;               &lt;int&gt;\n#&gt; 1   150      22                2                 20                   0\n#&gt; # ℹ 4 more variables: total_missing_values &lt;int&gt;, complete_rows &lt;int&gt;,\n#&gt; #   total_observations &lt;int&gt;, memory_usage &lt;dbl&gt;\nplot_intro(whr23_fig2_1)\n\n\n\nplot_missing(whr23_fig2_1)\n\n\n\n\nThere are hardly any missing values in the set, which will make analysis easier.\nOk, so how about the distribution of our variables? plot_bar() takes all discrete variables and plot_histogram() runs for the continuous variables. Depending on how many columns of each type your dataset has you will need to play with the nrow and ncol options so that everything renders to the RStudio plot column. For the histograms you can change the number of binsm change the x-axis to log or some other option (the default is continuous). You can also customize the look a bit with passing arguments to the ggtheme = and theme_config() functions.\n\nplot_bar(whr23_fig2_1)\n\n\n\n\nFor the discrete variable bar charts, for this dataset there isn’t much to look at. But for a dataset with demographic varaibles, geographic places, etc. this would be very helpful.\n\nplot_histogram(whr23_fig2_1, nrow = 5L)\n\n\n\n\nThe histograms render in alpha order of the variable name, not order in the dataset. When the plots render, we look through them to see if there are any unusual skews or other things we want to watch out for depending on the type of analyses to be run. In this case there are a few solitary bars in some of the histograms, like values above 0.4 in the generosity column. But nothing too skewed so that if we took a closer look at distributions by way of means or interquartiles that we’d be too worried.\nNow for my favorite part of this package, a correlation matrix! We’ll run plot_correlation() without the year, dystopia ladder score, and whiskers and make sure to only do continuous. We can also adjust things like the type of correlation (default is pearson), the size of the coefficient labels in the chart and other elements.\n\n## correlation...remove some columns, clean NA in pipe, continuous only, change text size\nwhr23_fig2_1 %&gt;%\n    select(-whr_year, -ladder_score_in_dystopia, -upperwhisker, -lowerwhisker) %&gt;%\n    filter(!is.na(residual)) %&gt;%\n    plot_correlation(type = \"continuous\", geom_text_args = list(\"size\" = 3))\n\n\n\n\nThe way my brain works is to look for visual patterns and relationships. So a correlation matrix like this is perfect to give me a broad view of how the continuous variables relate to each other. The matrix heatmap returns positive relationships in red, negative in blue. I first want to look at the relationships of the component variables to the ladder score, and we see positive associations for everything except for perception of corruption, which makes sense because you’d likely report being less happy if you lived in a corrupt country.\nThe weakest association is between generosity, which comes from a question asking “Have you donated to charity in the past month?” So while donations to charity are a good thing, they don’t necessarily move the needle on happiness. At least not in the aggregate. But maybe by country or region? Something to take a look at later. This is why we do EDA…\nWe also see that we could have run this without the “explained by…” columns as they have the same coefficients as the component variables.\nAs much as I love a correlation matrix, I love scatterplots even more. I clearly have a thing for patterns and relationships. The plot_scatterplot function returns plots for all the variables you pass along, against the one you call in the by = argument. Here we want to see the association between the ladder score and component variables from Chapter 2.\n\nplot_scatterplot(\n    whr23_fig2_1 %&gt;% select(ladder_score, social_support:perceptions_of_corruption, dystopia_residual, residual), \n    by = \"ladder_score\", nrow = 3L)\n\n\n\n\nWe know from the correlation heatmap that we don’t need the “explained_by_*” variables as they were redundant to the component variables. The x/y distributions here confirm what we saw in the correlations, including the slightly negative relationship between the ladder score and perceptions of corruption, and that generosity was a weaker relationship.\nWhile the scatterplot function does allow for some plot customization, one I tried but couldn’t get to work was using the geom_point_args() call to color the dots by region, like this using ggplot:\n\nwhr23_fig2_1 %&gt;%\n    ggplot(aes(x = perceptions_of_corruption, y = ladder_score)) +\n    geom_point(aes(color = region)) +\n    theme(legend.position = \"bottom\")\n\n\n\n\nThere are a few other functions offered to do principal component analysis and qq (quantile-quantile) plots, but they did not help much with this dataset.\nOverall there are plenty of helpful features in DataExplorer that make it worthwhile to use for EDA. I’d like the ability to color scatterplots by a discrete variable, or to facet the histograms or scatterplots, but as is, a robust EDA tool.\nThe explore package  The best function here is explore(dataset), which launches a shiny window with four tabs.\nThe “overview” tab shown here, displays a table with mean, min, max, and unique & missing value counts by variable.\n\n\n\n\n\nThe “variable” tab allows you to explore variables on their own… \n…or in relation to one another.  You not only get a chart appropriate to the variable type (categoricals with bars, continuous with area plots), but when you target against another variable you get a scatterplot.\nThe explain tab runs a decision tree against a target variable, and the data tab displays the entire dataset as a table, all rows and all columns. So before launching this you may want to be mindful of running it against too large a dataset.\nIf you don’t want to launch the shiny app, you can output a report in html…\n\n## creates html report of all individual reports\nwhr23_fig2_1 %&gt;%\n    report(output_dir = \"~/data/World Happiness Report\")\n\n…or run select features individually, depending on what you need….\n\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, social_support:perceptions_of_corruption,\n                 explained_by_log_gdp_per_capita:residual) %&gt;%\n    describe_all() \n#&gt; select: dropped 8 variables (country_name, region, whr_year, standard_error_of_ladder_score, upperwhisker, …)\n#&gt; # A tibble: 14 × 8\n#&gt;    variable                          type     na na_pct unique   min  mean   max\n#&gt;    &lt;chr&gt;                             &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 ladder_score                      dbl      13    8.7    138  1.86  5.54  7.8 \n#&gt;  2 social_support                    dbl      13    8.7    138  0.34  0.8   0.98\n#&gt;  3 healthy_life_expectancy           dbl      14    9.3    137 51.5  65.0  77.3 \n#&gt;  4 freedom_to_make_life_choices      dbl      13    8.7    138  0.38  0.79  0.96\n#&gt;  5 generosity                        dbl      13    8.7    138 -0.25  0.02  0.53\n#&gt;  6 perceptions_of_corruption         dbl      13    8.7    138  0.15  0.73  0.93\n#&gt;  7 explained_by_log_gdp_per_capita   dbl      13    8.7    138  0     1.41  2.2 \n#&gt;  8 explained_by_social_support       dbl      13    8.7    138  0     1.16  1.62\n#&gt;  9 explained_by_healthy_life_expect… dbl      14    9.3    137  0     0.37  0.7 \n#&gt; 10 explained_by_freedom_to_make_lif… dbl      13    8.7    138  0     0.54  0.77\n#&gt; 11 explained_by_generosity           dbl      13    8.7    138  0     0.15  0.42\n#&gt; 12 explained_by_perceptions_of_corr… dbl      13    8.7    138  0     0.15  0.56\n#&gt; 13 dystopia_residual                 dbl      14    9.3    137 -0.11  1.78  2.95\n#&gt; 14 residual                          dbl      14    9.3    137 -1.89  0     1.18\n\n\nwhr23_fig2_1 %&gt;%\n    explore(ladder_score)\n\n\n\n\nThe main vignette and reference guide is very robust, so no need to repeat too much here. But there are some fun features like decision trees, and lots of flexibilty to explore multiple variables in relation to each other.\nThe skimr package  Then there is skimr, one of the first EDA packages that I remember seeing. If there’s a feature I like most, it’s the basic skim() function, which returns means & other distributions, as well as little histograms.\n\n\n\nlibrary(skimr)\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, social_support:perceptions_of_corruption, residual) %&gt;%\n    skim()\n#&gt; select: dropped 15 variables (country_name, region, whr_year, standard_error_of_ladder_score, upperwhisker, …)\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n150\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nladder_score\n13\n0.91\n5.54\n1.14\n1.86\n4.72\n5.68\n6.33\n7.80\n▁▂▆▇▃\n\n\nsocial_support\n13\n0.91\n0.80\n0.13\n0.34\n0.72\n0.83\n0.90\n0.98\n▁▂▃▆▇\n\n\nhealthy_life_expectancy\n14\n0.91\n64.97\n5.75\n51.53\n60.65\n65.84\n69.41\n77.28\n▃▃▇▇▂\n\n\nfreedom_to_make_life_choices\n13\n0.91\n0.79\n0.11\n0.38\n0.72\n0.80\n0.87\n0.96\n▁▁▃▇▇\n\n\ngenerosity\n13\n0.91\n0.02\n0.14\n-0.25\n-0.07\n0.00\n0.12\n0.53\n▃▇▅▁▁\n\n\nperceptions_of_corruption\n13\n0.91\n0.73\n0.18\n0.15\n0.67\n0.77\n0.85\n0.93\n▁▁▁▅▇\n\n\nresidual\n14\n0.91\n0.00\n0.50\n-1.89\n-0.22\n0.07\n0.30\n1.18\n▁▂▅▇▂\n\n\n\n\n\n\n\nIt’s especially helpful on small and medium-sized datasets, to get a quick overview and look for outliers.\nHappiness data takeaways  Using these packages for EDA on the happiness data, we learned that:\n\nThere are not many missing values in the data.\nAll of the component variables except generosity have strong positive correlations with the happiness score.\nPerception of corruption has, as expected, a negative association with happiness.\n\nWe also came away wanting to know a bit more about differences by region, so that’s a good starting point for the next post, which will be a slightly deeper dive into the data.\nConclusion  There is no one perfect EDA package that suits all needs for any dataset. DataExplorer has some robust features, particularly in this usecase the correlation heatmap and the scatterplots. I loved the native reports and shiny app in explorer. I had planned to look at correlationfunnel, but it’s only really suited to a use-case with binary outcomes such as customer sign-up, churn, employee retention, college admissions outcomes (admits, yield), student success outcomes like retention and graduation. I’ll have to find another dataset to try that package. Doing these package test-drives reminded me that skimr is also very useful.\nGoing forward I’ll be setting up a more deliberate EDA workflow using parts of each of these packages, depending on the size of the dataset and the main questions I’d have of the data."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html",
    "href": "posts/exploring-happiness-eda/index.html",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "",
    "text": "Our cat Dalias (aka Potato), in a happy moment\n\n\nWhat makes us happy?\nA subjective question to be sure. The things that make me happy might not do much for you, and what brings you happiness might not work for me. To each their own? Or are there some basic things that boost our collective happiness, things most of us agree are good?\nThere have been attempts to answer the question in a universal way, by focusing on broad quality of life measures and activities. A perfect question for social scientists to dig into, and so they have.\nAmong the most referenced measures is the World Happiness Report. A yearly multi-chapter report published by Sustainable Development Solutions Network, using responses from the Gallup World Poll.\nFor this post (and at least one, maybe more to come) I want to dig into the data that has been made available. Every year the WHR research team releases data for chapter two, which has composite scores by country based on the ladder score question, along with related questions from the survey. They add logged GDP and other data in the full chapter report. GDP is made available in the chapter data for release.\nThe Data  The Chapter 2 data has been consistently offered for download for years now. There are two datasets:\n\nData for Figure 1 includes the three three-year rolling average of the happiness ladder question (a 0-10 scale, described in the statistical appendix) along with related measures, aggregated by country. We also get the ladder score of a hypothetical dystopian country.\nData for Table 1 has the output of the OLS regression model to predict each country’s ladder score.\n\nThe Figure 1 data also includes OLS output in form of the percent of each country’s happiness score that could be attributed to the component variables. Another column in the Figure 1 set includes a column with the dystopia score plus the country’s residual of the actual and predicted ladder scores. In the data loading code below you’ll see that I added a column separating out the residual.\nBoth the report’s statisitcal appendix (downloads a pdf) and on-line version of Chapter 2 explain everything in more detail so I won’t repeat it here.\nWorking with the data  I’ll be using r for all aspects of the work; importing the data, cleaning, analysing, and visualising. So let’s go…\nFor this post, I want to focus on Exploratory Data Analysis (EDA). It’s the part of the analytical process where you get a broad overview of the data…look for things that need cleaning, look for distributions and relationships. In the past I’d build my own charts and tables, and that took quite a lot of time and mental energy.\nThankfully there are packages to speed up the work. So to get a quick look at this first set of WHR data, I’ll test-drive the EDA packages DataExplorer by Boxuan Cui, and Roland Krasser’s explorer.\nTo start with let’s load packages to import and clean the data. These are the three packages I use for almost every analysis in r.\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\nTo get the WHR data into RStudio you can go two ways. First is to download the sheet to your local machine and read in:\n\n# read in WHR data, fix country names for later merges\nwhr23_fig2_1a &lt;- readxl::read_excel(\"yourfilepath/DataForFigure2.1WHR2023.xls\") %&gt;%\n    clean_names() %&gt;%\n    as_tibble() %&gt;%\n    mutate(residual = dystopia_residual - ladder_score_in_dystopia) %&gt;%\n    select(-residual_is_dystopia_minus_dystopia_plus_residual) %&gt;%\n    mutate(whr_year = 2023) %&gt;%\n    mutate(country_name = case_when(\n        country_name == \"Czechia\" ~ \"Czech Republic\",\n        country_name == \"State of Palestine\" ~ \"Palestinian Territories\",\n        country_name ==  \"Turkiye\" ~ \"Turkey\",\n        TRUE ~ country_name))\n\nYou could also use curl to download straight from the WHR page:\n\nlibrary(readxl)\nurl1 &lt;- \"https://happiness-report.s3.amazonaws.com/2023/DataForFigure2.1WHR2023.xls\"\ndestfile1 &lt;- \"DataForFigure2_1WHR2023.xls\"\ncurl::curl_download(url1, destfile1)\nwhr23_fig2_1a &lt;- readxl::read_excel(destfile1) \n## %&gt;% (and then the same cleaning steps shown above)\n\nThe data from the WHR does not include the world region for each country, something I will want for further analysis. I’m not sure what the source is for the region grouping they are using. I found a file with a region column on Kaggle for the 2021 survey, so downloaded that and merged on country name.\n\n# read in kaggle file with region names\nctreg &lt;- readr::read_csv(\"yourfilepath/world-happiness-report-2021.csv\") %&gt;%\n    as_tibble() %&gt;%\n    clean_names() %&gt;%\n    select (country_name, region = regional_indicator)\n\n# join to whr23 on country, add missing region for Congo (Kinshasa)\nwhr23_fig2_1 &lt;- whr23_fig2_1a %&gt;%\n    merge(ctreg, all = TRUE) %&gt;%\n    as_tibble() %&gt;%\n    select(country_name, region, whr_year, everything()) %&gt;%\n    mutate(region = ifelse(\n        country_name == \"Congo (Kinshasa)\", \"Sub-Saharan Africa\", region))\n\nAnother way to do it is to hard code them. I had to go back to the 2018 report to find a list.\nRegardless, we now have a dataset, so let’s explore it.\nThe DataExplorer package  Let’s start with DataExplorer. The create_report() function runs the full set of native reports and outputs to a directory of your choosing with a filename of your choosing. But for a review I want to go through a few of the individual report elements.\nintroduce() outputs a table showing rows, columns and other information. If you want to see this information in chart form, plot_intro() and plot_missing() do that.\n\n## DataExplorer for EDA\nlibrary(DataExplorer) # EDA tools\n\n# summary of completes, missings\nintroduce(whr23_fig2_1)\n#&gt; # A tibble: 1 × 9\n#&gt;    rows columns discrete_columns continuous_columns all_missing_columns\n#&gt;   &lt;int&gt;   &lt;int&gt;            &lt;int&gt;              &lt;int&gt;               &lt;int&gt;\n#&gt; 1   150      22                2                 20                   0\n#&gt; # ℹ 4 more variables: total_missing_values &lt;int&gt;, complete_rows &lt;int&gt;,\n#&gt; #   total_observations &lt;int&gt;, memory_usage &lt;dbl&gt;\nplot_intro(whr23_fig2_1)\n\n\n\nplot_missing(whr23_fig2_1)\n\n\n\n\nThere are hardly any missing values in the set, which will make analysis easier.\nOk, so how about the distribution of our variables? plot_bar() takes all discrete variables and plot_histogram() runs for the continuous variables. Depending on how many columns of each type your dataset has you will need to play with the nrow and ncol options so that everything renders to the RStudio plot column. For the histograms you can change the number of binsm change the x-axis to log or some other option (the default is continuous). You can also customize the look a bit with passing arguments to the ggtheme = and theme_config() functions.\n\nplot_bar(whr23_fig2_1)\n\n\n\n\nFor the discrete variable bar charts, for this dataset there isn’t much to look at. But for a dataset with demographic varaibles, geographic places, etc. this would be very helpful.\n\nplot_histogram(whr23_fig2_1, nrow = 5L)\n\n\n\n\nThe histograms render in alpha order of the variable name, not order in the dataset. When the plots render, we look through them to see if there are any unusual skews or other things we want to watch out for depending on the type of analyses to be run. In this case there are a few solitary bars in some of the histograms, like values above 0.4 in the generosity column. But nothing too skewed so that if we took a closer look at distributions by way of means or interquartiles that we’d be too worried.\nNow for my favorite part of this package, a correlation matrix! We’ll run plot_correlation() without the year, dystopia ladder score, and whiskers and make sure to only do continuous. We can also adjust things like the type of correlation (default is pearson), the size of the coefficient labels in the chart and other elements.\n\n## correlation...remove some columns, clean NA in pipe, continuous only, change text size\nwhr23_fig2_1 %&gt;%\n    select(-whr_year, -ladder_score_in_dystopia, -upperwhisker, -lowerwhisker) %&gt;%\n    filter(!is.na(residual)) %&gt;%\n    plot_correlation(type = \"continuous\", geom_text_args = list(\"size\" = 3))\n\n\n\n\nThe way my brain works is to look for visual patterns and relationships. So a correlation matrix like this is perfect to give me a broad view of how the continuous variables relate to each other. The matrix heatmap returns positive relationships in red, negative in blue. I first want to look at the relationships of the component variables to the ladder score, and we see positive associations for everything except for perception of corruption, which makes sense because you’d likely report being less happy if you lived in a corrupt country.\nThe weakest association is between generosity, which comes from a question asking “Have you donated to charity in the past month?” So while donations to charity are a good thing, they don’t necessarily move the needle on happiness. At least not in the aggregate. But maybe by country or region? Something to take a look at later. This is why we do EDA…\nWe also see that we could have run this without the “explained by…” columns as they have the same coefficients as the component variables.\nAs much as I love a correlation matrix, I love scatterplots even more. I clearly have a thing for patterns and relationships. The plot_scatterplot function returns plots for all the variables you pass along, against the one you call in the by = argument. Here we want to see the association between the ladder score and component variables from Chapter 2.\n\nplot_scatterplot(\n    whr23_fig2_1 %&gt;% select(ladder_score, social_support:perceptions_of_corruption, dystopia_residual, residual), \n    by = \"ladder_score\", nrow = 3L)\n\n\n\n\nWe know from the correlation heatmap that we don’t need the “explained_by_*” variables as they were redundant to the component variables. The x/y distributions here confirm what we saw in the correlations, including the slightly negative relationship between the ladder score and perceptions of corruption, and that generosity was a weaker relationship.\nWhile the scatterplot function does allow for some plot customization, one I tried but couldn’t get to work was using the geom_point_args() call to color the dots by region, like this using ggplot:\n\nwhr23_fig2_1 %&gt;%\n    ggplot(aes(x = perceptions_of_corruption, y = ladder_score)) +\n    geom_point(aes(color = region)) +\n    theme(legend.position = \"bottom\")\n\n\n\n\nThere are a few other functions offered to do principal component analysis and qq (quantile-quantile) plots, but they did not help much with this dataset.\nOverall there are plenty of helpful features in DataExplorer that make it worthwhile to use for EDA. I’d like the ability to color scatterplots by a discrete variable, or to facet the histograms or scatterplots, but as is, a robust EDA tool.\nThe explore package  The best function here is explore(dataset), which launches a shiny window with four tabs.\nThe “overview” tab shown here, displays a table with mean, min, max, and unique & missing value counts by variable.\n\n\n\n\n\nThe “variable” tab allows you to explore variables on their own… \n…or in relation to one another.  You not only get a chart appropriate to the variable type (categoricals with bars, continuous with area plots), but when you target against another variable you get a scatterplot.\nThe explain tab runs a decision tree against a target variable, and the data tab displays the entire dataset as a table, all rows and all columns. So before launching this you may want to be mindful of running it against too large a dataset.\nIf you don’t want to launch the shiny app, you can output a report in html…\n\n## creates html report of all individual reports\nwhr23_fig2_1 %&gt;%\n    report(output_dir = \"~/data/World Happiness Report\")\n\n…or run select features individually, depending on what you need….\n\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, social_support:perceptions_of_corruption,\n                 explained_by_log_gdp_per_capita:residual) %&gt;%\n    describe_all() \n#&gt; select: dropped 8 variables (country_name, region, whr_year, standard_error_of_ladder_score, upperwhisker, …)\n#&gt; # A tibble: 14 × 8\n#&gt;    variable                          type     na na_pct unique   min  mean   max\n#&gt;    &lt;chr&gt;                             &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 ladder_score                      dbl      13    8.7    138  1.86  5.54  7.8 \n#&gt;  2 social_support                    dbl      13    8.7    138  0.34  0.8   0.98\n#&gt;  3 healthy_life_expectancy           dbl      14    9.3    137 51.5  65.0  77.3 \n#&gt;  4 freedom_to_make_life_choices      dbl      13    8.7    138  0.38  0.79  0.96\n#&gt;  5 generosity                        dbl      13    8.7    138 -0.25  0.02  0.53\n#&gt;  6 perceptions_of_corruption         dbl      13    8.7    138  0.15  0.73  0.93\n#&gt;  7 explained_by_log_gdp_per_capita   dbl      13    8.7    138  0     1.41  2.2 \n#&gt;  8 explained_by_social_support       dbl      13    8.7    138  0     1.16  1.62\n#&gt;  9 explained_by_healthy_life_expect… dbl      14    9.3    137  0     0.37  0.7 \n#&gt; 10 explained_by_freedom_to_make_lif… dbl      13    8.7    138  0     0.54  0.77\n#&gt; 11 explained_by_generosity           dbl      13    8.7    138  0     0.15  0.42\n#&gt; 12 explained_by_perceptions_of_corr… dbl      13    8.7    138  0     0.15  0.56\n#&gt; 13 dystopia_residual                 dbl      14    9.3    137 -0.11  1.78  2.95\n#&gt; 14 residual                          dbl      14    9.3    137 -1.89  0     1.18\n\n\nwhr23_fig2_1 %&gt;%\n    explore(ladder_score)\n\n\n\n\nThe main vignette and reference guide are both very comprehensive, so no need to repeat too much here. But there are some fun features like decision trees, and lots of flexibilty to explore multiple variables in relation to each other.\nThe skimr package  Then there is skimr, one of the first EDA packages that I remember seeing. If there’s a feature I like most, it’s the basic skim() function, which returns means & other distributions, as well as little histograms.\n\n\n\nlibrary(skimr)\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, social_support:perceptions_of_corruption, residual) %&gt;%\n    skim()\n#&gt; select: dropped 15 variables (country_name, region, whr_year, standard_error_of_ladder_score, upperwhisker, …)\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n150\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nladder_score\n13\n0.91\n5.54\n1.14\n1.86\n4.72\n5.68\n6.33\n7.80\n▁▂▆▇▃\n\n\nsocial_support\n13\n0.91\n0.80\n0.13\n0.34\n0.72\n0.83\n0.90\n0.98\n▁▂▃▆▇\n\n\nhealthy_life_expectancy\n14\n0.91\n64.97\n5.75\n51.53\n60.65\n65.84\n69.41\n77.28\n▃▃▇▇▂\n\n\nfreedom_to_make_life_choices\n13\n0.91\n0.79\n0.11\n0.38\n0.72\n0.80\n0.87\n0.96\n▁▁▃▇▇\n\n\ngenerosity\n13\n0.91\n0.02\n0.14\n-0.25\n-0.07\n0.00\n0.12\n0.53\n▃▇▅▁▁\n\n\nperceptions_of_corruption\n13\n0.91\n0.73\n0.18\n0.15\n0.67\n0.77\n0.85\n0.93\n▁▁▁▅▇\n\n\nresidual\n14\n0.91\n0.00\n0.50\n-1.89\n-0.22\n0.07\n0.30\n1.18\n▁▂▅▇▂\n\n\n\n\n\n\n\nIt’s especially helpful on small and medium-sized datasets, to get a quick overview and look for outliers.\nHappiness data takeaways  Using these packages for EDA on the happiness data, we learned that:\n\nThere are not many missing values in the data.\nAll of the component variables except generosity have strong positive correlations with the happiness score.\nPerception of corruption has, as expected, a negative association with happiness.\n\nWe also came away wanting to know a bit more about differences by region, so that’s a good starting point for the next post, which will be a slightly deeper dive into the data.\nConclusion  There is no one perfect EDA package that suits all needs for any dataset. DataExplorer has some robust features, particularly in this usecase the correlation heatmap and the scatterplots. I loved the native reports and shiny app in explorer. I had planned to look at correlationfunnel, but it’s only really suited to a use-case with binary outcomes such as customer sign-up, churn, employee retention, college admissions outcomes (admits, yield), student success outcomes like retention and graduation. I’ll have to find another dataset to try that package. Doing these package test-drives reminded me that skimr is also very useful.\nGoing forward I’ll be setting up a more deliberate EDA workflow using parts of each of these packages, depending on the size of the dataset and the main questions I’d have of the data."
  },
  {
    "objectID": "posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/index.html#using-the-spotify-api-and-spotifyr-package-to-visualize-some-music-ive-made",
    "href": "posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/index.html#using-the-spotify-api-and-spotifyr-package-to-visualize-some-music-ive-made",
    "title": "Sad Songs & Pretty Charts - a Gosta Berling Music Data Visualization",
    "section": "",
    "text": "For this post, I thought I’d focus on music analytics, given that music and data science/analysis are two things I’ve spent most of my waking hours doing for a number of years now.\nOver the years I’ve made a lot of music in a number of different projects. For most of my time living in the Bay Area I’ve played with some friends in a band called Gosta Berling. We’ve released two EPs and a full album (click on the album covers to give listen)\n  \nOur sound could be called melancholy mood-pop. We like melody, but we were raised on brooding post-punk so a minor key vibe is definitely present. The Spotify API has musical features including danceability, energy, and valence (what they call ‘happiness’). I used Charlie Thompson’s spotifyr package to see how we score. spotifyr has a bunch of functions designed to make it easier to navigate Spotify’s JSON data structure.\nOne quick thing…I’m using Spotify data so in effect validating Spotify. While I appreciate the availability of the data for projects like this, Spotify needs to do much better by way of paying artists. We don’t have tons of streams, but as you can see from this account report… \n…artists get f$ck-all per stream. So sure, use Spotify, it’s a great tool for discovering new music. And while artists pressure them to pay more per stream, you can help by purchasing music from artists you like. The pandemic has killed touring income, so sales are all that many artists have to support themselves. Help them out, buy the music you like. Especially if they’re on Bandcamp and you buy 1st Fridays, when Bandcamp waives their revenue share, meaning the artist gets every penny. Did I mention you can buy our music on Bandcamp? :)\nAnyway, soapbox off…first thing, let’s load the packages we’ll be using:\n\n# load packages\nlibrary(spotifyr) # pull data from spotify\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot text\n\nlibrary(httr)\nlibrary(stringr) # work with string data\nlibrary(lubridate) # work with dates\nlibrary(GGally) # correlation plots\nlibrary(PerformanceAnalytics) # correlation plots\nlibrary(corrr)  # correlation plots\n\nTo get access the Spotify data, you need a developer key. Charlie’s explained how to do it on the package page, so I won’t repeat that here. To set up the keys in your .Renviron, run usethis::edit_r_environ() and add (where the xs are your codes):\n\nSPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx'\nSPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx'\n\n# or do\nSys.setenv(SPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx')\n\nThis call sets your access token for the data session\nIf you run into redirect issues, see this stackoverflow thread, specifically this comment\nFirst thing is to search the artist data for audio features. I’m pulling in everything into a dataframe. Now initially I had a filter for artist = 'Gosta Berling'. But that was also pulling in data from a Swedish prog-metal band Gösta Berling’s Saga. So I needed to filter on our artist ID and pull in albums and singles (for some reason our EPs were listed as singles, but whatever)\n\n# gets full range of information for tracks from artist\ngosta_audio1 &lt;- get_artist_audio_features(artist = \"4Vb2yqJJthJTAZxKz4Aryn\", include_groups = c(\"album\", \"single\"))\n\nOh…why more than one band with Gosta Berling in their name? Well, The Saga of Gösta Berling was Greta Garbo’s first feature-length film, and based on an old Swedish novel Gösta Berling’s Saga about a drunkard outcast priest seeking redemption. When, like our band, you’re a bunch of movie nerds, and a couple of you are especially obsessed with silent films & old Hollywood, you name your band Gosta Berling. And so does a Swedish band…anyway…more about the data.\nThe code here gets a dataframe for each record. I also needed to add album titles. Next steps were to merge the album dataframes together, extract the song IDs and pass them to the get_track_features() function as a list.\n\n# get album tracks, add album name could merge on other df, easier to quick fix this way\ntravel &lt;- get_album_tracks(id = \"0vBs7ZtBj3ROrRyac3M47q\")\ntravel$album &lt;- \"Travel\"\nsweetheart &lt;- get_album_tracks(id = \"0dJBaJ3VFxOtdG5L9yzALJ\")\nsweetheart$album &lt;- \"Everybody's Sweetheart\"\nwinterland  &lt;- get_album_tracks(id = \"6CMekiY6lCIuBZpzFDInpf\")\nwinterland$album &lt;- \"Winterland\"\n\n# merge album files, output track ids to use for audio features\ngbtracks &lt;- data.table::rbindlist(list(sweetheart, travel, winterland))\n#copy result from console to paste as vector below\ngbtrackids &lt;- dput(as.character(gbtracks$id)) \n\ngosta_audio2 &lt;- \n  get_track_audio_features(c(\"2SotrXjkvjTZf05XSMKGyp\", \"07cTJ65GZ4Lvr6b1CtgPll\", \"4ooz79IN3la97See8IMNRL\", \"7pgCh68iFO0LNUNKWTFFIP\", \"4ZCesDRgGWKEXwq8iKw5FB\", \"4ZdH5B3tijHjWiwyOErgtf\", \"5GWKeBYgOsv3PKutDIQoet\", \"0XXWRsY6URe2Vx7Bxs6k06\", \"0t3AGVXHyF3dEYuhvAYuNz\", \"4ObsuwrVLKUq5aF8whrFqk\", \"0PnjWfIPwsqBtllMILjzxB\", \n\"7uQtlGsKxXOzsSapKTZRFU\", \"3kQuG44stzA3pQf7g61Ipt\", \n\"0YH9wkimhRhCmstNZyxPgO\", \"7rEbjyNO0dTEK6x8HkLqAz\", \"4VgEAtVQtkwIHzKMOROk6X\", \"5R9M4s6QZljNPVVzxoy98h\", \"1FNtHQ0juoKg2yCf9u4VSg\", \"5NWmfmupE7FEJ9O1e9vizu\"),\nauthorization = get_spotify_access_token())\n\nThis gets a dataframe with most of what I want…just a few tweaks needed. First, since they weren’t pulled from the get_track_audio_features() call, I used the track id, name, and album track number from the gbtracks dataframe. Also, because the song key returned as only the numeric value, I created the letter name and mode (major or minor), and ordered the columns.\n\n# get track number and name, merge from gbtracks -\n# need b/c these fields not returned from get_track_audio_features()\ngbtrack2 &lt;- gbtracks %&gt;%\n  select(id, name, album, track_number) %&gt;%\n  rename(track_name = name)\n\n# merge to complete df. add names for key and mode\ngosta_audio &lt;- left_join(gosta_audio2, gbtrack2) %&gt;%\n  mutate(key_name = case_when(key == 0 ~ \"C\", key == 2 ~ \"D\", key == 4 ~ \"E\", key == 5 ~ \"F\",\n                              key == 7 ~ \"G\", key == 9 ~ \"A\", key == 11 ~ \"B\")) %&gt;%\n  mutate(mode_name = case_when(mode == 0 ~ \"Minor\", mode == 1 ~ \"Major\")) %&gt;%\n  mutate(key_mode = paste(key_name, mode_name, sep = \" \")) %&gt;%\n  rename(track_id = id) %&gt;%\n  select(album, track_name, track_number, key_mode, time_signature, duration_ms, \n         danceability, energy, loudness, tempo, valence, \n         acousticness, instrumentalness, liveness, speechiness,\n         key_name, mode_name, key, mode)\n\nOk, we’ve got a nice tidy dataframe, let’s do some analysis & visualization!\nSpotify’s developer pages have good explanations of the data. Some notes from spotify here about elements:\n\nMost of the audio features are 0-1, 1 being highest. e.g. higher speechiness = higher ratio of words::music. Valence is “happiness”, where higher = happier.\nLoundess in dB, tempo is BPM.\n\nSo let’s look at a quick summary of the audio features for our songs.\n\n#&gt;   duration_ms      danceability        energy          loudness      \n#&gt;  Min.   : 75933   Min.   :0.2500   Min.   :0.0476   Min.   :-21.350  \n#&gt;  1st Qu.:295380   1st Qu.:0.3545   1st Qu.:0.3260   1st Qu.:-12.031  \n#&gt;  Median :350053   Median :0.3920   Median :0.5190   Median : -9.943  \n#&gt;  Mean   :334762   Mean   :0.4105   Mean   :0.5233   Mean   :-10.705  \n#&gt;  3rd Qu.:385634   3rd Qu.:0.4820   3rd Qu.:0.7160   3rd Qu.: -7.537  \n#&gt;  Max.   :522760   Max.   :0.5730   Max.   :0.9360   Max.   : -6.014  \n#&gt;      tempo           valence        acousticness     instrumentalness \n#&gt;  Min.   : 82.15   Min.   :0.0349   Min.   :0.00371   Min.   :0.00881  \n#&gt;  1st Qu.:116.51   1st Qu.:0.1620   1st Qu.:0.12920   1st Qu.:0.50800  \n#&gt;  Median :141.83   Median :0.2940   Median :0.39300   Median :0.69800  \n#&gt;  Mean   :131.06   Mean   :0.3105   Mean   :0.41332   Mean   :0.62883  \n#&gt;  3rd Qu.:149.98   3rd Qu.:0.4405   3rd Qu.:0.63750   3rd Qu.:0.84450  \n#&gt;  Max.   :166.01   Max.   :0.6960   Max.   :0.88600   Max.   :0.94400  \n#&gt;     liveness       speechiness     \n#&gt;  Min.   :0.0703   Min.   :0.02540  \n#&gt;  1st Qu.:0.1020   1st Qu.:0.02810  \n#&gt;  Median :0.1160   Median :0.03060  \n#&gt;  Mean   :0.1333   Mean   :0.03699  \n#&gt;  3rd Qu.:0.1265   3rd Qu.:0.03865  \n#&gt;  Max.   :0.3300   Max.   :0.11600\n\nFirst I wanted to look at basic correlations for the values. There are a number of ways to run and visualize correlations in r…a few examples follow. First thing I needed to do was a subset of the gosta_audio df for easier calls with the various correlation packages.\nLet’s try correlations in base r. You get the coefficients in the console or you can output to a dataframe to hard-code the visualization.\n\ncor(gbcorr)\n#&gt;                  duration_ms danceability      energy    loudness      tempo\n#&gt; duration_ms       1.00000000   0.03575546 -0.09957649  0.16485951 -0.1589364\n#&gt; danceability      0.03575546   1.00000000 -0.10466026  0.09671649 -0.2719148\n#&gt; energy           -0.09957649  -0.10466026  1.00000000  0.85748849  0.5140085\n#&gt; loudness          0.16485951   0.09671649  0.85748849  1.00000000  0.4952005\n#&gt; tempo            -0.15893636  -0.27191484  0.51400852  0.49520052  1.0000000\n#&gt; valence          -0.04414383  -0.10232090  0.72025346  0.48053791  0.5519247\n#&gt; acousticness     -0.19009855   0.11222116 -0.74742026 -0.65043898 -0.3612391\n#&gt; instrumentalness  0.12784620   0.06977532 -0.53088295 -0.49709651 -0.4411810\n#&gt; liveness         -0.30987073  -0.25213421  0.49374017  0.30054882  0.5316901\n#&gt; speechiness      -0.30678610  -0.31639826  0.45449667  0.27298422  0.4217976\n#&gt;                      valence acousticness instrumentalness   liveness\n#&gt; duration_ms      -0.04414383   -0.1900986       0.12784620 -0.3098707\n#&gt; danceability     -0.10232090    0.1122212       0.06977532 -0.2521342\n#&gt; energy            0.72025346   -0.7474203      -0.53088295  0.4937402\n#&gt; loudness          0.48053791   -0.6504390      -0.49709651  0.3005488\n#&gt; tempo             0.55192475   -0.3612391      -0.44118097  0.5316901\n#&gt; valence           1.00000000   -0.7793878      -0.29646550  0.4743309\n#&gt; acousticness     -0.77938779    1.0000000       0.39266796 -0.3261889\n#&gt; instrumentalness -0.29646550    0.3926680       1.00000000 -0.3406087\n#&gt; liveness          0.47433091   -0.3261889      -0.34060869  1.0000000\n#&gt; speechiness       0.41684028   -0.3150009      -0.56643572  0.7459700\n#&gt;                  speechiness\n#&gt; duration_ms       -0.3067861\n#&gt; danceability      -0.3163983\n#&gt; energy             0.4544967\n#&gt; loudness           0.2729842\n#&gt; tempo              0.4217976\n#&gt; valence            0.4168403\n#&gt; acousticness      -0.3150009\n#&gt; instrumentalness  -0.5664357\n#&gt; liveness           0.7459700\n#&gt; speechiness        1.0000000\ngbcorrs1 &lt;- as.data.frame(cor(gbcorr))\n\nOr you could let some packages do the viz work for you. First, the GGally package, which returns a nice matrix visualization that shows which fields are most postively and negatively correlated.\n\nggcorr(gbcorr, label = TRUE)\n\n\n\n\nWe see here some strong postive associations with energy::loundess returning a .9 coefficient, and liveness::speechiness and energy::valence each returning at .7 coefficient. The energy::acousticness and loudness::acousticness combinations each return a -.7 coefficient, showing a negative relationship between those music features.\nWith the corrr package I tried a couple of approaches. First a basic matrix that prints to the console, and doesn’t look much different than base r.\n\ngbcorr %&gt;%\n  correlate(use = \"pairwise.complete.obs\", method = \"spearman\")\n#&gt; # A tibble: 10 × 11\n#&gt;    term     duration_ms danceability energy loudness  tempo valence acousticness\n#&gt;    &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 duratio…     NA            0.0799 -0.319   -0.189 -0.439  -0.191      -0.0737\n#&gt;  2 danceab…      0.0799      NA      -0.269   -0.124 -0.323  -0.209       0.128 \n#&gt;  3 energy       -0.319       -0.269  NA        0.872  0.658   0.761      -0.725 \n#&gt;  4 loudness     -0.189       -0.124   0.872   NA      0.574   0.458      -0.595 \n#&gt;  5 tempo        -0.439       -0.323   0.658    0.574 NA       0.665      -0.479 \n#&gt;  6 valence      -0.191       -0.209   0.761    0.458  0.665  NA          -0.770 \n#&gt;  7 acousti…     -0.0737       0.128  -0.725   -0.595 -0.479  -0.770      NA     \n#&gt;  8 instrum…      0.135        0.0333 -0.447   -0.586 -0.416  -0.177       0.339 \n#&gt;  9 liveness     -0.319       -0.321   0.319    0.144  0.479   0.488      -0.103 \n#&gt; 10 speechi…     -0.331       -0.715   0.382    0.283  0.640   0.396      -0.209 \n#&gt; # ℹ 3 more variables: instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, speechiness &lt;dbl&gt;\n\nNext, I used their rplot call and then rendered a network graph using the network_plot() call.\n\ngbcorrs2 &lt;- correlate(gbcorr)\nrplot(gbcorrs2)\n\n\n\n   # network graph\ncorrelate(gbcorr) %&gt;% \n  network_plot(min_cor=0.5)\n\n\n\n\nAnd finally the `performance analytics’ package, which was the first of the packages to include significance levels in the default output.\n\n\n\n\n\nGiven the correlations, I was interested in exploring the relationships a bit more. So I ran a few scatterplots, with song titles as data labels, and dots colored by album name (using primary color from the cover) to see also if any of the albums clustered at all along either axis. The ggrepel package is used to move the labels off of the dots.\nThere is a bit of a relationship between the Energy score and Valence - so our more energetic songs are our happiest songs. Another interesting way to explore this would be to do some sentiment analysis on the lyics and see if there’s a relationship between energy, valence and using words considered to be more positive in nature. That’s a project on my to-do list.\n\ngosta_audio %&gt;%\n  ggplot(aes(energy, valence, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) +\n  ylim(0, 1) +\n  xlim(0, 1) +\n  theme_minimal() +\n  labs(x = \"energy\", y = \"valence (happiness)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nNext I wondered if there’s a relationship between song tempo (beats per minute) & happiness. Our average BPM is 131, which isn’t too far the the mid-range of songs on Spotify. The histogram below used to be on the Spotify API page but they don’t seem to have it up anywhere anymore, so found it via the Wayback Machine\nSo let’s see the resulting scatterplot…\n\n\nshow tempo x valence scatterpplot code\ngosta_audio %&gt;%\n  ggplot(aes(tempo, valence, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) + \n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"tempo (bpm)\", y = \"valence (happiness)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\nIt’s not until we get to about the 130 BPM range is it that our songs start to get to even a .25 valence (happiness) score, and from there the relationship between tempo & happiness really kicks in.\nFinally, tempo and energy…\n\n\nshow tempo x energy scatterpplot code\ngosta_audio %&gt;%\n  ggplot(aes(tempo, energy, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) +\n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"tempo (bpm)\", y = \"energy\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\nSo yes, most of our songs are in the bottom half of the happy scale. And there does seem to be a bit of a relationship between tempo, energy and happiness and of course a relationship between tempo and energy. Going forward, I’d love to explore our song lyrics via text analysis, especially sentiment analysis to see if the songs Spotify classified as our most sad (low valence) had lyrics that were less positive.\nSo if you like slightly melancholy mood-pop that’s in the 130 +/- BPM range (think The National, Radiohead), I think you’ll like us.\nThanks for reading. And again, give us a listen, and maybe buy some music if you like. :)\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/exploring-happiness-analysis/index.html",
    "href": "posts/exploring-happiness-analysis/index.html",
    "title": "Exploring Happiness - Analysis",
    "section": "",
    "text": "Coming soon…for now, enjoy this picture of a happy cat on a balcony\n\n\n\nOur cat Leo, happy in the sun\n\n\nNow that we’re done with basic exploratory analysis on the World Happiness Report\nPredicting the happiness score  Ok, let’s run a couple of models to predict the happiness score and plot the results.\nBut why, you might ask, since the WHR authors already did that? Well, I’ve never really run regression models in r. I did in SAS for one job, and I haven’t had to since then. So join me as I walk through running a model and plotting the results."
  },
  {
    "objectID": "posts/call-me-by-my-name/index.html",
    "href": "posts/call-me-by-my-name/index.html",
    "title": "Call Me By My Name",
    "section": "",
    "text": "The actor Ryan O’Neal died on December 8, 2023. If you’re of a certain age and a film fan you might know him best from a pretty good run in the 1970s of very diverse films from Love Story, to the Peter Bogdonavich gems What’s Up Doc and Paper Moon, and then working with Stanley Kubrick in Barry Lyndon and Richard Attenborough in A Bridge Too Far. Not to mention the under-rated The Driver. If you’re of a certain age but more into celebrity gossip you might know him best from dating Farrah Fawcett and being John McEnroe’s father-in-law.\nIf your name is Ryan, like my friend Ryan Godfrey, you will know him not only from films (Ryan G watches a lot of movies) but for arguably launching the name Ryan into the American babysphere. Pop culture does sometimes influence what we name our kids (do you know anyone with a kid born in the last 10 years who named them Arya? I know at least one), and Ryan (Godfrey’s) post here:\n\n\n\nPopularity of the first name “Ryan”\n\n\n…showed that while correlation may not always be causation, Ryan was mostly a last name until Love Story hit big and made Ryan O’Neal a star. And ironically, Ryan wasn’t even Ryan O’Neal’s given first name…it was Charles.\nYou can go to the US Social Security Administration’s (SSA) baby names page and check for yourself what are the popular names in any given year, or track the popularity of names over time. You can also download data and do your own analysis. Or you can use the babynames package created by Hadley Wickham of Posit (formerly RStudio). Which is what we’ll do here to look at a few names of interest.\nThe SSA data is, as they note, based on applications for Social Security numbers (SSN). The dataset includes only SSN applciations for people born in the US and excludes any name counts below 5 in any given year.\nSo let’s check out how bay names have trended in the US.\nTo start we’ll load packages to import and clean the data. These are the three packages I use for almost every analysis in r, plus the babynames package.\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(babynames) # pacakge with name data from US Social Security Admin\n\nThe package comes with a few datasets. We will use the babynames set, which has year of birth, sex, name, number, and proportion of people of that sex with that name.\nUnlike most of my posts, I won’t be doing any data transformation or fancy visuals…I just want to see how a few names have trended over the years. First, let’s replicate Ryan G’s chart, and look at the name “Ryan”. We’ll start at 1940 since we already know from Ryan G’s chart when it took off.\nBefore we continue, a couple of caveats…\n\nThere was a baby boom from the late 1940s onward, so even with the dip after 1965 there have been many more births and more people getting SSNs than pre WWII. Many older workers born well before Social Security was a thing never applied for numbers so weren’t in the system. So…\nA more nuanced analysis would be proportion of births with the names. But this isn’t meant to be nuanced or statisitically significant, just a quick look at how name popularity has changed over time in the US.\n\nFrom here on in the r code is folded, so click the arrow to the left of the text above the plot to expand the code window.\n\n\nShow code for Ryans since 1940\n# read in babynames data from package as a local set\nnamesdf &lt;- babynames\n\nnamesdf %&gt;%\n    filter(sex == \"M\") %&gt;%\n    filter(name == \"Ryan\") %&gt;%\n    filter(year &gt; 1940) %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    scale_x_continuous(\n        breaks = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010), \n        labels = c(\"1940\", \"1950\", \"1960\", \"1970\", \"1980\", \"1990\", \"2000\", \"2010\")) +\n    scale_y_continuous(\n        breaks = c(0, 5000, 10000, 15000, 20000, 25000, 30000), \n        labels = c(\"0\", \"5000\", \"10000\", \"15000\", \"20000\", \"25000\", \"30000\")) +\n    theme_minimal() +\n    labs(title = \"Ryan becomes a popular boy's name in the 1970s and after\", x= \"\", y = \"\")\n\n\n\n\n\nSo yes, as Ryan G wrote, the name Ryan had a slight bump from Peyton Place but the big (baby) bump came when Love Story came out in 1970 and continued through the 1970s and 1980s while Ryan O’Neal’s celebrity status was at its peak.\nWe can see wisps of Ryans before 1970, and because the numbers hit almost 30,000 pre-1970 is a bit compressed. So let’s isolate those years…\n\n\nShow code for Ryans before 1970\nnamesdf %&gt;%\n    filter(sex == \"M\") %&gt;%\n    filter(name == \"Ryan\") %&gt;%\n    filter(year &lt; 1970) %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    labs(title = \"Fewer than 200 Ryans prior to late 1950s\", x= \"\", y = \"\")\n\n\n\n\n\nFewer than 200 up until the late 1950s, then modest increases after O’Neal starred in the night-time sopa opera Peyton Place from 1964 to 1969.\nBut wait, we’re only looking at boys named Ryan. Do you know any women named Ryan? You might…\n\n\nShow code for girl Ryans since 1940\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(year &gt; 1940) %&gt;%\n    filter(name == \"Ryan\") %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"gold\") +\n    theme_minimal() +\n    labs(title = \"Ryan...not just for boys\", x= \"\", y = \"\")\n\n\n\n\n\nThinking about O’Neal’s effect on the name Ryan, I got to thinking about the name of his daughter Tatum. She won an Oscar for Paper Moon so I wondered if the name became more popular in the early-mid 1970…\n\n\nShow code for Tatums\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(name == \"Tatum\") %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    labs(title = \"More Tatums in the 1970s, lots more in the 2000s\", x= \"\", y = \"\")\n\n\n\n\n\n…and sure neough it did. Not to the same extent as Ryan, but a definite pop from 1973 on. After Paper Moon she was in The Bad News Bears, International Velvet and Little Darlings. So like her dad had a run of fame through the early 1980s. But why the jump in the name’s popularity in the late 1990s and through the 2000s? Her very public relationship and marriage to tennis pro John McEnroe? She didn’t do much film or TV work until the early-mid 2000s. So that resurgence, combined with a nostaliga bump? Some other famous Tatum I’m overlooking?\nAll this name trending chatter reminded me of my own firmly held belief which is that it’s very statistically likely a Gen X man or woman in the US will have dated at least one Jennifer in their lives. I’ve dated more than one. It’s almost unavoidable. Why? Well let’s look at birth numbers…\n\n\nShow code for Jennifers\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(name == \"Jennifer\") %&gt;%\n    filter(year &gt;= 1940) %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    scale_x_continuous(\n        breaks = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010), \n        labels = c(\"1940\", \"1950\", \"1960\", \"1970\", \"1980\", \"1990\", \"2000\", \"2010\")) +\n    theme_minimal() +\n    labs(title = \"The Love Story effect on Jennifers\", x= \"\", y = \"\")\n\n\n\n\n\nJennifer had been on the rise from the 1940s onward, slow and steady. But boomed around 1970. And hey, the female lead in Love Story was named Jenny. Coincidence? Or was Love Story more of a cultural juggernaut than we give it credit 50+ years on? I’d say sorry, but…\nLately the name has fallen out of favor. And it’s striking how normally distributed the curve is, given the timeframe.\nAnd finally, in the interest of marital harmony, and because I showed these charts to my wife Ingrid when I made them just for fun, and then she asked what about her name…well I can’t leave her chart out of the post, right? So here’s the name Ingrid over time.\n\n\nShow code for Ingrids\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(name == \"Ingrid\") %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    labs(title = \"Ingrid fluctuates over time\", x= \"\", y = \"\")\n\n\n\n\n\nSo there you go. There’s much more to do…download the most recent sets, match names to regions and states. Look at other countries either through r packages like ukbabynames or use packages or API calls to national statistical services."
  },
  {
    "objectID": "posts/exploring-happiness-analysis/index.html#i-accidentally-pushed-this-to-github-way-too-early-and-it-published-and-now-for-some-reason-despite-trying-to-delete-i-cant-make-it-disappear-from-the-blog.-ill-finish-the-post-soon-so-until-i-can-either-delete-or-finish-consider-this-a-work-in-progress",
    "href": "posts/exploring-happiness-analysis/index.html#i-accidentally-pushed-this-to-github-way-too-early-and-it-published-and-now-for-some-reason-despite-trying-to-delete-i-cant-make-it-disappear-from-the-blog.-ill-finish-the-post-soon-so-until-i-can-either-delete-or-finish-consider-this-a-work-in-progress",
    "title": "Exploring Happiness - Part 2…Analysis",
    "section": "I accidentally pushed this to github way too early and it published, and now for some reason despite trying to delete I can’t make it disappear from the blog. I’ll finish the post soon, so until I can either delete or finish, consider this a work in progress",
    "text": "I accidentally pushed this to github way too early and it published, and now for some reason despite trying to delete I can’t make it disappear from the blog. I’ll finish the post soon, so until I can either delete or finish, consider this a work in progress\nIn Part 1 I did some exploratory data analysis on the World Happiness Report data for Chapter 2 of the 2023 edition.\nThat post got me thinking a bit about being more deliberate about my own initial workflow. Some people have project templates, but I mostly just need a starter script template. So I added some lines to my template script to really ingrain the habit of using those packages for the EDA phase. It’s below…feel free to borrow and of course modify to work\n\n\ncode for my r script template\n### template for r analysis work. save as w/ appropriate name to project directory\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\n# EDA tools\nlibrary(DataExplorer)\nlibrary(explore)\nlibrary(skimr)\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n# sets theme as default for all plots\ntheme_set(theme_light)\n\n## ggplot helpers - load if necessary\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot text\n\n\n### load data\n\n\n### clean data, redo as necesary after running basic EDA\n\n\n\n### EDA with DataExplorer, explore, skimr\n\n## DataExplorer summary of completes, missings\neda1 &lt;- introduce(DATA)\nview(eda1)\n\n## explorer summary\nwhr23_fig2_1 %&gt;%\n    describe_tbl()\n\n## skimr summary\nDATA %&gt;%\n    select() %&gt;%\n    skim()\n\n## go back and clean if issues seen here ##\n\n## dataexplorer plots\nplot_bar(DATA)\nplot_histogram(DATA, nrow = 5L)\n\n## dataexplorer correlations\nDATA %&gt;%\n    select(or deselect as needed) %&gt;%\n    filter(!is.na(if needed)) %&gt;%\n    plot_correlation(maxcat = 5L, type = \"continuous\", geom_text_args = list(\"size\" = 4))\n\n## dataexplorer scatterplots\nplot_scatterplot(\n    DATA %&gt;% select(), by = \"choose target for y axis\", nrow = 3L)\n\n## explorer shiny app\nexplore(DATA %&gt;%\n                    select())\n\n### continue with deeper analysis here or start new r script\n\n\nBut let’s get back to examining what makes people happy. By way of a quick recap:\n\nThe WHR comes from an annual world-wide survey administered by Gallup and is published by the Sustainable Development Solutions Network.\nData for Chapter 2 are made available every year, and include a 3-year rolling average of the ladder happiness score aggregated by country, other questions from the poll, and logged GDP for the country.\n\nThe EDA in part 1 showed:\n\nThere are not many missing values in the data.\nAll of the component variables except generosity have strong positive correlations with the happiness score.\nPerception of corruption has, as expected, a negative association with happiness.\n\nLeading me to want to explore relationship between the ladder score and components by country and region.\nBut first…I spaced on adding one varaible to the dataset, the GINI index, which measures “the extent to which the distribution of income (or, in some cases, consumption expenditure) among individuals or households within an economy deviates from a perfectly equal distribution….a Gini index of 0 represents perfect equality, while an index of 100 implies perfect inequality.” (from the World Bank GINI data page, click the Details tab)\nIt was referenced in the statistical appendix but was not included in the dataset made avaialble. The Chapter 2 authors used two GINI values:\n\na GINI of household income as reported to the Gallup survey and imputed via a STATA function, and\nthe World Bank’s GINI value, taken as the mean of GINI values from 2000 to 2022 to account for the spotty nature of by-country GINI values.\n\nWe can’t do the Gallup & STATA generated GINI because we can’t get the data and I don’t have STATA. So we’ll get the World Bank values, and add that into the data already created. Then some quick EDA and the analysis we came here to do.\nWe’ll start by loading the packages we’ll use here, as well as the WHR data we already created.\n\n\ncode for loading packages and WHR data\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\n# functions I use often enough to load them regularly...should probably write a personal package\nsource(\"~/Data/r/basic functions.R\")\n\n# load data \nwhr23_fig2_1a &lt;- readRDS(\n    file = \"~/Data/r/World Happiness Report/data/whr23_fig2_1.rds\") %&gt;%\n        filter(!is.na(ladder_score)) %&gt;%\n    rename(region_whr = region)\n\n\nTo get the GINI values you can go to the World Bank’s GINI page and downloaded the latest spreadsheet, or…\n…even better you could use one of the helpful r packages to get World Bank data; Vincent Arel-Bundock’s(http://arelbundock.com) WDI and the Geospatial Science and Human Security at Oak Ridge National Lab’s wbstats. We’ll use the WDI package here.\nI’ll create a GINI average as per the WHR authors and I’ll also extract the latest GINI value from the set. I’m not sure right now which is ultimately the best to use, but that’s what a bit more EDA is for.\n\n\ncode for loading GINI values\n# using the \"extra = TRUE\" option pulls in capital city & latitude longitude, \n# region, and World Bank income & lending indicators. We won't use them in this analysis but it's\n# worth knowing they're there.\nginis = WDI::WDI(indicator='SI.POV.GINI', start=2000, end=2023, extra = TRUE) %&gt;%\n        as_tibble() %&gt;%\n    select(-status, -lastupdated) %&gt;%\n    # fix missing regions \n    mutate(region =\n                    case_when(country == \"Czechia\" ~ \"Europe & Central Asia\",\n                                        country == \"Viet Nam\" ~ \"East Asia & Pacific\",\n                                        TRUE ~ region)) %&gt;%\n    # remove aggregated regions \n    filter(region != \"Aggregates\")%&gt;%\n    arrange(country, year) %&gt;%\n    rename(gini = SI.POV.GINI) %&gt;%\n    # create the latest and average columns\n    mutate(ginifill = gini) %&gt;%\n    group_by(country) %&gt;%\n    mutate(gini_avg = mean(gini, na.rm = TRUE)) %&gt;%\n    fill(ginifill, .direction = \"downup\") %&gt;%\n    ungroup() %&gt;%\n    filter(year == 2022) %&gt;%\n    mutate(gini_latest = ifelse(is.na(gini), ginifill, gini)) %&gt;%\n    select(country:gini, gini_latest, ginifill, gini_avg, everything()) %&gt;%\n    rename(country_name = country) %&gt;%\n    # clean up some country names to match with WHR data\n    mutate(country_name =\n                    case_when(country_name == \"Czechia\" ~ \"Czech Republic\",\n                                        country_name == \"Congo, Dem. Rep.\" ~ \"Congo (Kinshasa)\",\n                                        country_name == \"Congo, Rep.\" ~ \"Congo (Brazzaville)\",\n                                        country_name == \"Cote d'Ivoire\" ~ \"Ivory Coast\",\n                                        country_name == \"Egypt, Arab Rep.\" ~ \"Egypt\",\n                                        country_name == \"Eswatini\" ~ \"Swaziland\",\n                                        country_name == \"Gambia, The\" ~ \"Gambia\",\n                                        country_name == \"Hong Kong SAR, China\" ~ \"Hong Kong S.A.R. of China\",\n                                        country_name == \"Iran, Islamic Rep.\" ~ \"Iran\",\n                                        country_name == \"Korea, Rep.\" ~ \"South Korea\",\n                                        country_name == \"Kyrgyz Republic\" ~ \"Kyrgyzstan\",\n                                        country_name == \"Lao PDR\" ~ \"Laos\",\n                                        country_name == \"Russian Federation\" ~ \"Russia\",\n                                        country_name == \"Slovak Republic\" ~ \"Slovakia\",\n                                        country_name == \"Turkiye\" ~ \"Turkey\",\n                                        country_name == \"Venezuela, RB\" ~ \"Venezuela\",\n                                        country_name == \"Viet Nam\" ~ \"Vietnam\",\n                                        country_name == \"West Bank and Gaza\" ~ \"Palestinian Territories\",\n                                        country_name == \"Yemen, Rep.\" ~ \"Yemen\",\n                                        TRUE ~ country_name)) \n\n\nNow let’s add the GINI numberss to the WHR data we already have…\n\n\ncode for joining GINI to WHR data\nwhr23_fig2_1 &lt;- whr23_fig2_1a %&gt;%\n    merge(ginis, all = TRUE) %&gt;%\n    as_tibble() %&gt;%\n    select(country_name, iso3c, region, region_whr, whr_year:lowerwhisker, logged_gdp_per_capita,\n                 gini_avg, gini_latest, everything()) %&gt;%\n    ## fill in Taiwan, no longer in this set but still available \n    ### at https://pip.worldbank.org/country-profiles/TWN\n    mutate(gini_avg = ifelse(\n        country_name == \"Taiwan Province of China\", 32.09833333, gini_avg)) %&gt;%\n    mutate(gini_latest = ifelse(\n        country_name == \"Taiwan Province of China\", 31.48, gini_latest)) %&gt;%\n    filter(!is.na(ladder_score))\n\n\nN.B…yes, I use merge() and not left/right/full_join()…I learned SAS before SQL and old habits die hard (shrug emoji).\nN.B.2…Taiwan data is no longer avaialble in most WB sets, either in the spreadsheet or via the API the packages access. It is still at the WB’s Poverty & Inequality Platform so I downloaded what I could and hard-coded. Why? Because I’m a little OCD when it comes to trying to minimize missing data.\nAnyway…now we have to do some quick EDA on the GINI values in relation to the data we have. First I want to take a quick look at the by-country difference between the latest GINI value and the mean value for the period since 2000. We subtract the average from the latest value, do a quick skimr check, a density plot on the latest-average difference…\n\n\ncode for EDA skim & density plot\n# a quick skim \nwhr23_fig2_1 %&gt;%\n    mutate(gini_diff = gini_latest - gini_avg) %&gt;%\n    select(gini_latest, gini_avg, gini_diff) %&gt;%\n    skimr::skim()\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n137\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngini_latest\n7\n0.95\n36.73\n7.60\n23.20\n31.54\n35.30\n40.88\n63.00\n▃▇▃▁▁\n\n\ngini_avg\n7\n0.95\n38.09\n7.88\n24.79\n32.26\n36.68\n42.71\n62.40\n▆▇▅▂▁\n\n\ngini_diff\n7\n0.95\n-1.37\n2.32\n-8.91\n-2.85\n-1.00\n0.16\n4.37\n▁▂▇▇▁\n\n\n\n\n\ncode for EDA skim & density plot\n\nwhr23_fig2_1 %&gt;%\n    filter(!is.na(gini_avg)) %&gt;%\n    mutate(gini_diff = gini_latest - gini_avg) %&gt;%\n    select(country_name, gini_latest, gini_avg, gini_diff, region_whr) %&gt;%\n    arrange(gini_diff) %&gt;%\n    ggplot(aes(gini_diff)) +\n                    geom_density(fill = \"blue\") +\n    xlim(-9, 6)\n\n\n\n\n\nQuick EDA observations from the skim and denisty plot:\n\nThe GINI scale is 0 to 100, and the spread in this WHR set is 24.7 to 62.4, almost exactly as the entire panel from the WB GINI dataset.\nThe means, medians, standard deviations, and ranges for average and latest are close enough.\nThe difference (latest - average) is looks on the denisty plot to be clustered just below 0, and the median difference is only -1, so we’ll use the average.\nWe only lose 7 countries from the set by adding GINI.\n\nSo let’s use the average and see how it relates to our WHR variables by doing first a correlation matrix from the DataExplorer package…\n\n\ncode for EDA correlation\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, standard_error_of_ladder_score, gini_avg, logged_gdp_per_capita,\n                 social_support:perceptions_of_corruption,\n                  explained_by_log_gdp_per_capita:residual) %&gt;%\n    filter(!is.na(residual)) %&gt;%\n    filter(!is.na(gini_avg)) %&gt;%\n    DataExplorer::plot_correlation(maxcat = 5L, type = \"continuous\", geom_text_args = list(\"size\" = 3))\n\n\n\n\n\n…and the DataExplorer scatterplots.\n\n\ncode for EDA scatterplots\nDataExplorer::plot_scatterplot(\n    whr23_fig2_1 %&gt;% select(gini_avg, ladder_score, logged_gdp_per_capita,\n                                                    social_support:perceptions_of_corruption), \n    by = \"gini_avg\", nrow = 3L)\n\n\n\n\n\nWe can see from the correlation matrix and the scatterplots that:\n\nthere is a mild but persistent relationship between a lower GINI score (less inequality) and the measures that correlate with more happines…the happiness score itself, life expectancy, freedom to make choices, etc.\nThe one negative relationship was with perceptions of corruption, or the higher the inequality measure in their country, the more likely people answering the survey were to report that they perceived higher levels of corruption.\n\nAlso, interestingly, a lower GINI index correlated to a higher GDP per capita…in other words, it’s better for all to spread the wealth.\nPredicting the happiness score  Ok, let’s run a couple of models to predict the happiness score and plot the results.\nBut why, you might ask, since the WHR authors already did that? Well, I’ve never really run regression models in r. I did in SAS for one job, and I haven’t had to since then. So join me as I walk through running a model and plotting the results.\nI had hoped to pull in a few other indicators to add to the model. The main one I wanted was literacy. Unfortunately there were too many missing values in the UNESCO set…most European countries and a few other key countries. I then thought about public expenditure on education but was worried about colinearity with the GINI index (no, I didn’t test it). So we’ll stick with what we have and not cloud up the model with too many exogenous variables."
  },
  {
    "objectID": "posts/exploring-happiness-eda-pt2/index.html",
    "href": "posts/exploring-happiness-eda-pt2/index.html",
    "title": "Exploring Happiness - EDA Part 2",
    "section": "",
    "text": "Happiness in a cone\n\n\nIn Part 1 I did some exploratory data analysis on the World Happiness Report data for Chapter 2 of the 2023 edition.\nThat post got me thinking a bit about being more deliberate about my own initial workflow. Some people have project templates, but I mostly just need a starter script template. So I added some lines to my template script to really ingrain the habit of using those packages for the EDA phase. It’s below…feel free to borrow and of course modify to work\n\n\ncode for my r script template\n### template for r analysis work. save as w/ appropriate name to project directory\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\n# EDA tools\nlibrary(DataExplorer)\nlibrary(explore)\nlibrary(skimr)\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n# sets theme as default for all plots\ntheme_set(theme_light)\n\n## ggplot helpers - load if necessary\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot text\n\n\n### load data\n\n\n### clean data, redo as necesary after running basic EDA\n\n\n\n### EDA with DataExplorer, explore, skimr\n\n## DataExplorer summary of completes, missings\neda1 &lt;- introduce(DATA)\nview(eda1)\n\n## explorer summary\nwhr23_fig2_1 %&gt;%\n    describe_tbl()\n\n## skimr summary\nDATA %&gt;%\n    select() %&gt;%\n    skim()\n\n## go back and clean if issues seen here ##\n\n## dataexplorer plots\nplot_bar(DATA)\nplot_histogram(DATA, nrow = 5L)\n\n## dataexplorer correlations\nDATA %&gt;%\n    select(or deselect as needed) %&gt;%\n    filter(!is.na(if needed)) %&gt;%\n    plot_correlation(maxcat = 5L, type = \"continuous\", geom_text_args = list(\"size\" = 4))\n\n## dataexplorer scatterplots\nplot_scatterplot(\n    DATA %&gt;% select(), by = \"choose target for y axis\", nrow = 3L)\n\n## explorer shiny app\nexplore(DATA %&gt;%\n                    select())\n\n### continue with deeper analysis here or start new r script\n\n\nBut let’s get back to examining what makes people happy. By way of a quick recap:\n\nThe WHR comes from an annual world-wide survey administered by Gallup and is published by the Sustainable Development Solutions Network.\nData for Chapter 2 are made available every year, and include a 3-year rolling average of the ladder happiness score aggregated by country, other questions from the poll, and logged GDP for the country.\n\nThe EDA in part 1 showed:\n\nThere are not many missing values in the data.\nAll of the component variables except generosity have strong positive correlations with the happiness score.\nPerception of corruption has, as expected, a negative association with happiness.\n\nBut…I spaced on adding one varaible to the dataset, the GINI index, which measures “the extent to which the distribution of income (or, in some cases, consumption expenditure) among individuals or households within an economy deviates from a perfectly equal distribution….a Gini index of 0 represents perfect equality, while an index of 100 implies perfect inequality.” (from the World Bank GINI data page, click the Details tab)\nIt was referenced in the statistical appendix but was not included in the dataset made available. The Chapter 2 authors used two GINI values:\n\na GINI of household income as reported to the Gallup survey and imputed via a STATA function, and\nthe World Bank’s GINI value, taken as the mean of GINI values from 2000 to 2022 to account for the spotty nature of by-country GINI values.\n\nWe can’t do the Gallup & STATA generated GINI because we can’t get the data and I don’t have STATA. So we’ll get the World Bank values, and add that into the data already created. Then we’ll do some more EDA looking at how GINI relates to variables in the set. I was going to do some more in-depth analysis in this post, but I want to keep posts short and focused. So analysis will be in a separate post.\nWe’ll start by loading the packages we’ll use here, as well as the WHR data we already created.\n\n\ncode for loading packages and WHR data\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\n# functions I use often enough to load them regularly...should probably write a personal package\nsource(\"~/Data/r/basic functions.R\")\n\n# load data \nwhr23_fig2_1a &lt;- readRDS(\n    file = \"~/Data/r/World Happiness Report/data/whr23_fig2_1.rds\") %&gt;%\n        filter(!is.na(ladder_score)) %&gt;%\n    rename(region_whr = region)\n\n\nTo get the GINI values you can go to the World Bank’s GINI page and downloaded the latest spreadsheet, or…\n…even better you could use one of the helpful r packages to get World Bank data; Vincent Arel-Bundock’s(http://arelbundock.com) WDI and the Geospatial Science and Human Security at Oak Ridge National Lab’s wbstats. We’ll use the WDI package here.\nI’ll create a GINI average as per the WHR authors and I’ll also extract the latest GINI value from the set. I’m not sure right now which is ultimately the best to use, but that’s what a bit more EDA is for.\n\n\ncode for loading GINI values\n# using the \"extra = TRUE\" option pulls in capital city & latitude longitude, \n# region, and World Bank income & lending indicators. We won't use them in this analysis but it's\n# worth knowing they're there.\nginis = WDI::WDI(indicator='SI.POV.GINI', start=2000, end=2023, extra = TRUE) %&gt;%\n        as_tibble() %&gt;%\n    select(-status, -lastupdated) %&gt;%\n    # fix missing regions \n    mutate(region =\n                    case_when(country == \"Czechia\" ~ \"Europe & Central Asia\",\n                                        country == \"Viet Nam\" ~ \"East Asia & Pacific\",\n                                        TRUE ~ region)) %&gt;%\n    # remove aggregated regions \n    filter(region != \"Aggregates\")%&gt;%\n    arrange(country, year) %&gt;%\n    rename(gini = SI.POV.GINI) %&gt;%\n    # create the latest and average columns\n    mutate(ginifill = gini) %&gt;%\n    group_by(country) %&gt;%\n    mutate(gini_avg = mean(gini, na.rm = TRUE)) %&gt;%\n    fill(ginifill, .direction = \"downup\") %&gt;%\n    ungroup() %&gt;%\n    filter(year == 2022) %&gt;%\n    mutate(gini_latest = ifelse(is.na(gini), ginifill, gini)) %&gt;%\n    select(country:gini, gini_latest, ginifill, gini_avg, everything()) %&gt;%\n    rename(country_name = country) %&gt;%\n    # clean up some country names to match with WHR data\n    mutate(country_name =\n                    case_when(country_name == \"Czechia\" ~ \"Czech Republic\",\n                                        country_name == \"Congo, Dem. Rep.\" ~ \"Congo (Kinshasa)\",\n                                        country_name == \"Congo, Rep.\" ~ \"Congo (Brazzaville)\",\n                                        country_name == \"Cote d'Ivoire\" ~ \"Ivory Coast\",\n                                        country_name == \"Egypt, Arab Rep.\" ~ \"Egypt\",\n                                        country_name == \"Eswatini\" ~ \"Swaziland\",\n                                        country_name == \"Gambia, The\" ~ \"Gambia\",\n                                        country_name == \"Hong Kong SAR, China\" ~ \"Hong Kong S.A.R. of China\",\n                                        country_name == \"Iran, Islamic Rep.\" ~ \"Iran\",\n                                        country_name == \"Korea, Rep.\" ~ \"South Korea\",\n                                        country_name == \"Kyrgyz Republic\" ~ \"Kyrgyzstan\",\n                                        country_name == \"Lao PDR\" ~ \"Laos\",\n                                        country_name == \"Russian Federation\" ~ \"Russia\",\n                                        country_name == \"Slovak Republic\" ~ \"Slovakia\",\n                                        country_name == \"Turkiye\" ~ \"Turkey\",\n                                        country_name == \"Venezuela, RB\" ~ \"Venezuela\",\n                                        country_name == \"Viet Nam\" ~ \"Vietnam\",\n                                        country_name == \"West Bank and Gaza\" ~ \"Palestinian Territories\",\n                                        country_name == \"Yemen, Rep.\" ~ \"Yemen\",\n                                        TRUE ~ country_name)) \n\n\nI had hoped to pull in a few other indicators to add to the model. The main one I wanted was literacy. Unfortunately there were too many missing values in the UNESCO set…most European countries and a few other key countries. I then thought about public expenditure on education but was worried about colinearity with the GINI index (no, I didn’t test it). So we’ll stick with what we have and not cloud up the model and other analysis with too many exogenous variables. If this were a\nSo let’s add the GINI numbers to the WHR data we already have…\n\n\ncode for joining GINI to WHR data\nwhr23_fig2_1 &lt;- whr23_fig2_1a %&gt;%\n    merge(ginis, all = TRUE) %&gt;%\n    as_tibble() %&gt;%\n    select(country_name, iso3c, region, region_whr, whr_year:lowerwhisker, logged_gdp_per_capita,\n                 gini_avg, gini_latest, everything()) %&gt;%\n    ## fill in Taiwan, no longer in this set but still available \n    ### at https://pip.worldbank.org/country-profiles/TWN\n    mutate(gini_avg = ifelse(\n        country_name == \"Taiwan Province of China\", 32.09833333, gini_avg)) %&gt;%\n    mutate(gini_latest = ifelse(\n        country_name == \"Taiwan Province of China\", 31.48, gini_latest)) %&gt;%\n    filter(!is.na(ladder_score))\n\n\nN.B…yes, I use merge() and not left/right/full_join()…I learned SAS before SQL and old habits die hard (shrug emoji).\nN.B.2…Taiwan data is no longer avaialble in most WB sets, either in the spreadsheet or via the API the packages access. It is still at the WB’s Poverty & Inequality Platform so I downloaded what I could and hard-coded. Why? Because I’m a little OCD when it comes to trying to minimize missing data.\nAnyway…now we have to do some quick EDA on the GINI values in relation to the data we have. First I want to take a quick look at the by-country difference between the latest GINI value and the mean value for the period since 2000. We subtract the average from the latest value, do a quick skimr check, a density plot on the latest-average difference…\n\n\ncode for EDA skim & density plot\n# a quick skim \nwhr23_fig2_1 %&gt;%\n    mutate(gini_diff = gini_latest - gini_avg) %&gt;%\n    select(gini_latest, gini_avg, gini_diff) %&gt;%\n    skimr::skim()\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n137\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngini_latest\n7\n0.95\n36.73\n7.60\n23.20\n31.54\n35.30\n40.88\n63.00\n▃▇▃▁▁\n\n\ngini_avg\n7\n0.95\n38.09\n7.88\n24.79\n32.26\n36.68\n42.71\n62.40\n▆▇▅▂▁\n\n\ngini_diff\n7\n0.95\n-1.37\n2.32\n-8.91\n-2.85\n-1.00\n0.16\n4.37\n▁▂▇▇▁\n\n\n\n\n\ncode for EDA skim & density plot\n\nwhr23_fig2_1 %&gt;%\n    filter(!is.na(gini_avg)) %&gt;%\n    mutate(gini_diff = gini_latest - gini_avg) %&gt;%\n    select(country_name, gini_latest, gini_avg, gini_diff, region_whr) %&gt;%\n    arrange(gini_diff) %&gt;%\n    ggplot(aes(gini_diff)) +\n                    geom_density(fill = \"blue\") +\n    xlim(-9, 6)\n\n\n\n\n\nQuick EDA observations from the skim and denisty plot:\n\nThe GINI scale is 0 to 100, and the spread in this WHR set is 24.7 to 62.4, almost exactly as the entire panel from the WB GINI dataset.\nThe means, medians, standard deviations, and ranges for average and latest are close enough.\nThe difference (latest - average) looks on the denisty plot to be clustered just below 0, and the median difference is only -1, so we’ll use the average.\nWe only lose 7 countries from the set by adding GINI.\n\nSo let’s use the average and see how it relates to our WHR variables by doing first a correlation matrix from the DataExplorer package…\n\n\ncode for EDA correlation\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, standard_error_of_ladder_score, gini_avg, logged_gdp_per_capita,\n                 social_support:perceptions_of_corruption,\n                  explained_by_log_gdp_per_capita:residual) %&gt;%\n    filter(!is.na(residual)) %&gt;%\n    filter(!is.na(gini_avg)) %&gt;%\n    DataExplorer::plot_correlation(maxcat = 5L, type = \"continuous\", geom_text_args = list(\"size\" = 3))\n\n\n\n\n\n…and the DataExplorer scatterplots.\n\n\ncode for EDA scatterplots\nDataExplorer::plot_scatterplot(\n    whr23_fig2_1 %&gt;% select(gini_avg, ladder_score, logged_gdp_per_capita,\n                                                    social_support:perceptions_of_corruption), \n    by = \"gini_avg\", nrow = 3L)\n\n\n\n\n\nWe can see from the correlation matrix and the scatterplots that:\n\nThere is a mild but persistent relationship between a lower GINI score (less inequality) and the measures that correlate with more happiness…the happiness ladder score itself, life expectancy, freedom to make choices, etc.\nThe one negative relationship was with perceptions of corruption - that is, the higher the inequality measure in their country the more likely people answering the survey were to report that they perceived higher levels of corruption.\nAnd interestingly, a lower GINI index correlated to a higher GDP per capita…so maybe it’s better for all to spread the wealth?\n\nThat’s it for basic EDA on the data. The next post will dig a bit deeper…look at some differences by region, and I’ll do a quick regression to try and predict happiness score and see how close I can get to the WHR model."
  }
]