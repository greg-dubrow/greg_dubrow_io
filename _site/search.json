[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gregers kjerulf dubrow",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n30 Day Chart Challenge 2025\n\n\n\n\n\n\npost\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\nchartchallenge\n\n\ndenmark\n\n\neducation\n\n\nhigher education\n\n\n\nLet’s make some charts!\n\n\n\n\n\nApr 3, 2025\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nIf You Sell It, Will They Come?\n\n\n\n\n\n\npost\n\n\nrstats\n\n\nggplot\n\n\nfootball\n\n\n\nExamining football attendance data\n\n\n\n\n\nOct 23, 2024\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\n30 Day Chart Challenge 2024\n\n\n\n\n\n\npost\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\nchartchallenge\n\n\ntidytuesday\n\n\neurostat\n\n\noecd\n\n\nspotify\n\n\ndenmark\n\n\ncrime\n\n\nbirths\n\n\n\nLet’s make some charts!\n\n\n\n\n\nApr 30, 2024\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nMy Year of Riding Danishly pt 2\n\n\n\n\n\n\npost\n\n\nrstats\n\n\nggplot\n\n\nregression\n\n\nols\n\n\nbicycle\n\n\ndenmark\n\n\n\nNow with new residual plots and unique days ridden\n\n\n\n\n\nApr 25, 2024\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Note on Quarto Blogs and GDPR\n\n\n\n\n\n\npost\n\n\nnews\n\n\nprivacy\n\n\nGDPR\n\n\ncookies\n\n\n\nCookies, but only if you want them\n\n\n\n\n\nFeb 20, 2024\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nMy Year of Riding Danishly\n\n\n\n\n\n\npost\n\n\nrstats\n\n\neda\n\n\nggplot\n\n\nregression\n\n\nols\n\n\nbicycle\n\n\ndenmark\n\n\n\nWhen your bike is how how you get everywhere\n\n\n\n\n\nFeb 15, 2024\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nr to Tableau, then show it in Quarto\n\n\n\n\n\n\npost\n\n\nnews\n\n\nrstats\n\n\ntableau\n\n\n\nAdventures in the data multiverse\n\n\n\n\n\nFeb 1, 2024\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Happiness - EDA Part 2\n\n\n\n\n\n\npost\n\n\nrstats\n\n\neda\n\n\n\nAdding a splash of GINI\n\n\n\n\n\nDec 21, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nCall Me By My Name\n\n\n\n\n\n\npost\n\n\nnews\n\n\nrstats\n\n\nbabynames\n\n\n\nRyans and Tatums and Jennifers oh my\n\n\n\n\n\nDec 13, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Happiness - Part 1…EDA\n\n\n\n\n\n\npost\n\n\nrstats\n\n\neda\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nRandom music for the new routine\n\n\n\n\n\n\npost\n\n\nnews\n\n\nrstats\n\n\nmusic\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nMigration from Hugo to Quarto\n\n\n\n\n\n\npost\n\n\nnews\n\n\nhowto\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\n…and we’re back. Migration and resurrection\n\n\n\n\n\n\npost\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nSad Songs & Pretty Charts - a Gosta Berling Music Data Visualization\n\n\n\n\n\n\npost\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nmusic\n\n\nspotify\n\n\ngosta berling\n\n\n\n\n\n\n\n\n\nFeb 28, 2021\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nInvited Talk at University of San Francisco, February 2020\n\n\n\n\n\n\npost\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nhigher education\n\n\n\n\n\n\n\n\n\nFeb 26, 2021\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday, February 2, 2021 - HBCU Enrollment\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nhigher education\n\n\n\n\n\n\n\n\n\nFeb 25, 2021\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday, April 07, 2020 - Le Tour! (Stage 2, charts!)\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nsports\n\n\ncycling\n\n\ntour de france\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday, April 07, 2020 - Le Tour! (Stage 1, cleaning the data)\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nsports\n\n\ncycling\n\n\ntour de france\n\n\n\nA mountain of data about the Tour de France\n\n\n\n\n\nNov 30, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday, November 27, 2018 - Maryland Bridges\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday, November 24, 2020 - Hiking Trails in WA State\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\nNov 27, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Happiness - Analysis\n\n\n\n\n\n\npost\n\n\nnews\n\n\nrstats\n\n\neda\n\n\n\n\n\n\n\n\n\nDec 21, 2019\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "gregers kjerulf dubrow",
    "section": "",
    "text": "I’m a data analyst/data scientist living in København Denmark. I am open to work, be it full-time, contract, or freelance r/statistics and general editing consultation for projects like graduate theses & dissertations or other research projects.\nThough my main subject area expertise is in higher education policy, undergraduate admissions & enrollment management, and student success, the work I have done translates very well to people analytics (employee learning and retention) and business problems like customer churn and retention.\nI work in r for everything from data import & cleaning, to analysis, modeling & visualization. I have also used SAS and excel and have basic working knowledge of SQL & Python and a passing familiarity with Tableau.\nMy favorite part of data work is visualisation and making nice charts & tables. I’m also quite happy to do the data grunt work that is important but less glamorous - cleaning and munging data to get it ready for analysis and machine learning modeling.\nI was born in Denmark and grew up in the US. After many years in San Francisco I lived in Lyon France from May to December 2022 , working as an ESL instructor after earning a CELTA via the ELT Hub.\nIn this space I’ll mostly be posting code-throughs and results from my personal data projects; music, football (the soccer kind), education, #tidytuesday data projects and other work. All views expressed here are my own.\nOver the years I’ve made some music with Gosta Berling, Slowness, Big Still, The Trolleyvox and Idle Wilds. My photography is at Flickr.\nUnless otherwise cited, all photographs used in posts are mine, with a CC BY-NC-ND 4.0 license"
  },
  {
    "objectID": "posts/tidy_tuesday_maryland_bridges/index.html",
    "href": "posts/tidy_tuesday_maryland_bridges/index.html",
    "title": "Tidy Tuesday, November 27, 2018 - Maryland Bridges",
    "section": "",
    "text": "This dataset was posted to the #TidyTuesday repo back in November 2018. I worked on it a bit then, but didn’t properly finish it until March of 2020. With the blog finally set up figured I might as well post it as an entry.\nUpdate for migration from Hugo to Quarto…now that code-fold is native to code chunks, I’ll sometimes use if for long bits of code. Just click the down arrow to show code\n\n# readin in data, create df for plots\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\nFirst let’s read in the file from the raw data file on github\n\ntt_balt_gh <-\nread_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-27/baltimore_bridges.csv\",\n         progress = show_progress())\n\n\n\nOrganize and clean the data for analysis\n\nclean up some date and name issues\na decade-built field, factored for sorting\ntime since last inspection\n\n\n\nshow data cleaning code\n# keeping the comparison date March 21 when I originally did analysis\ntoday <- as.Date(c(\"2020-03-21\"))\n#Sys.Date()\ntoday_yr <- as.numeric(format(today, format=\"%Y\"))\n\ntt_mdbrdf <- as.data.frame(tt_balt_gh) %>%\n  mutate(age = today_yr - yr_built) %>%\n  #  mutate(vehicles_n = as.numeric(str_remove(vehicles, \" vehicles\")))\n  ## not needed, avg_daily_traffic has same info\n  mutate(inspection_yr = inspection_yr + 2000) %>%\n  mutate(county = ifelse(county == \"Baltimore city\", \"Baltimore City\", county)) %>%\n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  mutate(bridge_condition = factor(bridge_condition, levels = c(\"Good\", \"Fair\", \"Poor\"))) %>%\n  mutate(decade_built = case_when(yr_built <= 1899 ~ \"pre 1900\", \n                                  yr_built >= 1900 & yr_built <1910 ~ \"1900-09\",\n                                  yr_built >= 1910 & yr_built <1920 ~ \"1910-19\",\n                                  yr_built >= 1920 & yr_built <1930 ~ \"1920-29\",\n                                  yr_built >= 1930 & yr_built <1940 ~ \"1930-39\",\n                                  yr_built >= 1940 & yr_built <1950 ~ \"1940-49\",\n                                  yr_built >= 1950 & yr_built <1960 ~ \"1950-59\",\n                                  yr_built >= 1960 & yr_built <1970 ~ \"1960-69\",\n                                  yr_built >= 1970 & yr_built <1980 ~ \"1970-79\",\n                                  yr_built >= 1980 & yr_built <1990 ~ \"1980-89\",\n                                  yr_built >= 1990 & yr_built <2000 ~ \"1990-99\",\n                                  yr_built >= 2000 & yr_built <2010 ~ \"2000-09\",\n                                  TRUE ~ \"2010-19\")) %>%\n  mutate(decade_built = factor(decade_built, levels = \n                                 c(\"pre 1900\", \"1900-09\", \"1910-19\", \"1920-29\", \"1930-39\",\n                                   \"1940-49\", \"1950-59\", \"1960-69\", \"1970-79\", \n                                   \"1980-89\", \"1990-99\", \"2000-09\", \"2010-19\"))) %>%\n  mutate(inspect_mmyy = ISOdate(year = inspection_yr, month = inspection_mo, day = \"01\")) %>%\n  mutate(inspect_mmyy = as.Date(inspect_mmyy, \"%m/%d/%y\")) %>%\n  mutate(inspect_days = today - inspect_mmyy) %>%\n  mutate(inspect_daysn = as.numeric(inspect_days)) %>%\n  mutate(inspect_years = inspect_daysn/ 365.25) %>%\n  mutate(inspect_months = inspect_daysn / 30.417)\n\n\n\n\nThe first few charts look at bridges built by decade, the condition of all bridges by county, and how long since last inspection.\n\ntt_mdbrdf %>% \n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  count(decade_built) %>%\n  ggplot(aes(decade_built, n)) +\n  geom_bar(stat = \"identity\", fill = \"navy\") +\n  geom_text(aes(label = n), color = \"white\", vjust = 1.2) +\n  labs(title = \"Peak bridge-building years in Maryland were 1950s to 1990s\" ,\n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nBaltimore City has the lowest percentage of bridges in good condition, Anne Arundel the most. Baltimore City & Harford County seems to have the largest percentage of bridges in poor condition.\n\n\nshow stacked bar chart code\n## percent bridge condition by county\n# need to create df object to do subset label call in bar chart\nbrcondcty <- \ntt_mdbrdf %>%\n  count(county, bridge_condition) %>%\n  group_by(county) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() \n\nggplot(brcondcty, aes(x = county, y = pct, fill = bridge_condition)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(data = subset(brcondcty, bridge_condition != \"Poor\"), \n            aes(label = percent(pct)), position = \"stack\", \n            color= \"#585858\", vjust = 1, size = 3.5) +\n  scale_y_continuous(label = percent_format()) +\n  labs(title = \"Percent bridge condition by county\" , \n        x = \"\", y = \"\", fill = \"Bridge Condition\") +\n  scale_fill_brewer(type = \"seq\", palette = \"Blues\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nGiven the condition percentages in Baltimore County & City and Harford County, it’s no surprise that their bridges are older than in other counties.\n\n\nshow bar chart code\n## median age of bridges by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medage = median(age)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medage)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medage, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median bridge age by county\" , \n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt’s somewhat reassuring then that Baltimore City bridges at least have less time in months since last inspection than do the counties.\n\n\nshow bar chart code\n## median months since last inspection by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medinsp = median(inspect_months)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medinsp)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medinsp, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median months since last inspection, by county\",\n       subtitle = \"as of March 2020\",\n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt might be the outliers pulling the smoothing line straight, but there doesn’t seem to be too much of a relationship between age and time since last inspection.\n\n\nshow scatterplot code\n## age by months since last inspection\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\n\nAnd in fact, removing the outliers shows a slight relationship; the older bridges do seem to get inspected more frequently. In terms of a better visualization, looking at this again, I wonder if some jittering or another type of plot might have been more visually appealing, givne the clustering of most recent inspections.\n\n# same code as above but with outliers removed\ntt_mdbrdf %>%\n  filter(age <150, inspect_months <60) %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(title = \"Months since inspection, outliers removed\", \n       x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\nNot sure if scatter-plot with colors by county is best way to go for this idea. Maybe a tree map?\n\n\nshow scatterplot code\n# same but colors by county\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  labs(title = \"Months since last inspection (from current date)\", \n       x = \"Months since last inspection (from current date)\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.8, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nFunky distributions here…Anne Arundel & Baltimore City have the highest median daily riders, but Howard County’s upper quartile is way out there.\nshowing the code here to illustrate how I like to run the mean & interquartiles in the same code as rendering the plot.\n\n# median & interquartiles of daily riders of bridges by county -\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medtraf = median(avg_daily_traffic),\n            lq = quantile(avg_daily_traffic, 0.25),\n            uq = quantile(avg_daily_traffic, 0.75)) %>%\n  ungroup() %>%\n  ggplot(aes(county, medtraf)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"navy\") +\n  geom_point(size = 3, color = \"orange\", alpha = .8) +\n  geom_text(aes(label = comma(medtraf, digits = 0)), \n            size = 4, color = \"orange\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = comma(uq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = comma(lq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Median & interquartile ranges of average daily riders per bridge, by county\" , \n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nAs with the other scatterplot with colors for county, might need a different way to see relationship between bridge age and daily traffic by county.\n\n\nshow scatterplot code\n## age by avg daily riders by county\ntt_mdbrdf %>%\n  ggplot(aes(avg_daily_traffic, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  scale_x_continuous(labels = comma) +\n  labs(title = \"Average daily traffic per bridge, by county\" , \n        x = \"Average daily traffic\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.75, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nCover image for post (CC BY-SA 4.0) from Wikipedia\nThis post was last updated on 2023-05-14"
  },
  {
    "objectID": "posts/migrating-from-hugo-to-quarto/index.html",
    "href": "posts/migrating-from-hugo-to-quarto/index.html",
    "title": "Migration from Hugo to Quarto",
    "section": "",
    "text": "The Saône & The Rhône converge in Lyon\n\n\nAs I mentioned in the Migration and Resurrection post, some issues with my old Hugo build and Netlify playing nicely with Hugo & Wowchemy led to me deciding to rebuild things in Quarto. I was thinking about it anyway, as Quarto seems robust and even more user-friendly (to me) than blogdown & rmarkdown, which I still love. I just love Quarto more.\nThe basic idea of the blog remains the same - present the code & analysis for topics that interest me. While switching from SAS to r, I learned so much from reading tons of data blogs and watching how-to videos that I wanted to give back to the rstats community by showing and explaining my work. It also helps me as my own reference for code and approach to visualization when I forget how I did a pivot-long or cleaned and set-up data to work well with the visualization I wanted to do.\nI decided to set some ground rules for the old posts:\n\nMinimal text edits…only for typos and occasional clunky language.\nBut no editing the code. The projects I posted showed where I was in using r at the time. Though to be honest, if anything I’ve regressed a bit as I haven’t used r that much since I left my last data job. Besides, the rmarkdown -> quarto migration already entailed enough editing in the YAML and code chunk headers.\nThe only exception to the code edit rule was changing the chunk options to the #| syntax and adding code-fold options to some code chunks. It wasn’t easily doable in blogdown & Hugo when I first launched the site but it’s a native functionality to Quarto, so hoorah!\nRepost the projects with date-stamps from the original posting. I want this to still accurately document my own data analysis/data science progression, even with that long gap between posts.\n\nSo how did I do it?\nFirst I spent a sunny weekend afternoon in May watching two presentations about Quarto.\nThis Welcome to Quarto Workshop from August 2022 led by Thomas Mock.\n\n\n\n\n\n\n\n\n\n\nAfter that it was Isabella Velásquez’s presentation on building a blog with Quarto.\nI also read the extensive Quarto documentation.\nEach of their personal blogs are nicely done in Quarto so I’ll also be poking around their github repos.\nAs I had already working with blogdown & rmarkdown, the transition to Quarto was smooth. Minor grammatical differences in the YAML and code chunks, but nothing that didn’t make sense.\nSetting up the blog is as simple as starting any new project in r. Just go to: File -> New Project -> New Directory -> Quarto blog and fill in the name and directory location.\nAfter setting up the basic design using one of the packaged themes and drafting the landing and About pages, I pushed the new files to github and hoped that Netlify would play nicely and render the new site.\nOn the first commit, which wiped out all of my old content and replaced with the new files, Netlify did its thing but I got the dreaded Error 404, site not found. With a little digging I found out that I had to go the Build & Deploy -> Continuous Deployment -> Build Settings section and add _site to the Publish Directory box like this:\n\n\n\n\n\nDid that, did another git commit and voila, up and running.\nNext step was to spend a sunny Sunday afternoon redoing my old rmd files to qmd, and navigating the differences in YAML and code chunk options.\n\n\n\ncode chunk options in rmd\n\n\n\n\n\ncode chunk options in qmd\n\n\nI also like the intelligent completion bits in Quarto \nand using ctrl + space to see all the parameters for the option you’re setting.\nquick aside…used veed.io for convert a screen-capture movie to animated gif…quick and easy\nGoing forward I’ll probably tweak the design now and then as I learn a bit more customization and functionality in Quarto and learn CSS and other styling tools for things like wrapping text around images and other tweaks and enhancements. But for now the site looks good and it’s time to get back to adding new data posts.\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/blog-migration-and-resurrection/index.html",
    "href": "posts/blog-migration-and-resurrection/index.html",
    "title": "…and we’re back. Migration and resurrection",
    "section": "",
    "text": "So much has happened since my last post here in February 2021. The blog was set up during the early stages of COVID lockdown when all of a sudden I had more time on my hands.\nSoon after that last post my wife and I made the decision to move to Europe in early 2022, so started preparations for that - arrange for a shipping container, learn French, apply for a CELTA program.\nThe world started to repoen a bit in late summer 2021, but then my wife broke her ankle. So all of a sudden I was doing lots of home health care & keeping the house running on top of work. The wasn’t much brain space for independent data projects & keeping the blog running.\nThen in late April 2022 we moved. The first stop was Lyon France\n\n\n\n\n\nwhere I earned the CELTA via the ELT Hub.\nI had expected teaching gigs to trickle in and I’d do independent data work while also spending time learning French and exploring Lyon and more of France. But the gigs came in abundance, and they were all new preps. So free time was at a minimum.\nThen in December 2022 we moved again. A job at CIEE in Denmark\n\n\n\n\n\ncame up so we accelerated the ultimate end goal of moving here - back to where I was born and still have lots of family and friends. Now that we’re settled in our first non-sublet in almost a year and I’ve gotten settled at work, there’s finally some time to get my head back into data work. I was partly inspired by serving as a mentor in the DDSA mentoring program. I also started using r at work to automate a few mundane tasks and to do a little analysis on our students.\nI’ll write about the migration process in a bit more detail in another post, but the tl/dr as to why is because in trying to do a simple update to my old main picture (me in a mask, early in lockdown) the build was failing in Netlify. So rather than try and diagnose the Hugo/Wowchemy issues (which turned out to be the Ubuntu deploy image…an easy reset), I saw that Posit’s new Quarto platform is very robust and a bit easier to publish with, so I decided to just rebuild the entire site.\nTo read about the migration details, go here. All of the old posts are up, time-stamped with the original posting dates. And hopefully soon, new posts as I work through a fairly long list of project ideas and try to get back to working on #tidytuesday datasets.\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/tidy-tuesday-maryland-bridges/index.html",
    "href": "posts/tidy-tuesday-maryland-bridges/index.html",
    "title": "Tidy Tuesday, November 27, 2018 - Maryland Bridges",
    "section": "",
    "text": "The Francis Scott Key Bridge in Baltimore\n\n\nThis dataset was posted to the #TidyTuesday repo back in November 2018. I worked on it a bit then, but didn’t properly finish it until March of 2020. With the blog finally set up figured I might as well post it as an entry.\nUpdate for migration from Hugo to Quarto…now that code-fold is native to code chunks, I’ll sometimes use if for long bits of code. Just click the down arrow to show code\n\n# readin in data, create df for plots\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\nFirst let’s read in the file from the raw data file on github\n\ntt_balt_gh <-\nread_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-27/baltimore_bridges.csv\",\n         progress = show_progress())\n\n\n\nOrganize and clean the data for analysis\n\nclean up some date and name issues\na decade-built field, factored for sorting\ntime since last inspection\n\n\n\nshow data cleaning code\n# keeping the comparison date March 21 when I originally did analysis\ntoday <- as.Date(c(\"2020-03-21\"))\n#Sys.Date()\ntoday_yr <- as.numeric(format(today, format=\"%Y\"))\n\ntt_mdbrdf <- as.data.frame(tt_balt_gh) %>%\n  mutate(age = today_yr - yr_built) %>%\n  #  mutate(vehicles_n = as.numeric(str_remove(vehicles, \" vehicles\")))\n  ## not needed, avg_daily_traffic has same info\n  mutate(inspection_yr = inspection_yr + 2000) %>%\n  mutate(county = ifelse(county == \"Baltimore city\", \"Baltimore City\", county)) %>%\n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  mutate(bridge_condition = factor(bridge_condition, levels = c(\"Good\", \"Fair\", \"Poor\"))) %>%\n  mutate(decade_built = case_when(yr_built <= 1899 ~ \"pre 1900\", \n                                  yr_built >= 1900 & yr_built <1910 ~ \"1900-09\",\n                                  yr_built >= 1910 & yr_built <1920 ~ \"1910-19\",\n                                  yr_built >= 1920 & yr_built <1930 ~ \"1920-29\",\n                                  yr_built >= 1930 & yr_built <1940 ~ \"1930-39\",\n                                  yr_built >= 1940 & yr_built <1950 ~ \"1940-49\",\n                                  yr_built >= 1950 & yr_built <1960 ~ \"1950-59\",\n                                  yr_built >= 1960 & yr_built <1970 ~ \"1960-69\",\n                                  yr_built >= 1970 & yr_built <1980 ~ \"1970-79\",\n                                  yr_built >= 1980 & yr_built <1990 ~ \"1980-89\",\n                                  yr_built >= 1990 & yr_built <2000 ~ \"1990-99\",\n                                  yr_built >= 2000 & yr_built <2010 ~ \"2000-09\",\n                                  TRUE ~ \"2010-19\")) %>%\n  mutate(decade_built = factor(decade_built, levels = \n                                 c(\"pre 1900\", \"1900-09\", \"1910-19\", \"1920-29\", \"1930-39\",\n                                   \"1940-49\", \"1950-59\", \"1960-69\", \"1970-79\", \n                                   \"1980-89\", \"1990-99\", \"2000-09\", \"2010-19\"))) %>%\n  mutate(inspect_mmyy = ISOdate(year = inspection_yr, month = inspection_mo, day = \"01\")) %>%\n  mutate(inspect_mmyy = as.Date(inspect_mmyy, \"%m/%d/%y\")) %>%\n  mutate(inspect_days = today - inspect_mmyy) %>%\n  mutate(inspect_daysn = as.numeric(inspect_days)) %>%\n  mutate(inspect_years = inspect_daysn/ 365.25) %>%\n  mutate(inspect_months = inspect_daysn / 30.417)\n\n\n\n\nThe first few charts look at bridges built by decade, the condition of all bridges by county, and how long since last inspection.\n\ntt_mdbrdf %>% \n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  count(decade_built) %>%\n  ggplot(aes(decade_built, n)) +\n  geom_bar(stat = \"identity\", fill = \"navy\") +\n  geom_text(aes(label = n), color = \"white\", vjust = 1.2) +\n  labs(title = \"Peak bridge-building years in Maryland were 1950s to 1990s\" ,\n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nBaltimore City has the lowest percentage of bridges in good condition, Anne Arundel the most. Baltimore City & Harford County seems to have the largest percentage of bridges in poor condition.\n\n\nshow stacked bar chart code\n## percent bridge condition by county\n# need to create df object to do subset label call in bar chart\nbrcondcty <- \ntt_mdbrdf %>%\n  count(county, bridge_condition) %>%\n  group_by(county) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() \n\nggplot(brcondcty, aes(x = county, y = pct, fill = bridge_condition)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(data = subset(brcondcty, bridge_condition != \"Poor\"), \n            aes(label = percent(pct)), position = \"stack\", \n            color= \"#585858\", vjust = 1, size = 3.5) +\n  scale_y_continuous(label = percent_format()) +\n  labs(title = \"Percent bridge condition by county\" , \n        x = \"\", y = \"\", fill = \"Bridge Condition\") +\n  scale_fill_brewer(type = \"seq\", palette = \"Blues\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nGiven the condition percentages in Baltimore County & City and Harford County, it’s no surprise that their bridges are older than in other counties.\n\n\nshow bar chart code\n## median age of bridges by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medage = median(age)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medage)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medage, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median bridge age by county\" , \n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt’s somewhat reassuring then that Baltimore City bridges at least have less time in months since last inspection than do the counties.\n\n\nshow bar chart code\n## median months since last inspection by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medinsp = median(inspect_months)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medinsp)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medinsp, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median months since last inspection, by county\",\n       subtitle = \"as of March 2020\",\n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt might be the outliers pulling the smoothing line straight, but there doesn’t seem to be too much of a relationship between age and time since last inspection.\n\n\nshow scatterplot code\n## age by months since last inspection\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\n\nAnd in fact, removing the outliers shows a slight relationship; the older bridges do seem to get inspected more frequently. In terms of a better visualization, looking at this again, I wonder if some jittering or another type of plot might have been more visually appealing, givne the clustering of most recent inspections.\n\n# same code as above but with outliers removed\ntt_mdbrdf %>%\n  filter(age <150, inspect_months <60) %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(title = \"Months since inspection, outliers removed\", \n       x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\nNot sure if scatter-plot with colors by county is best way to go for this idea. Maybe a tree map?\n\n\nshow scatterplot code\n# same but colors by county\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  labs(title = \"Months since last inspection (from current date)\", \n       x = \"Months since last inspection (from current date)\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.8, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nFunky distributions here…Anne Arundel & Baltimore City have the highest median daily riders, but Howard County’s upper quartile is way out there.\nshowing the code here to illustrate how I like to run the mean & interquartiles in the same code as rendering the plot.\n\n# median & interquartiles of daily riders of bridges by county -\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medtraf = median(avg_daily_traffic),\n            lq = quantile(avg_daily_traffic, 0.25),\n            uq = quantile(avg_daily_traffic, 0.75)) %>%\n  ungroup() %>%\n  ggplot(aes(county, medtraf)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"navy\") +\n  geom_point(size = 3, color = \"orange\", alpha = .8) +\n  geom_text(aes(label = comma(medtraf, digits = 0)), \n            size = 4, color = \"orange\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = comma(uq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = comma(lq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Median & interquartile ranges of average daily riders per bridge, by county\" , \n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nAs with the other scatterplot with colors for county, might need a different way to see relationship between bridge age and daily traffic by county.\n\n\nshow scatterplot code\n## age by avg daily riders by county\ntt_mdbrdf %>%\n  ggplot(aes(avg_daily_traffic, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  scale_x_continuous(labels = comma) +\n  labs(title = \"Average daily traffic per bridge, by county\" , \n        x = \"Average daily traffic\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.75, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nCover image for post (CC BY-SA 4.0) from Wikipedia\nThis post was last updated on 2023-05-14"
  },
  {
    "objectID": "blog-migration-and-resurrection/index.html",
    "href": "blog-migration-and-resurrection/index.html",
    "title": "…and we’re back. Migration and resurrection",
    "section": "",
    "text": "Lots of things happening since last post in February 2021. The blog was set up during the early stages of COVID lockdown when all of a sudden I had more time on my hands.\nBut since that last post my wife and I made the decision to move to Europe in early 2022 and started preparations for that. The world started to repoen a bit in late summer 2021, and then my wife broke her ankle. So all of a sudden I was doing lots of home health care & keeping the house running on top of work. The wasn’t much brain space for independent data projects & keeping the blog running.\nThen in April 2022 we moved. The first stop was Lyon France\n\n\n\n\n\nwhere I earned a CELTA via the ELT Hub.\nI had expected teaching gigs to trickle in and I’d do independent data work while also spending time learning French and exploring Lyon and more of France. But the gigs came in abundance, and they were all new preps. So free time was at a minimum.\nThen in December 2022 we moved again. A job at CIEE in Denmark\n\n\n\n\n\ncame up and we accelerated the ultimate end goal of moving here - back to where I was born and still have lots of family and friends. Now that we’re settled in our first non-sublet in almost a year and I’ve gotten settled at work, there’s finally some time to get my head back into data work. I was partly inspired by serving as a mentor in the DDSA mentoring program. I also started using r at work to automate a few mundane tasks and to do a little analysis on our students.\nI’ll write about the migration process in a bit more detail in another post, but the tl/dr as to why is because in trying to do a simple update to my old main picture (me in a mask, early in lockdown) the build was failing in Netlify. So rather than try and diagnose the Hugo/Wowchemy issues (which turned out to be the Ubuntu deploy image…an easy reset), I saw that Posit’s new Quarto platform is very robust and a bit easier to publish with, so I decided to just rebuild the entire site.\nTo read about the migration details, go here. All of the old posts are up, time-stamped with the original posting dates. And hopefully soon, new posts as I work through a fairly long list of project ideas."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "© Copyright Gregers Kjerulf Dubrow\nThis is my personal website. All views are mine, and nothing I post here should be taken as an endorsement by my employer or any organizations of which I am a member.\nContent on this site is provided under a Creative Commons (CC-BY) 4.0 license. You may reuse this content as long as you indicate cite me as the author and provide a link back to the original material. Source code of the site is provided under the MIT license and may be reused without restriction."
  },
  {
    "objectID": "posts/tidy-tuesday-feb-02-2021-hbcu/index.html",
    "href": "posts/tidy-tuesday-feb-02-2021-hbcu/index.html",
    "title": "Tidy Tuesday, February 2, 2021 - HBCU Enrollment",
    "section": "",
    "text": "When the the Tidy Tuesday HBCU dataset was posted I got excited because here was something right in my scope of knowledge; enrollment in higher education. It was a great first Tidy Tuesday set for Black History Month. To keep this from being a journal-length piece, I won’t put on my history of higher ed teacher hat & get into how HBCUs came to be. Here’s a good overview.\nOne of my strengths as a data analyst is a brain that’s constantly asking questions of a dataset:\n* what’s in there?\n* what relationships exist between variables?\n* what I can add to the dataset to glean more insight?\nFor Tidy Tuesday though that slows things down – it’s mostly meant to be a quick turnaround thing where you share results to Twitter. So my deep-dive habits mean I’m usually a bit behind getting Tidy Tuesday analysis done the week the data are posted. My goal for this analysis is to show:\n* changes over time in the racial and gender make-up of students at HBCUs\n* changes over time in overall enrollment at HBCUs by sector (public 4yr, private 4y, etc), relative to non-HBCUs\n* changes over time in tuition & fees by sector, HBCUs vs non-HBCUs\nThe Tidy Tuesday data has some of what I need - it comes from Data.World via IPEDS (essentially the US Department of Education’s higher ed data library) . I’m supplementing it with data from the Delta Cost Project, which aggrgated years worth of data from IPEDS.\nSo let’s dive into the data.\nFirst we load the packages we’ll be using…\n\n# load packages\nlibrary(tidytuesdayR) # load the tidy tuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot\nlibrary(ggrepel) # helper functions for ggplot\n\n…then lets make a couple of things we’ll use a lot going forward.\n\n# create notin operator to help with cleaning & analysis\n`%notin%` &lt;- negate(`%in%`)\n\n# function for year-over-year percent changes\npctchange &lt;- function(x) {\n  (x - lag(x)) / lag(x)\n}\n\nNow we’ll load the sets from TidyTuesday and then the Delta Cost Project files.\nI’ll load the Tidy Tuesday data objects via the tidytuesdayR package and clean up the data. This week’s data came in a few separate files. For my analysis I need a dataframe of HBCU enrollments with Black and non-Black students. So I need to combine the two HBCU enrollment files (one each for Black students and all students) into one and subtract Black from All to get non-Black. I’ll show code for loading the Black student set in, cleaning it up a bit, and skip to the subtraction & joins. For the full code experience, head to my Tidy Tuesday repo\n\n\nshow tt_hbcu_black data load & cleaning code\ntt_hbcu_load &lt;- tt_load(\"2021-02-02\")\n\ntt_hbcu_black &lt;- as_tibble(tt_hbcu_load$hbcu_black) %&gt;%\n  clean_names() %&gt;%\n  mutate(year = as.character(year)) %&gt;%\n  mutate(ethnicity = \"Black\") %&gt;%\n  select(year, ethnicity, enroll_n = total_enrollment, women = females, men = males, \n         four_year_all = x4_year, two_year_all = x2_year,\n         total_public, four_year_pub = x4_year_public, two_year_pub = x2_year_public, \n         total_private, four_year_pri = x4_year_private, two_year_pri = x2_year_private) \n\n\n\n\nshow tt_hbcu_all data load & cleaning code\ntt_hbcu_all &lt;- as_tibble(tt_hbcu_load$hbcu_all) %&gt;%\n  clean_names() %&gt;%\n  mutate(ethnicity = \"All\") %&gt;%\n  mutate(year = as.character(year)) %&gt;%\n  select(year, ethnicity, enroll_n = total_enrollment, women = females, men = males, \n         four_year_all = x4_year, two_year_all = x2_year,\n         total_public, four_year_pub = x4_year_public, two_year_pub = x2_year_public, \n         total_private, four_year_pri = x4_year_private, two_year_pri = x2_year_private)  \n\n\nLet’s join tt_hbcu_black with tt_hbcu_all and create the non-Black ethnicity group. There is probably a better way to do the subtractions programmatically with a function but my r skills aren’t there yet. The final dataframes I’ll use for plots are tt_hbcu_enr_eth_sex & tt_hbcu_enr_eth_sect. Because of the way the source data was arrayed I could do ethnicty by gender and ethnicity by sector but not all three vectors. So, two sets…\n\n\nshow dataframe join code\ntt_hbcu_notblack = as.data.frame(tt_hbcu_all) %&gt;%\n  select(-ethnicity) %&gt;%\n    # renaming multiple columns in one line of code!\n  rename_with(~ paste(.x, \"all\", sep = \"_\"), .cols = (2:12)) %&gt;%\n  bind_cols(tt_hbcu_black) %&gt;%\n  select(-year...13, -ethnicity) %&gt;%\n  rename(year = year...1) %&gt;%\n  # the subtractions by column to get non-black. again, might be a better way to do it w/ a function.\n  mutate(enroll_n_nb = enroll_n_all - enroll_n) %&gt;%\n  mutate(women_nb = women_all - women) %&gt;%\n  mutate(men_nb = men_all - men) %&gt;%\n  mutate(four_year_all_nb = four_year_all_all - four_year_all) %&gt;%\n  mutate(two_year_all_nb = two_year_all_all - two_year_all) %&gt;%\n  mutate(total_public_nb = total_public_all - total_public) %&gt;%\n  mutate(four_year_pub_nb = four_year_pub_all - four_year_pub) %&gt;%\n  mutate(two_year_pub_nb = two_year_pub_all - two_year_pub) %&gt;%\n  mutate(total_private_nb = total_private_all - total_private) %&gt;%\n  mutate(four_year_pri_nb = four_year_pri_all - four_year_pri) %&gt;%\n  mutate(two_year_pri_nb = two_year_pri_all - two_year_pri) %&gt;%\n  mutate(ethnicity = \"Not Black\") %&gt;%\n  select(year, ethnicity, enroll_n_nb:two_year_pri_nb) %&gt;%\n  rename_with(~ str_remove(.x, \"_nb\"), .cols = (3:13))\n\n# create final dataframes, turn wide to long\n# note sex pct is by eth group\ntt_hbcu_enr_eth_sex = as.data.frame(rbind(tt_hbcu_all, tt_hbcu_black)) %&gt;%\n  rbind(tt_hbcu_notblack) %&gt;%\n  select(-four_year_all:-two_year_pri) %&gt;%\n  arrange(year, ethnicity) %&gt;% \n  group_by(year) %&gt;%\n   mutate(enroll_eth_pct = enroll_n / first(enroll_n)) %&gt;%\n  ungroup() %&gt;%\n  pivot_longer(cols = women:men,\n               names_to = \"sex\",\n               values_to = \"sex_n\") %&gt;%\n  arrange(year, ethnicity) %&gt;%\n  group_by(year, ethnicity) %&gt;%\n  mutate(enroll_sex_pct = sex_n / sum(sex_n)) %&gt;%\n  ungroup() %&gt;%\n  select(year, ethnicity, enroll_n, enroll_eth_pct, sex, sex_n, enroll_sex_pct) %&gt;%\n  arrange(year, ethnicity, sex)\n\n# note pct_sect_eth is by eth group by year, pct_eth_sect is pct eth w/in sector\ntt_hbcu_enr_eth_sect = rbind(tt_hbcu_all, tt_hbcu_black) %&gt;%\n  rbind(tt_hbcu_notblack) %&gt;%\n  select(-women, -men) %&gt;%\n  arrange(year, ethnicity) %&gt;%\n  pivot_longer(cols = four_year_all:two_year_pri,\n               names_to = \"sector\", \n               values_to = \"sector_n\") %&gt;%\n  arrange(year, ethnicity) %&gt;%\n  mutate(pct_sect_eth = sector_n / enroll_n) %&gt;%\n  arrange(year, sector) %&gt;%\n  group_by(year, sector) %&gt;%\n  mutate(pct_eth_sect = sector_n / (sum(sector_n) /2)) %&gt;%\n  ungroup()\n\n\nOk, the Tidy Tuesday data is sorted, so let’s use the haven package load in the Delta Cost Project (DCP) data they provided as SAS files.\nThey’ve split their data into two files, one from 1987 to 1999 and one from 2000 to 2015. There’s a ton of data in the set, but all I want for this analysis is enrollments and tuition - I’ll pull those fields from each set, then rbind together into a dataframe called tuitenr_8715_agg for year-over-year by-sector analysis. Unfortunately while DCP has enrollment by ethnicity, they only do it for total enrollment - grad and undergrad combined - so I can’t do some of the HBCU vs non-HBCU comparisons. There are other sources which I’ll describe at the end of the post.\nOk, loading the 2000 to 2015 data…\n\n\nshow DCP 2000 - 2015 load code\ndelta0015all &lt;- (haven::read_sas(\"~/Data/ipeds/delta_public_release_00_15.sas7bdat\", NULL))\ndelta0015_tuitenr &lt;- delta0015all %&gt;%\n  filter(between(sector_revised, 1, 6)) %&gt;%\n  mutate(sector_desc = case_when(sector_revised == 1 ~  \"Public 4yr\",\n                                 sector_revised == 2    ~ \"Private nonprofit 4yr\",\n                                 sector_revised == 3    ~ \"Private for-profit 4yr\",\n                                 sector_revised == 4    ~ \"Public 2yr\",\n                                 sector_revised == 5    ~ \"Private nonprofit 2yr\",\n                                 sector_revised == 6    ~ \"Private for-profit 2yr\")) %&gt;%\n # mutate(total_undergraduates = ifelse(is.na(total_undergraduates), 0, total_undergraduates)) %&gt;%\n  select(unitid, instname, sector_revised, sector_desc, hbcu,\n         year = academicyear, tuition_fee_ug_in = tuitionfee02_tf, \n         tuition_fee_ug_oos = tuitionfee03_tf, \n         total_undergraduates)\n\n\nNow let’s load the 1987 to 1999 data…\n\n\nshow DCP 1987 - 1999 load code\ndelta8799all &lt;- (haven::read_sas(\"~/Data/ipeds/delta_public_release_87_99.sas7bdat\", NULL))\n\ndelta8799_tuitenr &lt;- as_tibble(delta8799all) %&gt;%\n  filter(between(sector_revised, 1, 6)) %&gt;%\n  mutate(sector_desc = case_when(sector_revised == 1 ~  \"Public 4yr\",\n                                 sector_revised == 2    ~ \"Private nonprofit 4yr\",\n                                 sector_revised == 3    ~ \"Private for-profit 4yr\",\n                                 sector_revised == 4    ~ \"Public 2yr\",\n                                 sector_revised == 5    ~ \"Private nonprofit 2yr\",\n                                 sector_revised == 6    ~ \"Private for-profit 2yr\")) %&gt;%\n # mutate(total_undergraduates = ifelse(is.na(total_undergraduates), 0, total_undergraduates)) %&gt;%\n  select(unitid, instname, sector_revised, sector_desc, hbcu,\n         year = academicyear, tuition_fee_ug_in = tuitionfee02_tf, \n         tuition_fee_ug_oos = tuitionfee03_tf, \n         total_undergraduates)\n\n\n…and let’s join and munge the DCP files…\n\n\nshow DCP join & clean code\ntuitenr_8715 &lt;- rbind(delta0015_tuitenr, delta8799_tuitenr)\n\ntuitenr_8715_agg &lt;-\n  tuitenr_8715 %&gt;%\n  # filter(!sector_desc == \"Private for-profit 2yr\" &\n  #          !sector_desc == \"Private for-profit 4yr\") %&gt;%\n#  filter(!is.na(tuition_fee_ug_in)) %&gt;%\n  group_by(year, sector_desc, hbcu) %&gt;%\n  summarise(enr_tot = sum(total_undergraduates, na.rm = TRUE),\n            mean_tuit_in = mean(tuition_fee_ug_in, na.rm = TRUE),\n            mean_tuit_oos = mean(tuition_fee_ug_oos, na.rm = TRUE)\n            ) %&gt;%\n  mutate(level = ifelse(str_detect(sector_desc, \"2yr\"), \"2yr\", \"4yr\")) %&gt;%\n#  summarise(mean_tuit_in = mean(tuition_fee_ug_in, na.rm = T)) %&gt;%\n  ungroup() %&gt;%\n  mutate(hbcu_f = ifelse(hbcu == 1, \"HBCU\", \"Not HBCU\")) %&gt;%\n  mutate(hbcu_f = factor(hbcu_f, levels = c(\"HBCU\", \"Not HBCU\"))) \n\n\nBecause I’ll be doing some year-over-year percent change analysis, I’ll create a separate dataframe from tuitenr_8715_agg (the DCP data) so I can compare HBCU to non-HBCU changes by year.\nRemember the pctchange function I created when I loaded the packages? Here it’s put to good use. I’ll only show code for one of the frames - I created frames for each sector, HBCU and non-HBCU. The final data set will rbind 10 frames into tuit_8715_pctchgh. BTW, this is another example of when I wish I were a bit better at purrr/mapping and functions so I could have done it with less code.\nSince the data provided starts in 1987, I’ll filter that year out of the dataframes.\n\ntuit_8715_pctchgh1 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 1, sector_desc == \"Public 4yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\n\n\nshow the remaining sector files code\ntuit_8715_pctchgh2 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 1, sector_desc == \"Private nonprofit 4yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh3 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 1, sector_desc == \"Public 2yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh4 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 1, sector_desc == \"Private nonprofit 2yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh5 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Public 4yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh6 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Private nonprofit 4yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh7 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Public 2yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh8 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Private nonprofit 2yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh9 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Private for-profit 4yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh10 &lt;- tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2, sector_desc == \"Private for-profit 2yr\") %&gt;%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %&gt;% \n  arrange(year) %&gt;% \n  mutate(across(4:6, pctchange)) %&gt;% \n  select(year, hbcu, sector_desc, everything()) %&gt;%\n  ungroup() %&gt;%\n  filter(year &gt; 1987)\n\ntuit_8715_pctchgh &lt;- rbind(tuit_8715_pctchgh1, tuit_8715_pctchgh2) %&gt;%\n  rbind(tuit_8715_pctchgh3) %&gt;%\n  rbind(tuit_8715_pctchgh4) %&gt;%\n  rbind(tuit_8715_pctchgh5) %&gt;%\n  rbind(tuit_8715_pctchgh6) %&gt;%\n  rbind(tuit_8715_pctchgh7) %&gt;%\n  rbind(tuit_8715_pctchgh8) %&gt;%\n  rbind(tuit_8715_pctchgh9) %&gt;%\n  rbind(tuit_8715_pctchgh10) \n\n\nOk, the data is in place, let’s see what it tells us! I’ll fold the code for the more basic charts, click on the arrow to see it. You’ll note that this analysis is mostly line graphs. That’a because I’m mostly doing trend analysis, and I like line graphs for time series.\nFirst up we’ll use the Tidy Tuesday data to look at HBCU enrollment by ethnicity (Black and non-Black) from 1990 to 2015. What do we see? Overall, periods of increase and decrease in total enrollment. But interestingly we see that non-Black enrollment kept increasing, even as Black enrollment started to drop. Of course Black students make up a vast majority of HBCU enrollment, so their trends drive overall numbers. It is worth noting though that the changes started in the 2010s, coinciding with changes in how NCES counted ethncity. It’s beyond the scope of this quick (ha) analysis, but worth digging deeper to see the effect of changes on the ethnic mix of HBCUs.\n\n\nshow the enrollment by ethnicity line charts code\n## tt hbcu data, black men v women over time\ntt_hbcu_enr_eth_sex %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  ggplot(aes(year, enroll_n, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format(), breaks = scales::pretty_breaks()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  facet_grid(ethnicity ~ ., scales = \"free_y\") +\n  labs(title = \"Black Enrollment at HBCUs Rising & Falling Since 1990\",\n       subtitle = \"Non-Black enrollment slowly & streadily increasing\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Total UG Enrollment\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity line charts code\n\ntt_hbcu_enr_eth_sex %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  filter(ethnicity == \"Black\") %&gt;%\n  distinct(year, ethnicity, .keep_all = T) %&gt;%\n  ggplot(aes(year, enroll_eth_pct, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1.25) +\n  scale_y_continuous(limits = c(.5, 1), \n                     labels = scales::percent_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Slight Decrease Over Time in Percentage of Black Students Enrolled at HBCUs\",\n       subtitle = \"Might be due to changes in how ethnicity is coded by US Dept of Ed\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Pct Black students\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), \n        plot.title = element_text(size = 11),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nNext let’s look at enrollment by ethnicity & sex. HBCU enrollment mirrors that of higher ed enrollment in general - since the early 1980s women outnumber men. There are differences by sector (4-year, 2-year, public & private) and field of study but the oveall trend has been consistent. We do see that for non-Black students at HBCUs the percentage of women is a bit less than that of Black students.\n\n\nshow the enrollment by ethnicity & sex line charts code\ntt_hbcu_enr_eth_sex %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  ggplot(aes(year, sex_n, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  # scale_y_continuous(limits = c(0, 250000),\n  #                    breaks = c(0, 50000, 100000, 150000, 200000, 250000),\n  #                    labels = scales::comma_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Black Women Enroll in Higher Numbers Than Men at HBCUs\",\n       subtitle = \"Same Pattern as Overall Undergrad Enrollments in the US\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Total UG Enrollment\") +\n  facet_grid(ethnicity ~ sex, scales = \"free_y\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity & sex line charts code\n\n# sex - women as pct\ntt_hbcu_enr_eth_sex %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  filter(sex == \"women\") %&gt;%\n  ggplot(aes(year, enroll_sex_pct, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(limits = c(.5, .8), \n                     labels = scales::percent_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Women a Greater Percentage of those Enrolled at HBCUs\",\n       subtitle = \"Slightly higher among Black students than not Black\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Pct Women enrolled\") +\n  facet_grid(ethnicity ~ ., scales = \"free_y\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nThe next thing I want to look at is enrollment by sector for HBCUs. In this case we think of sectors along two axes - public and private, and 2-year (community/junior colleges) vs 4-year (baccalaureate granting). What these first two sector charts show us is that most HBCU enrollment is in publics and in 4-year schools. Given the land-grant history of most HBCUs, that makes sense.\nI am leaving in the code for the first chart to highlight how one can a) have two different colored lines within the same chart in a faceted visualization (the scale_color_manual call) and b) use the amazing ggtext package to get two different colors of text in the subtitle line, so it does the work of a legend. The &lt;span style = &lt;/span&gt; and the element_markdown call in the theme specification is how you make it happen.\nThe third chart takes a slightly different view of the data, looking at percent of enrollment by sector within each of the ethnic categories we have in the data, Black and not-Black. What the first two charts showed is confirmed here - that Black students in HBCUs are mostly in public 4-years, and that trend has held steady since at least 1990.\n\ntt_hbcu_enr_eth_sect %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %&gt;%\n  filter(sector %in% c(\"total_private\", \"total_public\")) %&gt;%\n  ggplot(aes(year, pct_sect_eth, group = sector, color = sector)) +\n  geom_line(size = 1) +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"Public HBCUs Enroll Greater Pct of Students \",\n       subtitle = \"Greater Pct Black students in Private HBCU than non-Black students; \n       &lt;span style = 'color:#56B4E9;'&gt;Blue = Public&lt;/span&gt; \n       &lt;span style = 'color:#E69F00;'&gt;Orange = Private&lt;/span&gt;\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by Publc & Private HBCUs\") +\n  facet_grid(ethnicity ~ .) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_markdown(size = 8, face = \"italic\"),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\n\nshow the enrollment by ethnicity & sector charts code\ntt_hbcu_enr_eth_sect %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %&gt;%\n  filter(sector %in% c(\"four_year_all\", \"two_year_all\")) %&gt;%\n  ggplot(aes(year, pct_sect_eth, group = sector, color = sector)) +\n  geom_line(size = 1) +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"4-year HBCUs Enroll ~ 90% of all Students\",\n       subtitle = \"Black students more likely to be in 4-year HBCU than non-Black students; \n       &lt;span style = 'color:#56B4E9;'&gt;Blue = 4-year&lt;/span&gt; \n       &lt;span style = 'color:#E69F00;'&gt;Orange = 2-year&lt;/span&gt;\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by 4yr & 2yr HBCUs\") +\n  facet_grid(ethnicity ~ .) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_markdown(size = 8, face = \"italic\"),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity & sector charts code\n\ntt_hbcu_enr_eth_sect %&gt;%\n  filter(year &gt; 1989) %&gt;%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %&gt;%\n  filter(sector %in% c(\"four_year_pub\", \"four_year_pri\", \"two_year_pri\", \"two_year_pub\")) %&gt;%\n  mutate(sector = str_replace(sector, \"four_year_pub\", \"4 Year Public\")) %&gt;%\n  mutate(sector = str_replace(sector, \"two_year_pub\", \"2 Year Public\")) %&gt;%\n  mutate(sector = str_replace(sector, \"four_year_pri\", \"4 Year Private\")) %&gt;%\n  mutate(sector = str_replace(sector, \"two_year_pri\", \"2 Year Private\")) %&gt;%\n#  ggplot(aes(year, enroll_sect_pct, group = sector, color = sector)) +\n  ggplot(aes(year, pct_sect_eth, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"Black Students most likely to be in 4-year HBCUs\",\n       subtitle = \"Non-black students moving to 2-year HBCUs from 4-yr; \n       &lt;span style = 'color:#E69F00;'&gt;Percents sum to 100% across ethnic groups&lt;/span&gt;\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by Sector\") +\n  facet_grid(ethnicity ~ sector) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        axis.text.x = element_text(size = 6),\n        plot.subtitle = element_markdown(face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nSo we’ve done what we can with the Tidy Tuesday data, time to compare HBCUs to non-HBCUs using the Delta Cost Project data. We’ll stay with sector analysis and look at enrollments by sector. For the chart output I wanted, I used Thomas Lin Pederson’s patchwork package. I’ll fold the code that creates the component charts and just show the patchwork call.\n\n\nshow the enrollment by sector charts code\nenr19902015hbcu &lt;-\ntuitenr_8715_agg %&gt;%\n  filter(hbcu == 1) %&gt;% \n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(title = \"HBCU Enrollment Across Sector 1990-2015\",\n       #caption = \"Source: Delta Cost Project\",\n    x = \"\", y = \"Total UG enrollment\") +\n  facet_wrap(~ sector_desc, scales = \"free\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n                axis.text.y = element_text(size = 7), \n        #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\nenr19902015nothbcu &lt;-\n  tuitenr_8715_agg %&gt;%\n  filter(hbcu == 2) %&gt;%\n#  filter(level == \"2yr\") %&gt;% \n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#56B4E9\", group = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(title = \"non-HBCU Enrollment Across Sector 1990-2015\",\n       caption = \"Source: Delta Cost Project\",\n       x = \"\", y = \"Total UG enrollment\") +\n  facet_wrap(~ sector_desc, scales = \"free\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#56B4E9\"),\n                axis.text.y = element_text(size = 7), \n        #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\nenr19902015all &lt;- enr19902015hbcu / enr19902015nothbcu\nenr19902015all\n\n\n\n\nFor a change of pace let’s see a slope graph of percent change in tuition by sector from 1990 to 2015. I found a helpful code-thru of a similar-ish slope graph here and used that as my guide. I used patchwork for this too, and because in this instance I used patchwork for title and other adornments, I’ll show code for all steps.\nTwo observations stand out - first, HBCUs are less expensive by sector than non-HBCUs. But for both HBCUs and non-HBCUs, the price gap between private 4-year schools and the other sectors widened during this time period.\nBut these are sticker prices and don’t include institutional aid, or “tuition discount”. If a college offers you a $3000 institutional grant (not a federal or state grant or loan), the money is essentially foregone revenue for the school, or a discount to the stated tuition price.\n\n\nshow the slope graph code\ntuitsect_hbcu &lt;-\n    tuitenr_8715_agg %&gt;%\n    filter(hbcu == 1 & (year == 1990 | year == 2015)) %&gt;%\n    #  filter(level == \"2yr\") %&gt;% \n    ggplot(aes(year, mean_tuit_in, group = sector_desc)) +\n    geom_line(aes(color = sector_desc)) +\n    geom_point(aes(color = sector_desc)) +\n    scale_color_manual(values = c(\"Public 4yr\" = \"#999999\", \n                                                                \"Public 2yr\" = \"#E69F00\", \n                                                                \"Private nonprofit 2yr\" = \"#56B4E9\", \n                                                                \"Private nonprofit 4yr\" = \"#009E73\")) +\n    geom_text_repel(data = tuitenr_8715_agg %&gt;%\n                                        filter(hbcu == 1 & year == 2015),\n                                    aes(label = sector_desc), hjust = \"left\", nudge_x = .5,\n                                    direction = \"y\", size = 4) +\n    annotate(\"text\", x = 1991, y = 12000, label = \"HBCUs\", size = 5, fontface = \"italic\") +\n    scale_x_continuous(breaks = c(1990, 2015)) +\n    scale_y_continuous(labels = scales::dollar_format(),\n                                         limits = c(0, 15000),\n                                         breaks = c(0, 2500, 5000, 7500, 10000, 12500, 15000)) +\n    labs(x = \"\", y = \"Average Tuition & Fees\") +\n    theme_light() +\n    theme(panel.grid.major = element_blank(), panel.grid.major.x = element_blank(), \n                panel.grid.minor = element_blank(), panel.grid.minor.y = element_blank(),\n                panel.background = element_blank(), \n                strip.background = element_rect(fill = \"#56B4E9\"),\n                panel.border = element_blank(), legend.position = \"none\",  \n                axis.text.y = element_text(size = 8), \n                axis.ticks = element_blank(), axis.text.x = element_blank(), \n                axis.title.y = element_text(hjust = -.07),\n                #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n                plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\ntuitsect_nonhbcu &lt;-\n    tuitenr_8715_agg %&gt;%\n    filter(hbcu == 2 & (year == 1990 | year == 2015)) %&gt;%\n    #  filter(level == \"2yr\") %&gt;% \n    ggplot(aes(year, mean_tuit_in, group = sector_desc)) +\n    geom_line(aes(color = sector_desc)) +\n    geom_point(aes(color = sector_desc)) +\n    scale_color_manual(values = c(\"Public 4yr\" = \"#999999\", \n                                                                \"Public 2yr\" = \"#E69F00\", \n                                                                \"Private nonprofit 2yr\" = \"#56B4E9\", \n                                                                \"Private nonprofit 4yr\" = \"#009E73\",\n                                                                \"Private for-profit 2yr\" = \"#0072B2\", \n                                                                \"Private for-profit 4yr\" = \"#CC79A7\")) +\n    geom_text_repel(data = tuitenr_8715_agg %&gt;%\n                                        filter(hbcu == 2 & year == 2015),\n                                    aes(label = sector_desc), hjust = \"left\", nudge_x = .5, \n                                    direction = \"y\", size = 3) +\n    annotate(\"text\", x = 1992, y = 20000, label = \"not HBCUs\", size = 5, fontface = \"italic\") +\n    scale_x_continuous(breaks = c(1990, 2015)) +\n    scale_y_continuous(labels = scales::dollar_format(),\n                                         limits = c(0, 25200),\n                                         breaks = c(0, 5000, 10000, 15000, 20000, 25000)) +\n    labs(x = \"\", y = \"\") +\n    theme_light() +\n    theme(panel.grid.major = element_blank(), panel.grid.major.x = element_blank(), \n                panel.grid.minor = element_blank(), panel.grid.minor.y = element_blank(),\n                panel.background = element_blank(), \n                strip.background = element_rect(fill = \"#56B4E9\"),\n                panel.border     = element_blank(),  axis.text.y = element_text(size = 8), \n                legend.position = \"none\", axis.ticks       = element_blank(),\n                #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n                plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\ntuitsec &lt;- tuitsect_hbcu / tuitsect_nonhbcu  + plot_annotation(\n    title = 'Tuition & Fees at HBCUs Lower by Sector than non-HBCUs',\n    subtitle = 'Private non-profit 4-yr tuition increased more than other sectors',\n    caption = \"Source: Delta Cost Project\", \n    theme = theme(plot.subtitle = element_text(face = \"italic\", size = 9)))\ntuitsec\n\n\n\n\nFinally, let’s check out some year-over-year percent changes in enrollment & tuition, using the line graphs I love so much. Code is basic ggplot, so not worth showing here. If there’s anything of interest it was in adding a geom_hline call to add the light gray reference line at 0%.\nAs we’d seen in eariler charts, enrollment at HBCUs ping-ponged up and down. There was a bit less volatility in non-HBCUS, and in general enrollments didn’t decrease from 2000 onward.\nAt both HBCUs and non-HBCUs, public & private 4-years raised tuition & fees every year. The publics had a couple of instances of sharper increases, likely in response to recession-hit state budgets. The 2-year schools, especially 2-year HBCUs, had wider up & down swings in tuition.\n\n\nshow the faceted line graph code\ntuit_8715_pctchgh %&gt;%\n  mutate(hbcu_f = case_when(hbcu == 1 ~ \"HBCU\", TRUE ~ \"Not HBCU\")) %&gt;%\n  #  filter(hbcu == 1 & year &gt;= 1990) %&gt;% \n  filter(year &gt;= 1990) %&gt;%\n  filter(sector_desc %in% c(\"Public 4yr\", \"Private nonprofit 4yr\", \"Public 2yr\")) %&gt;%\n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  geom_hline(yintercept=0, color = \"gray\") +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(title = \"Year-over-year Percent Changes to UG Enrollment\",\n       subtitle = \"HBCU sectors had wider positive & negative swings\", \n       caption = \"Source: Delta Cost Project\", \n       x = \"\", y = \"Pct Chnage in-state tuition & fees\") +\n  facet_grid(hbcu_f ~ sector_desc) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the faceted line graph code\n\ntuit_8715_pctchgh %&gt;%\n  mutate(hbcu_f = case_when(hbcu == 1 ~ \"HBCU\", TRUE ~ \"Not HBCU\")) %&gt;%\n  #  filter(hbcu == 1 & year &gt;= 1990) %&gt;% \n  filter(year &gt;= 1990) %&gt;%\n  filter(sector_desc %in% c(\"Public 4yr\", \"Private nonprofit 4yr\", \"Public 2yr\")) %&gt;%\n  ggplot(aes(year, mean_tuit_in)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  geom_hline(yintercept=0, color = \"gray\") +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(title = \"Year-over-year Percent Changes to Tuition & Fees\",\n       subtitle = \"More volatility in 2-year sector, steady increases in every sector\", \n       caption = \"Source: Delta Cost Project\", \n       x = \"\", y = \"Pct Chnage in-state tuition & fees\") +\n  facet_grid(hbcu_f ~ sector_desc) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nThis was designed to be broad-scope, more-or-less exploratory analysis. Just looking for general trends, what might be worth a deeper dive. Like with the percent changes in tuition or enrollment…what was happening in the years with the wider + or - swings? The 2001 recession? The 2009/10 economic collapse? Which specific schools had more success with enrollment? Which HBCUs are the most & least expensive?\nAnd what other questions one could answer with the data I used here? Well for one, the tuition discount rates by sectors and HBCU/not HBCU would be an interesting thing to look at. Did HBCUs need to more aggressively discount tution to meet enrollment targets? The Delta Cost Project set includes a tuition discount variable. There’s a regression analysis in there somewhere, as well as other predictive analysis.\nI’d also want to look at which ethnic groups made up the growth in non-Black enrollments starting in 2010. Or was it that the changes to how ethnicity was recorded meant that students who used to be classified as Black become Hispanic or multi-ethnic. I know from my time in undergrad admissions at UC Berkeley that this accounted for a decline in the federal count of African-American students. However, when we compared the numbers with the UC definitions, we didn’t see a decline.\nTo get those numbers I could download IPEDS data directly from NCES or use r tools like the Urban Insitute’s educationdata package which is an API wrapper to that scrapes NCES and other websites for data.\nThere is a ton of higher education data out there, and it’s never been easier to get at scale than now. Trust me, as someone who has done individual downloads, ordered CD-ROMs, even used the late-great WebCASPAR, if you have an education policy or outcomes related question, there is publicly available data for analysis.\nCover image for post from GWU CCAS Graduate Blog\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/tidy-tuesday-nov-11-2020-hiking-trails-in-wash-state/index.html",
    "href": "posts/tidy-tuesday-nov-11-2020-hiking-trails-in-wash-state/index.html",
    "title": "Tidy Tuesday, November 24, 2020 - Hiking Trails in WA State",
    "section": "",
    "text": "Trail-head lake in the Cascade Mountains.\n\n\n\nDiving deep into the Tidy Tuesday pool…\nEvery week I’m so impressed by the amount of beautiful, creative and informative analyses and data viz efforts that spring from #TidyTuesday, the weekly data project that evolved out of the R4DS Learning Community. And every week I say to myself, “I should really give it a go, grab the data and post some results”. Then life intervenes, work gets crazy, and before I know it…\nWell, around Thanksgiving, with some slack time and some inspiration, I finally got around to it. A resulting benefit was it got me motivated to finally get around to getting this blog set up. My github repo has a folder for any tidytuesday projects I get around to.\nHere’s the code and resulting charts & tables.\n\n# load packages\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\n\nFirst let’s read in the file using the tidytuesdayR package. We’ll also look at the raw data\n\ntt_watrail <- tt_load(\"2020-11-24\")\n#> \n#>  Downloading file 1 of 1: `hike_data.rds`\n\nglimpse(tt_watrail$hike_data)\n#> Rows: 1,958\n#> Columns: 8\n#> $ name        <chr> \"Lake Hills Greenbelt\", \"Snow Lake\", \"Skookum Flats\", \"Ten…\n#> $ location    <chr> \"Puget Sound and Islands -- Seattle-Tacoma Area\", \"Snoqual…\n#> $ length      <chr> \"2.3 miles, roundtrip\", \"7.2 miles, roundtrip\", \"7.8 miles…\n#> $ gain        <chr> \"50\", \"1800\", \"300\", \"1585\", \"500\", \"500\", \"425\", \"450\", \"…\n#> $ highpoint   <chr> \"330.0\", \"4400.0\", \"2550.0\", \"2370.0\", \"1000.0\", \"2200.0\",…\n#> $ rating      <chr> \"3.67\", \"4.16\", \"3.68\", \"3.92\", \"4.14\", \"3.14\", \"5.00\", \"2…\n#> $ features    <list> <\"Dogs allowed on leash\", \"Wildlife\", \"Good for kids\", \"L…\n#> $ description <chr> \"Hike through a pastoral area first settled and farmed in …\n\n\n\nThere are a few things we want to do with the data for the working dataframe:\n\ncreate columns for miles, direction, type from length\ncreate specific location columns frolm location\nchange rating, gain and highpoint to numeric\ncreate a rating group\nchange features to character vector, also unnest; makes the resulting df long. we’ll use distinct when we only need 1 obs per trail\n\n\ntt_watraildf <- tt_watrail$hike_data %>%\n  mutate(length_miles = parse_number(length)) %>%\n  mutate(across(gain:rating, as.numeric)) %>%\n  mutate(rating_grp = case_when(rating == 0 ~ \"0\",\n                                rating >0 & rating < 2 ~ \"1\",\n                                rating >=2 & rating < 3 ~ \"2\",\n                                rating >=3 & rating < 4 ~ \"3\",\n                                rating >=4 & rating < 5 ~ \"4\",\n                                rating == 5 ~ \"5\")) %>%\n  mutate(trail_type = case_when(grepl(\"roundtrip\", length) ~ \"Round trip\",\n                          grepl(\"one-way\", length) ~ \"One Way\",\n                          grepl(\"of trails\", length) ~ \"Trails\")) %>% \n  mutate(location_split = location) %>%\n  separate(location_split, c(\"location_region\",\"location_specific\"), sep = ' -- ') %>%\n  mutate(features = lapply(features, sort, na.last = TRUE)) %>%\n  mutate(feature_v = sapply(features,FUN = function(x) if (all(is.na(x))) NA else paste(x,collapse = \", \"))) %>%\n  mutate(feature_v = str_trim(feature_v)) %>%\n  mutate(features_unnest = features) %>%\n  unnest(cols = c(features_unnest), keep_empty = TRUE) %>% \n  mutate(feature_v = ifelse(is.na(feature_v), \"none\", feature_v)) %>%\n  mutate(features_unnest = ifelse(is.na(features_unnest), \"none\", features_unnest)) %>%\n  mutate(feature_init = case_when(features_unnest == \"Dogs allowed on leash\" ~ \"DA\",\n                                  features_unnest == \"Dogs not allowed\" ~ \"DN\",\n                                  features_unnest == \"Wildlife\" ~ \"Wl\",\n                                  features_unnest == \"Good for kids\" ~ \"GK\",\n                                  features_unnest == \"Lakes\" ~ \"Lk\",\n                                  features_unnest == \"Fall foliage\" ~ \"FF\",\n                                  features_unnest == \"Ridges/passes\" ~ \"RP\",\n                                  features_unnest == \"Established campsites\" ~ \"EC\",\n                                  features_unnest == \"Mountain views\" ~ \"MV\",\n                                  features_unnest == \"Old growth\" ~ \"OG\",\n                                  features_unnest == \"Waterfalls\" ~ \"Wf\",\n                                  features_unnest == \"Wildflowers/Meadows\" ~ \"WM\",\n                                  features_unnest == \"Rivers\" ~ \"Ri\",\n                                  features_unnest == \"Coast\" ~ \"Co\",\n                                  features_unnest == \"Summits\" ~ \"Su\")) %>%\n  mutate(feature_init = ifelse(is.na(feature_init), \"none\", feature_init)) %>%\n  mutate(feature_type = if_else(feature_init %in% c(\"DA\",\"DN\",\"GK\"), \"Companion\", \"Feature\")) %>%\n  mutate(feature_type = ifelse(feature_init == \"none\", \"none\", feature_type)) %>%\n  group_by(name) %>%\n  mutate(feature_n = n()) %>%\n  ungroup() %>%\n  mutate(feature_n = ifelse(feature_init == \"none\", 0, feature_n)) %>%\n  select(name, location_region, location_specific, trail_type, length_miles, \n         gain, highpoint, rating, rating_grp, features, feature_v, features_unnest, \n         feature_init, feature_type, feature_n, description, location, length)\n\n\n\nTo get a sense of what the data look like, I’ll run some historgrams and scatterplots to see how things cluster, if there are outliers or anything else especially noticable.\nUsing log10 for the length scale to even out the spread. The patchwork package stitches the plots together in a neat panel.\n\nhist_length <-\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles)) +\n  geom_histogram(alpha = 0.8) +\n  scale_x_log10() +\n  labs(x = \"Length (miles), log10\")\n\nhist_gain <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(gain)) +\n  geom_histogram(alpha = 0.8) +\n  scale_x_log10()\n\nhist_high <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(highpoint)) +\n  geom_histogram(alpha = 0.8) \n\nhist_rate <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(rating)) +\n  geom_histogram(alpha = 0.8) \n\n(hist_length | hist_gain) /\n  (hist_high | hist_rate)\n\n\n\n\nFor the scatterplots, I plotted length by gain, faceting by ratings groups and then by region. We do have to be careful with ratings, as they are user-generated and some trails have very few votes. Log10 used again for length.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles, gain)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"Length (miles) log10\", y = \"Total Gain\",\n       title = \"Length v Gain, by Rating Group\") +\n  facet_wrap(vars(rating_grp))\n\n\n\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles, gain)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"Length (miles) log 10\", y = \"Total Gain\",\n       title = \"Length v Gain, by Region\") +\n  facet_wrap(vars(location_region))\n\n\n\n\nThe outliers in terms of gain & length clustered in a few regions, so I wanted to see which they were. Not a surprise they clustered in the Cascades & Rainier.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  filter(gain > 15000) %>%\n  filter(length_miles > 90) %>%\n  select(location_region, name, length_miles, gain) %>%\n  arrange(name) \n#> # A tibble: 5 × 4\n#>   location_region      name                                   length_miles  gain\n#>   <chr>                <chr>                                         <dbl> <dbl>\n#> 1 Southwest Washington Pacific Crest Trail (PCT) Section H -…         148. 27996\n#> 2 South Cascades       Pacific Crest Trail (PCT) Section I -…          99  17771\n#> 3 Central Cascades     Pacific Crest Trail (PCT) Section K -…         117  26351\n#> 4 North Cascades       Pacific Northwest Trail - Pasayten Tr…         119  21071\n#> 5 Mount Rainier Area   Wonderland Trail                                93  22000\n\n\n\nNow that we see how the length, gain, highpoint & ratings spread out, I want build a table to see the averages by region.\nI’ve been wanting to take a deeper dive into gt & reactable. I’ve got some basic gt calls down, but for this excercise I wanted to learn how to conditionally format columns based on value. So inspired by Thomas Mock’s gt primer, a basic table with heatmap-like formatting for some columns. See his explainer for details on the code, and for more features than I’m including.\n\n# create by region averages df\nbyregion <-  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  group_by(location_region) %>%\n  summarise(n_region = n(),\n            avglength = mean(length_miles),\n            avgrating = mean(rating),\n            avggain = mean(gain),\n            avghigh = mean(highpoint),\n            minhigh = min(highpoint),\n            maxhigh = max(highpoint)) %>%\n  mutate_at(vars(avglength:avgrating), round, 2) %>%\n  mutate_at(vars(avggain:avghigh), round, 0) \n\nbyregion %>%\n  gt() %>%\n  fmt_number(columns = vars(avggain, avghigh, minhigh, maxhigh), decimals = 0, use_seps = TRUE) %>%\n  # sets the columns and palette to format cell color by value range\n  data_color(\n    columns = vars(avglength, avgrating, avggain, avghigh, minhigh, maxhigh),\n    colors = scales::col_numeric(\n      palette = c(\"#ffffff\", \"#f2fbd2\", \"#c9ecb4\", \"#93d3ab\", \"#35b0ab\"),\n      domain = NULL)) %>%\n#  tab_style calls add border boxes first to column labels, then body cells\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_column_labels(\n        columns = gt::everything()\n        ))) %>%\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_body(\n        rows = gt::everything()\n      ))) %>%\n    cols_align(columns = TRUE, align = \"center\") %>%\n  cols_align(columns = \"location_region\", align = \"left\") %>%\n  cols_width(vars(location_region) ~ px(150),\n             vars(n_region) ~ px(60),\n             starts_with(\"avg\") ~ px(80),\n             ends_with(\"high\") ~ px(90)\n             ) %>%\n  tab_header(title = \"Regional Averages\",\n             subtitle = md(\"_North Cascades have longest trails,\n                           all mountain areas have lots of gain and highest points_\")) %>%\n  cols_label(location_region = \"Region\", n_region = \"N\", avglength = \"Avg Length (miles)\",\n            avgrating = \"Avg Rating\", avggain = \"Avg Gain (ft)\",avghigh = \"Avg high point\",\n             minhigh = \"Lowest high point\", maxhigh = \"Max high point\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Regional Averages\n    \n    \n      North Cascades have longest trails,\nall mountain areas have lots of gain and highest points\n    \n    \n      Region\n      N\n      Avg Length (miles)\n      Avg Rating\n      Avg Gain (ft)\n      Avg high point\n      Lowest high point\n      Max high point\n    \n  \n  \n    Central Cascades\n219\n9.53\n3.04\n2,276\n4,752\n600\n9,511\n    Central Washington\n79\n5.71\n2.78\n823\n2,260\n240\n6,876\n    Eastern Washington\n142\n9.33\n2.15\n1,592\n4,410\n300\n7,310\n    Issaquah Alps\n76\n5.03\n2.53\n984\n1,518\n250\n3,004\n    Mount Rainier Area\n193\n8.19\n3.35\n1,881\n5,222\n800\n10,080\n    North Cascades\n292\n11.24\n3.08\n2,535\n5,111\n125\n9,200\n    Olympic Peninsula\n209\n8.13\n3.32\n1,572\n2,821\n20\n6,988\n    Puget Sound and Islands\n190\n4.25\n2.80\n452\n573\n10\n3,750\n    Snoqualmie Region\n216\n8.71\n3.15\n2,198\n4,467\n450\n9,416\n    South Cascades\n188\n8.44\n3.07\n1,641\n4,732\n922\n12,276\n    Southwest Washington\n120\n6.58\n2.69\n1,171\n1,774\n20\n7,800\n  \n  \n  \n\n\n\n\n\n\nNow let’s look at the effect of trail features on rating.\nFirst we’ll look at average rating by feature, then fit a model. First, a scatter-plot of number of features listed for a trail with user rating. Looks like at a certain point, it’s diminshing returns on trail features in terms of effect on rating.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(feature_n, rating)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"# of features on a trail\", y = \"User rating\",\n       title = \"Features and Rating by Trail Region\") +\n  facet_wrap(vars(location_region))\n\n\n\n\nHere’s a table similar to the one for averages by region. I used the unnested features, so trails will be represented more than once. Dog-free trails do get the highest ratings, but it’s likely because they also tend to have highest high points, so offer views, are challenging, and so get good ratings.\n\nbyfeature <- \ntt_watraildf %>%\n  group_by(features_unnest) %>%\n  summarise(n_feature = n(),\n            avgrating = mean(rating),\n            avglength = mean(length_miles),\n            avggain = mean(gain),\n            avghigh = mean(highpoint),\n            minhigh = min(highpoint),\n            maxhigh = max(highpoint)) %>%\n  mutate_at(vars(avglength:avgrating), round, 2) %>%\n  mutate_at(vars(avggain:avghigh), round, 0) %>%\n  arrange(desc(avgrating))\n\n## create table\nbyfeature %>%\n  gt() %>%\n  fmt_number(columns = vars(n_feature, avggain, avghigh, minhigh, maxhigh), decimals = 0, use_seps = TRUE) %>%\n  # sets the columns and palette to format cell color by value range\n  data_color(\n    columns = vars(avglength, avgrating, avggain, avghigh, minhigh, maxhigh),\n    colors = scales::col_numeric(\n      palette = c(\"#ffffff\", \"#f2fbd2\", \"#c9ecb4\", \"#93d3ab\", \"#35b0ab\"),\n      domain = NULL)) %>%\n  # tab_style calls add border boxes first to column labels, then body cells\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_column_labels(\n        columns = gt::everything()\n      ))) %>%\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_body(\n        rows = gt::everything()\n      ))) %>%\n  tab_header(title = \"Averages by Feature\",\n             subtitle = md(\"_Dog-free trails with waterfalls & high peaks earn high ratings_\")) %>%\n  cols_align(columns = TRUE, align = \"center\") %>%\n  cols_align(columns = \"features_unnest\", align = \"left\") %>%\n  cols_width(vars(features_unnest) ~ px(150),\n             vars(n_feature) ~ px(60),\n             starts_with(\"avg\") ~ px(80),\n             ends_with(\"high\") ~ px(90)\n             ) %>%\n  cols_label(features_unnest = \"Feature\", n_feature = \"N\", avglength = \"Avg Length (miles)\",\n             avgrating = \"Avg Rating\", avggain = \"Avg Gain (ft)\",avghigh = \"Avg high point\",\n             minhigh = \"Lowest high point\", maxhigh = \"Max high point\") \n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Averages by Feature\n    \n    \n      Dog-free trails with waterfalls & high peaks earn high ratings\n    \n    \n      Feature\n      N\n      Avg Rating\n      Avg Length (miles)\n      Avg Gain (ft)\n      Avg high point\n      Lowest high point\n      Max high point\n    \n  \n  \n    Dogs not allowed\n255\n3.52\n9.28\n1,921\n4,173\n10\n10,080\n    Waterfalls\n282\n3.46\n9.64\n1,938\n3,648\n150\n10,080\n    Established campsites\n396\n3.40\n12.80\n2,380\n4,487\n25\n12,276\n    Ridges/passes\n496\n3.25\n12.24\n2,864\n5,575\n400\n9,511\n    Lakes\n583\n3.19\n9.92\n1,988\n4,231\n20\n9,511\n    Old growth\n534\n3.16\n9.00\n1,746\n3,364\n25\n8,096\n    Mountain views\n1,175\n3.13\n9.72\n2,201\n4,621\n20\n12,276\n    Summits\n454\n3.11\n10.40\n2,854\n5,250\n200\n12,276\n    Wildflowers/Meadows\n952\n3.10\n9.26\n1,967\n4,243\n10\n9,511\n    Rivers\n547\n3.05\n9.76\n1,731\n3,205\n10\n12,276\n    Good for kids\n694\n3.01\n4.63\n569\n2,080\n10\n8,245\n    Wildlife\n747\n3.00\n8.84\n1,541\n3,241\n10\n10,080\n    Dogs allowed on leash\n1,045\n2.94\n7.04\n1,379\n3,183\n20\n12,276\n    Fall foliage\n508\n2.94\n8.28\n1,618\n3,334\n20\n9,249\n    Coast\n106\n2.89\n4.12\n351\n433\n10\n6,454\n    none\n68\n2.38\n7.26\n1,758\n3,849\n60\n8,970\n  \n  \n  \n\n\n\n\n\n\nAnd finally a quick model to see what might affect a trail rating.\nIt’s a simple linear model using length, gain, highpoint, & number of features to predict rating. The elevation of the highest point and number of features are both significant. I’d need to do more digging to see what the power of the estimate is on the rating. It’s also slightly counter-intuitive given that we saw in the charts that length, elevation and gain seem to positively affect rating. But then the model only accounts for 4% of varaince, so it’s not telling us much.\n\n# creat df with distinct observations for each trail \ntt_watraildf_dist <- tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) \n\nwtmodel1 <- lm(rating ~ length_miles + gain + highpoint + feature_n, data = tt_watraildf_dist)\nsummary(wtmodel1)\n#> \n#> Call:\n#> lm(formula = rating ~ length_miles + gain + highpoint + feature_n, \n#>     data = tt_watraildf_dist)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.6984 -0.3776  0.3716  0.9284  2.4565 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.205e+00  8.942e-02  24.663  < 2e-16 ***\n#> length_miles -6.565e-03  5.678e-03  -1.156    0.248    \n#> gain         -3.590e-05  3.100e-05  -1.158    0.247    \n#> highpoint     8.318e-05  1.742e-05   4.775 1.93e-06 ***\n#> feature_n     1.272e-01  1.484e-02   8.576  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.398 on 1919 degrees of freedom\n#> Multiple R-squared:  0.0488, Adjusted R-squared:  0.04682 \n#> F-statistic: 24.61 on 4 and 1919 DF,  p-value: < 2.2e-16\n\nThere’s plenty more to do with the set, and some responses I’ve seen on Twitter have been creative…network graphs, better models…but I was able to brush up on gt, learned how to unnest and keep obs where the list was empty. So a successful #tudytuesday.\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/index.html",
    "href": "posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/index.html",
    "title": "Sad Songs & Pretty Charts - a Gosta Berling Music Data Visualization",
    "section": "",
    "text": "For this post, I thought I’d focus on music analytics, given that music and data science/analysis are two things I’ve spent most of my waking hours doing for a number of years now.\nOver the years I’ve made a lot of music in a number of different projects. For most of my time living in the Bay Area I’ve played with some friends in a band called Gosta Berling. We’ve released two EPs and a full album (click on the album covers to give listen)\n  \nOur sound could be called melancholy mood-pop. We like melody, but we were raised on brooding post-punk so a minor key vibe is definitely present. The Spotify API has musical features including danceability, energy, and valence (what they call ‘happiness’). I used Charlie Thompson’s spotifyr package to see how we score. spotifyr has a bunch of functions designed to make it easier to navigate Spotify’s JSON data structure.\nOne quick thing…I’m using Spotify data so in effect validating Spotify. While I appreciate the availability of the data for projects like this, Spotify needs to do much better by way of paying artists. We don’t have tons of streams, but as you can see from this account report… \n…artists get f$ck-all per stream. So sure, use Spotify, it’s a great tool for discovering new music. And while artists pressure them to pay more per stream, you can help by purchasing music from artists you like. The pandemic has killed touring income, so sales are all that many artists have to support themselves. Help them out, buy the music you like. Especially if they’re on Bandcamp and you buy 1st Fridays, when Bandcamp waives their revenue share, meaning the artist gets every penny. Did I mention you can buy our music on Bandcamp? :)\nAnyway, soapbox off…first thing, let’s load the packages we’ll be using:\n\n# load packages\nlibrary(spotifyr) # pull data from spotify\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot text\n\nlibrary(httr)\nlibrary(stringr) # work with string data\nlibrary(lubridate) # work with dates\nlibrary(GGally) # correlation plots\nlibrary(PerformanceAnalytics) # correlation plots\nlibrary(corrr)  # correlation plots\n\nTo get access the Spotify data, you need a developer key. Charlie’s explained how to do it on the package page, so I won’t repeat that here. To set up the keys in your .Renviron, run usethis::edit_r_environ() and add (where the xs are your codes):\n\nSPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx'\nSPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx'\n\n# or do\nSys.setenv(SPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx')\n\nThis call sets your access token for the data session\nIf you run into redirect issues, see this stackoverflow thread, specifically this comment\nFirst thing is to search the artist data for audio features. I’m pulling in everything into a dataframe. Now initially I had a filter for artist = 'Gosta Berling'. But that was also pulling in data from a Swedish prog-metal band Gösta Berling’s Saga. So I needed to filter on our artist ID and pull in albums and singles (for some reason our EPs were listed as singles, but whatever)\n\n# gets full range of information for tracks from artist\ngosta_audio1 &lt;- get_artist_audio_features(artist = \"4Vb2yqJJthJTAZxKz4Aryn\", include_groups = c(\"album\", \"single\"))\n\nOh…why more than one band with Gosta Berling in their name? Well, The Saga of Gösta Berling was Greta Garbo’s first feature-length film, and based on an old Swedish novel Gösta Berling’s Saga about a drunkard outcast priest seeking redemption. When, like our band, you’re a bunch of movie nerds, and a couple of you are especially obsessed with silent films & old Hollywood, you name your band Gosta Berling. And so does a Swedish band…anyway…more about the data.\nThe code here gets a dataframe for each record. I also needed to add album titles. Next steps were to merge the album dataframes together, extract the song IDs and pass them to the get_track_features() function as a list.\n\n# get album tracks, add album name could merge on other df, easier to quick fix this way\ntravel &lt;- get_album_tracks(id = \"0vBs7ZtBj3ROrRyac3M47q\")\ntravel$album &lt;- \"Travel\"\nsweetheart &lt;- get_album_tracks(id = \"0dJBaJ3VFxOtdG5L9yzALJ\")\nsweetheart$album &lt;- \"Everybody's Sweetheart\"\nwinterland  &lt;- get_album_tracks(id = \"6CMekiY6lCIuBZpzFDInpf\")\nwinterland$album &lt;- \"Winterland\"\n\n# merge album files, output track ids to use for audio features\ngbtracks &lt;- data.table::rbindlist(list(sweetheart, travel, winterland))\n#copy result from console to paste as vector below\ngbtrackids &lt;- dput(as.character(gbtracks$id)) \n\ngosta_audio2 &lt;- \n  get_track_audio_features(c(\"2SotrXjkvjTZf05XSMKGyp\", \"07cTJ65GZ4Lvr6b1CtgPll\", \"4ooz79IN3la97See8IMNRL\", \"7pgCh68iFO0LNUNKWTFFIP\", \"4ZCesDRgGWKEXwq8iKw5FB\", \"4ZdH5B3tijHjWiwyOErgtf\", \"5GWKeBYgOsv3PKutDIQoet\", \"0XXWRsY6URe2Vx7Bxs6k06\", \"0t3AGVXHyF3dEYuhvAYuNz\", \"4ObsuwrVLKUq5aF8whrFqk\", \"0PnjWfIPwsqBtllMILjzxB\", \n\"7uQtlGsKxXOzsSapKTZRFU\", \"3kQuG44stzA3pQf7g61Ipt\", \n\"0YH9wkimhRhCmstNZyxPgO\", \"7rEbjyNO0dTEK6x8HkLqAz\", \"4VgEAtVQtkwIHzKMOROk6X\", \"5R9M4s6QZljNPVVzxoy98h\", \"1FNtHQ0juoKg2yCf9u4VSg\", \"5NWmfmupE7FEJ9O1e9vizu\"),\nauthorization = get_spotify_access_token())\n\nThis gets a dataframe with most of what I want…just a few tweaks needed. First, since they weren’t pulled from the get_track_audio_features() call, I used the track id, name, and album track number from the gbtracks dataframe. Also, because the song key returned as only the numeric value, I created the letter name and mode (major or minor), and ordered the columns.\n\n# get track number and name, merge from gbtracks -\n# need b/c these fields not returned from get_track_audio_features()\ngbtrack2 &lt;- gbtracks %&gt;%\n  select(id, name, album, track_number) %&gt;%\n  rename(track_name = name)\n\n# merge to complete df. add names for key and mode\ngosta_audio &lt;- left_join(gosta_audio2, gbtrack2) %&gt;%\n  mutate(key_name = case_when(key == 0 ~ \"C\", key == 2 ~ \"D\", key == 4 ~ \"E\", key == 5 ~ \"F\",\n                              key == 7 ~ \"G\", key == 9 ~ \"A\", key == 11 ~ \"B\")) %&gt;%\n  mutate(mode_name = case_when(mode == 0 ~ \"Minor\", mode == 1 ~ \"Major\")) %&gt;%\n  mutate(key_mode = paste(key_name, mode_name, sep = \" \")) %&gt;%\n  rename(track_id = id) %&gt;%\n  select(album, track_name, track_number, key_mode, time_signature, duration_ms, \n         danceability, energy, loudness, tempo, valence, \n         acousticness, instrumentalness, liveness, speechiness,\n         key_name, mode_name, key, mode)\n\nOk, we’ve got a nice tidy dataframe, let’s do some analysis & visualization!\nSpotify’s developer pages have good explanations of the data. Some notes from spotify here about elements:\n\nMost of the audio features are 0-1, 1 being highest. e.g. higher speechiness = higher ratio of words::music. Valence is “happiness”, where higher = happier.\nLoundess in dB, tempo is BPM.\n\nSo let’s look at a quick summary of the audio features for our songs.\n\n#&gt;   duration_ms      danceability        energy          loudness      \n#&gt;  Min.   : 75933   Min.   :0.2500   Min.   :0.0476   Min.   :-21.350  \n#&gt;  1st Qu.:295380   1st Qu.:0.3545   1st Qu.:0.3260   1st Qu.:-12.031  \n#&gt;  Median :350053   Median :0.3920   Median :0.5190   Median : -9.943  \n#&gt;  Mean   :334762   Mean   :0.4105   Mean   :0.5233   Mean   :-10.705  \n#&gt;  3rd Qu.:385634   3rd Qu.:0.4820   3rd Qu.:0.7160   3rd Qu.: -7.537  \n#&gt;  Max.   :522760   Max.   :0.5730   Max.   :0.9360   Max.   : -6.014  \n#&gt;      tempo           valence        acousticness     instrumentalness \n#&gt;  Min.   : 82.15   Min.   :0.0349   Min.   :0.00371   Min.   :0.00881  \n#&gt;  1st Qu.:116.51   1st Qu.:0.1620   1st Qu.:0.12920   1st Qu.:0.50800  \n#&gt;  Median :141.83   Median :0.2940   Median :0.39300   Median :0.69800  \n#&gt;  Mean   :131.06   Mean   :0.3105   Mean   :0.41332   Mean   :0.62883  \n#&gt;  3rd Qu.:149.98   3rd Qu.:0.4405   3rd Qu.:0.63750   3rd Qu.:0.84450  \n#&gt;  Max.   :166.01   Max.   :0.6960   Max.   :0.88600   Max.   :0.94400  \n#&gt;     liveness       speechiness     \n#&gt;  Min.   :0.0703   Min.   :0.02540  \n#&gt;  1st Qu.:0.1020   1st Qu.:0.02810  \n#&gt;  Median :0.1160   Median :0.03060  \n#&gt;  Mean   :0.1333   Mean   :0.03699  \n#&gt;  3rd Qu.:0.1265   3rd Qu.:0.03865  \n#&gt;  Max.   :0.3300   Max.   :0.11600\n\nFirst I wanted to look at basic correlations for the values. There are a number of ways to run and visualize correlations in r…a few examples follow. First thing I needed to do was a subset of the gosta_audio df for easier calls with the various correlation packages.\nLet’s try correlations in base r. You get the coefficients in the console or you can output to a dataframe to hard-code the visualization.\n\ncor(gbcorr)\n#&gt;                  duration_ms danceability      energy    loudness      tempo\n#&gt; duration_ms       1.00000000   0.03575546 -0.09957649  0.16485951 -0.1589364\n#&gt; danceability      0.03575546   1.00000000 -0.10466026  0.09671649 -0.2719148\n#&gt; energy           -0.09957649  -0.10466026  1.00000000  0.85748849  0.5140085\n#&gt; loudness          0.16485951   0.09671649  0.85748849  1.00000000  0.4952005\n#&gt; tempo            -0.15893636  -0.27191484  0.51400852  0.49520052  1.0000000\n#&gt; valence          -0.04414383  -0.10232090  0.72025346  0.48053791  0.5519247\n#&gt; acousticness     -0.19009855   0.11222116 -0.74742026 -0.65043898 -0.3612391\n#&gt; instrumentalness  0.12784620   0.06977532 -0.53088295 -0.49709651 -0.4411810\n#&gt; liveness         -0.30987073  -0.25213421  0.49374017  0.30054882  0.5316901\n#&gt; speechiness      -0.30678610  -0.31639826  0.45449667  0.27298422  0.4217976\n#&gt;                      valence acousticness instrumentalness   liveness\n#&gt; duration_ms      -0.04414383   -0.1900986       0.12784620 -0.3098707\n#&gt; danceability     -0.10232090    0.1122212       0.06977532 -0.2521342\n#&gt; energy            0.72025346   -0.7474203      -0.53088295  0.4937402\n#&gt; loudness          0.48053791   -0.6504390      -0.49709651  0.3005488\n#&gt; tempo             0.55192475   -0.3612391      -0.44118097  0.5316901\n#&gt; valence           1.00000000   -0.7793878      -0.29646550  0.4743309\n#&gt; acousticness     -0.77938779    1.0000000       0.39266796 -0.3261889\n#&gt; instrumentalness -0.29646550    0.3926680       1.00000000 -0.3406087\n#&gt; liveness          0.47433091   -0.3261889      -0.34060869  1.0000000\n#&gt; speechiness       0.41684028   -0.3150009      -0.56643572  0.7459700\n#&gt;                  speechiness\n#&gt; duration_ms       -0.3067861\n#&gt; danceability      -0.3163983\n#&gt; energy             0.4544967\n#&gt; loudness           0.2729842\n#&gt; tempo              0.4217976\n#&gt; valence            0.4168403\n#&gt; acousticness      -0.3150009\n#&gt; instrumentalness  -0.5664357\n#&gt; liveness           0.7459700\n#&gt; speechiness        1.0000000\ngbcorrs1 &lt;- as.data.frame(cor(gbcorr))\n\nOr you could let some packages do the viz work for you. First, the GGally package, which returns a nice matrix visualization that shows which fields are most postively and negatively correlated.\n\nggcorr(gbcorr, label = TRUE)\n\n\n\n\n\n\n\n\nWe see here some strong postive associations with energy::loundess returning a .9 coefficient, and liveness::speechiness and energy::valence each returning at .7 coefficient. The energy::acousticness and loudness::acousticness combinations each return a -.7 coefficient, showing a negative relationship between those music features.\nWith the corrr package I tried a couple of approaches. First a basic matrix that prints to the console, and doesn’t look much different than base r.\n\ngbcorr %&gt;%\n  correlate(use = \"pairwise.complete.obs\", method = \"spearman\")\n#&gt; # A tibble: 10 × 11\n#&gt;    term     duration_ms danceability energy loudness  tempo valence acousticness\n#&gt;    &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 duratio…     NA            0.0799 -0.319   -0.189 -0.439  -0.191      -0.0737\n#&gt;  2 danceab…      0.0799      NA      -0.269   -0.124 -0.323  -0.209       0.128 \n#&gt;  3 energy       -0.319       -0.269  NA        0.872  0.658   0.761      -0.725 \n#&gt;  4 loudness     -0.189       -0.124   0.872   NA      0.574   0.458      -0.595 \n#&gt;  5 tempo        -0.439       -0.323   0.658    0.574 NA       0.665      -0.479 \n#&gt;  6 valence      -0.191       -0.209   0.761    0.458  0.665  NA          -0.770 \n#&gt;  7 acousti…     -0.0737       0.128  -0.725   -0.595 -0.479  -0.770      NA     \n#&gt;  8 instrum…      0.135        0.0333 -0.447   -0.586 -0.416  -0.177       0.339 \n#&gt;  9 liveness     -0.319       -0.321   0.319    0.144  0.479   0.488      -0.103 \n#&gt; 10 speechi…     -0.331       -0.715   0.382    0.283  0.640   0.396      -0.209 \n#&gt; # ℹ 3 more variables: instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, speechiness &lt;dbl&gt;\n\nNext, I used their rplot call and then rendered a network graph using the network_plot() call.\n\ngbcorrs2 &lt;- correlate(gbcorr)\nrplot(gbcorrs2)\n\n\n\n\n\n\n\n   # network graph\ncorrelate(gbcorr) %&gt;% \n  network_plot(min_cor=0.5)\n\n\n\n\n\n\n\n\nAnd finally the `performance analytics’ package, which was the first of the packages to include significance levels in the default output.\n\n\n\n\n\n\n\n\n\nGiven the correlations, I was interested in exploring the relationships a bit more. So I ran a few scatterplots, with song titles as data labels, and dots colored by album name (using primary color from the cover) to see also if any of the albums clustered at all along either axis. The ggrepel package is used to move the labels off of the dots.\nThere is a bit of a relationship between the Energy score and Valence - so our more energetic songs are our happiest songs. Another interesting way to explore this would be to do some sentiment analysis on the lyics and see if there’s a relationship between energy, valence and using words considered to be more positive in nature. That’s a project on my to-do list.\n\ngosta_audio %&gt;%\n  ggplot(aes(energy, valence, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) +\n  ylim(0, 1) +\n  xlim(0, 1) +\n  theme_minimal() +\n  labs(x = \"energy\", y = \"valence (happiness)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n\nNext I wondered if there’s a relationship between song tempo (beats per minute) & happiness. Our average BPM is 131, which isn’t too far the the mid-range of songs on Spotify. The histogram below used to be on the Spotify API page but they don’t seem to have it up anywhere anymore, so found it via the Wayback Machine\nSo let’s see the resulting scatterplot…\n\n\nshow tempo x valence scatterpplot code\ngosta_audio %&gt;%\n  ggplot(aes(tempo, valence, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) + \n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"tempo (bpm)\", y = \"valence (happiness)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nIt’s not until we get to about the 130 BPM range is it that our songs start to get to even a .25 valence (happiness) score, and from there the relationship between tempo & happiness really kicks in.\nFinally, tempo and energy…\n\n\nshow tempo x energy scatterpplot code\ngosta_audio %&gt;%\n  ggplot(aes(tempo, energy, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) +\n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"tempo (bpm)\", y = \"energy\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nSo yes, most of our songs are in the bottom half of the happy scale. And there does seem to be a bit of a relationship between tempo, energy and happiness and of course a relationship between tempo and energy. Going forward, I’d love to explore our song lyrics via text analysis, especially sentiment analysis to see if the songs Spotify classified as our most sad (low valence) had lyrics that were less positive.\nSo if you like slightly melancholy mood-pop that’s in the 130 +/- BPM range (think The National, Radiohead), I think you’ll like us.\nThanks for reading. And again, give us a listen, and maybe buy some music if you like. :)\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/invited-talk-usf-feb2020/index.html",
    "href": "posts/invited-talk-usf-feb2020/index.html",
    "title": "Invited Talk at University of San Francisco, February 2020",
    "section": "",
    "text": "In early 2020 (back in the days of in-person gatherings) I was invited to give a talk at two budget forums at the University of San Francisco. The general theme was looking at the landscape of enrollments in higher education, with a specific focus on liberal arts colleges, especially Jesuit colleges. Because I collected data from a variety of sources and did much of the work in r, I thought it would make for a good data blog post. Plus, like the Tidy Tuesday HBCU enrollment post, it’s about higher education, which has been my area of professional expertise for a while now.\nI structured the talk around these general questions:\n\nWhat are the major trends affecting the higher education landscape in the US today, particularly traditional liberal arts colleges?\n\nChanging demographics impacting enrollments\nAffordability\n\nWhat social/political threats are on the horizon that colleges should start addressing?\n\nThe talk was divided into four segments:\n\nThe high school graduation picture in California (USF gets most of its students from CA)\nHistorical enrollment at USF, and compared to other Jesuit colleges\nCollege Costs and Affordability\n\nWhat social/political threats are on the horizon that colleges should start addressing?\n\nThe data, code and resulting presentation are in this github repo. What I plan to do in this post is in effect annotate some of the code to explain how I put everything together. And of course to show some charts & tables.\nFirst up is pulling in a few decades of high school graduation and enrollment data and wrangling it all to show historical enrollment and projections through 2029. The full code for that is at the github repo in the file 01_hs enrollment data.R. So what did I do?\nFirst, loaded some packages:\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot\nlibrary(rCAEDDATA) # aggregated CA HS data\nlibrary(readr) # read in flat files\nlibrary(janitor) # data munging/cleaning utilities\n\nThe rCAEDDATA package was put together by my former SFSU IR colleague David Ranzolin. Though the last update was October 2017, it still contains a trove of data, including public HS graduation numbers from 1993 to 2016. That said, I didn’t actually use the package functions, just downloaded the data included with the package. For later years I manually downloaded files from the CA Department of Education’s data pages: 2017 here, and the later years here. The 2017 file has the same structure as the files in David’s package. Later years need a bit of restructuring:\n\ncahsgrad18 &lt;- read.delim(\"data/cahsgrad18.txt\", stringsAsFactors=FALSE) %&gt;%\n    clean_names() %&gt;% \n    filter(reporting_category == \"TA\") %&gt;%\n    filter(aggregate_level == \"S\") %&gt;%\n    filter(dass == \"All\") %&gt;%\n    filter(charter_school == \"All\") %&gt;%\n    mutate(YEAR = \"2018\") %&gt;%\n    mutate(YEAR = factor(YEAR)) %&gt;%\n    mutate_at(vars(ends_with(\"_code\")), as.character) %&gt;%\n    mutate(county_code = ifelse(nchar(county_code) == 1, \n                str_pad(county_code, 2, \"left\", \"0\"), county_code)) %&gt;%\n    mutate(CDS_CODE = paste(county_code, district_code, school_code, sep = \"\")) %&gt;%\n    mutate(GRADS = as.integer(ifelse(regular_hs_diploma_graduates_count == \"*\", \n                0, regular_hs_diploma_graduates_count))) %&gt;%\n    mutate(UC_GRADS = as.integer(ifelse(met_uc_csu_grad_req_s_count == \"*\", \n                0, met_uc_csu_grad_req_s_count))) %&gt;%\n    select(CDS_CODE, GRADS, UC_GRADS, YEAR) %&gt;%\n    group_by(YEAR) %&gt;%\n    summarise(total_grads = sum(GRADS),\n                        uccsu = sum(UC_GRADS),\n                        notuccsu = total_grads - uccsu)\n\nNext, some projected HS graduation data from the CA Department of Finance. I did a quick transposing of the “HS Grads Table” tab in excel and used that to read into r. You can see the file I used at the github repo’s data folder.\n\ngrproj_to2028 &lt;- readxl::read_excel(\"data/capublic_k12_enrollproj_to2028.xlsx\",\n                                sheet = \"hsgrads-tr\") %&gt;%\n    filter(year != \"2017-18\") %&gt;%\n    mutate(yearend = str_sub(year, 6, 7)) %&gt;%\n    mutate(YEAR = paste(\"20\", yearend, sep = \"\")) %&gt;%\n    #mutate(YEAR = factor(year_ch)) %&gt;%\n    mutate(uccsu = as.integer(NA)) %&gt;%\n    mutate(notuccsu = as.integer(NA)) %&gt;%\n    mutate(notuccsu = as.integer(NA)) %&gt;%\n    mutate(type = \"Projected\") %&gt;%\n    select(YEAR, total_grads = total, uccsu, notuccsu, type) %&gt;%\n    # amend 2018-19 with actual results from\n    # https://dq.cde.ca.gov/dataquest/dqcensus/CohRateLevels.aspx?cds=00&agglevel=state&year=2018-19\n    mutate(total_grads = ifelse(YEAR == \"2019\", 417496, total_grads)) %&gt;%\n    mutate(uccsu = ifelse(YEAR == \"2019\", 210980, uccsu)) %&gt;%\n    mutate(notuccsu = ifelse(YEAR == \"2019\", total_grads - uccsu, notuccsu)) %&gt;%\n    mutate(type = ifelse(YEAR == \"2019\", \"Actual\", type))\n\nThe projected HS grad stats didn’t have values for UC/CSU grads, so I needed to impute that as part of the merging of the actual & projected files. I also calculated year-over-year percent changes for a few fields. (though I did it manually not with a function like in the Tidy Tuesday HBCU post). The code below gets us the cahsgrads_1993_2028 dataframe.\n\ncahsgrads_1993_2028 &lt;- rbind(cahsgrad93to18_tot, grproj_to2028) %&gt;%\n    mutate(pctucgrads = uccsu / total_grads) %&gt;%\n    arrange(YEAR) %&gt;%\n    # add projected uccsu grads based on constant 2017-18 to 2018-19 increase 0.0061437\n    mutate(pctucgrads = ifelse(YEAR &gt;= \"2020\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2021\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2022\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2023\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2024\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2025\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2026\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2027\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2028\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(pctucgrads = ifelse(YEAR == \"2029\", lag(pctucgrads) + 0.0061437, pctucgrads)) %&gt;%\n    mutate(uccsu = ifelse(type == \"Projected\", round(pctucgrads * total_grads, 0), uccsu)) %&gt;%\n    mutate(notuccsu = ifelse(type == \"Projected\", round(total_grads -uccsu, 0), notuccsu)) %&gt;%\n    mutate(gr_tot_change = (total_grads - lag(total_grads))) %&gt;%\n    mutate(gr_tot_pct_change = (total_grads/lag(total_grads)- 1)) %&gt;%\n    mutate(gr_uc_change = (uccsu - lag(uccsu))) %&gt;%\n    mutate(gr_uc_pct_change = (uccsu/lag(uccsu) - 1)) %&gt;%\n    mutate(gr_notuc_change = (notuccsu - lag(notuccsu))) %&gt;%\n    mutate(gr_notuc_pct_change = (notuccsu/lag(notuccsu) - 1)) %&gt;%\n    select(YEAR, total_grads, uccsu, notuccsu, type, pctucgrads, type, everything())\n\ncahsgrads_1993_2028 &lt;- cahsgrads_1993_2028 %&gt;%\n    mutate(pctucgrads = ifelse(year_ch &gt;= \"9293\", uccsu / total_grads, pctucgrads))\n\nNow that we have the data, let’s make the chart I presented to the group, showing actual & projected high school grads in California, breaking out the UC/CSU eligible grads.\nWhat does the chart tell us? Well…\n\n67% increase in grads from 1993 to 2018\nUC/CSU eligibility 33% in 1993, 50% in 2018\nGrads expected to peak in 2023, then decline slightly\n\nBut these assumptions were all pre-COVID - I gave the talks in early February of 2020. Given factors such as migration patterns within the state, people moving out of CA, parents moving their kids to private schools, etc., the actual graduation picture is sure to change.\n\n\nshow the enrollment charts code\ncahsgrads_1993_2028 %&gt;%\n    select(YEAR, uccsu, notuccsu) %&gt;%\n    pivot_longer(-YEAR, names_to = \"ucelig\", values_to = \"n\") %&gt;%\n    ggplot(aes(YEAR, n, fill = rev(ucelig))) +\n    geom_bar(stat = \"identity\", color = \"black\") +\n    geom_segment(aes(x = 27.5, y = 0, xend = 27.5, yend = 500000),\n                             size = 2, color = \"grey\") +\n    scale_y_continuous(labels = scales::comma, limits = c(0, 500000)) + \n    scale_x_discrete(breaks = c(\"9293\", \"9798\", \"0203\", \"0708\", \"1213\", \"1718\", \"2223\", \"2829\")) +\n    scale_fill_manual(values = c(\"#1295D8\", \"white\"),\n                                        labels = c(\"UC CSU Eligible\", \"Not UC/CSU Elig\")) +\n    labs(x = \"\", y = \"\", caption = \"Sources: Actual: CA Dept of Education. Projections: CA Dept of Finance\",\n             fill = \"UC/CSU Eligible?\") +\n    annotate(\"text\", x = 28, y = 500000, label = \"Projected\",\n                     size = 6, fontface = \"italic\", hjust = -.25) +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(),   panel.grid.minor = element_blank(),\n                legend.position = c(.2, .9),\n                legend.title=element_text(size=12), legend.text=element_text(size=12),\n                axis.text.x = element_text(size = 7))\n\n\n\n\n\n\n\n\n\nNext I presented some data on USF enrollment and compared USF to their Jesuit college peers. For that I used two sources: data from the Delta Cost Project (DCP), and since DCP stops at 2015, downloaded some files directly from IPEDS and read in the CSVs. I could also have used r tools like the Urban Insitute’s educationdata package an API wrapper to that scrapes NCES and other websites for data. (I also referenced that package in my HBCU post).\nThe import code isn’t all that challenging - read in the CSV, select the fields I needed, do a bit of basic cleaning. So no need to show it. You can see it in the github repo - go to the file ’02_ipeds_enroll.R`. Though if anything’s worth highlighting it’s the need to create an object of IPEDS unitids for Jesuit colleges so I could group them during analysis. The Jesuit colleges include names you know: Georgetown, Gonazaga, Boston College, the Loyolas (Chicago, LA, New Orleans, Baltimore), etc…\n\njesids &lt;- c(\"164924\", \"181002\", \"186432\", \"122931\", \"169716\", \"159656\", \"127918\", \"192323\", \n                    \"163046\", \"122612\", \"236595\", \"239105\", \"203368\", \"179159\", \"215770\", \n                        \"215929\", \"131496\", \"166124\", \"102234\", \"117946\", \"206622\", \"102234\", \n                        \"166124\", \"117946\", \"206622\", \"235316\", \"129242\")\n\nFirst up is USF enrollment from Fall 1987 to Fall 2018 (the latest year that IPEDS had available as of February 2020). The ggplot code is mostly basic, so I’ve folded it…click the arrow to show the code. It’s mostly worth checking out for this neat solution to a crowded x axis - the every_nth function to count every n value, and apply it to the breaks - in this case I set it to n=3. I’d tried scales::pretty_breaks() but it didn’t work. I also used ggrepel to move the labels a bit.\nWhat’s the enrollment picture at USF? Well, this chart tells us that:\n\nUndergraduate enrollment have increased by 60% since 1987.\nGraduate enrollments hovering around 3,500 for a number of years, and up to +/- 4,000 since 2016.\nRatio of undergraduate::graduate enrollments steady over time, generally +/- 2% points from 60%.\n\nHow does this relate to the high school graduation trends in CA & nearby states?\n\nWith 63% of new students coming from California, high school enrollments here will have most impact.\nWestern Interstate Commission for Higher Education (WICHE) projects HS grads from all western states to peak in 2024 at 862,000, then decline for a few years, rebounding again around 2032. (Knocking at the College Door, https://knocking.wiche.edu)\nDuring 1980s the Gen X population drop mitigated by increased college-going rates – what will happen this time?\n\n\n\nshow the enrollment charts code\nevery_nth = function(n) {\n  return(function(x) {x[c(TRUE, rep(FALSE, n - 1))]})\n}\n\nplot_usfenr_ug &lt;-\n    ipeds_fallenroll_8718 %&gt;%\n    filter(UNITID == \"122612\", level == \"Undergraduate\") %&gt;%\n    select(year, level, tot_enr) %&gt;%\n    ggplot(aes(year, tot_enr)) +\n    geom_bar(stat = \"identity\", fill = \"#00543C\") +\n    # geom_text(aes(label = scales::comma(round(tot_enr), accuracy = 1)), \n    #                   color = \"#919194\", vjust = -.75, size = 3) +\n    geom_text_repel(data = ipeds_fallenroll_8718 %&gt;%\n                                        filter(UNITID == \"122612\", level == \"Undergraduate\",\n                                        year %in% c(\"Fall 1987\", \"Fall 1990\", \"Fall 1990\", \n                                        \"Fall 1993\", \"Fall 1996\", \"Fall 1999\",\n                                        \"Fall 2002\", \"Fall 2005\", \"Fall 2008\", \"Fall 2011\", \n                                        \"Fall 2014\", \"Fall 2017\", \"Fall 2018\")),\n                                    aes(label = scales::comma(round(tot_enr), \n                                            accuracy = 1)), nudge_y = 400,\n                                        min.segment.length = 0,\n                                        size = 3, color = \"#919194\") +\n    scale_x_discrete(breaks = every_nth(n = 3)) +\n    scale_y_continuous(label = scales::comma, limits = c(0, 7000),\n                                         breaks = c(0, 1750, 3500, 5250, 7000)) +\n    labs(x = \"\", y = \"\") +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(),   panel.grid.minor = element_blank(),\n                axis.text.y = element_text(size = 10))\n\nplot_usfenr_gr &lt;-\n    ipeds_fallenroll_8718 %&gt;%\n    filter(UNITID == \"122612\", level == \"Graduate\") %&gt;%\n    select(year, level, tot_enr) %&gt;%\n    ggplot(aes(year, tot_enr)) +\n    geom_bar(stat = \"identity\", fill = \"#FDBB30\") +\n    geom_text_repel(data = ipeds_fallenroll_8718 %&gt;%\n                                        filter(UNITID == \"122612\", level == \"Graduate\",\n                                     year %in% c(\"Fall 1987\", \"Fall 1990\", \"Fall 1990\", \n                                   \"Fall 1993\", \"Fall 1996\", \"Fall 1999\",\n                                   \"Fall 2002\", \"Fall 2005\", \"Fall 2008\", \"Fall 2011\", \n                                   \"Fall 2014\", \"Fall 2017\", \"Fall 2018\")),\n                                    aes(label = scales::comma(round(tot_enr), \n                                                            accuracy = 1)), nudge_y = 200,\n                                    min.segment.length = 0,\n                                    size = 3, color = \"#919194\") +\n    scale_x_discrete(breaks = every_nth(n = 3)) +\n    scale_y_continuous(label = scales::comma, limits = c(0, 4500),\n                                         breaks = c(0, 1500, 3000, 4500)) +\n    labs(x = \"\", y = \"\") +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(),   panel.grid.minor = element_blank(),\n                axis.text.y = element_text(size = 10))\n\nplot_usfenr_all &lt;- plot_usfenr_ug + plot_usfenr_gr +\n    plot_layout(ncol = 1) + plot_annotation(\n    title = 'Fall Enrollment at USF: 1987 - 2018',\n    subtitle = \"&lt;span style = 'color:#00543C;'&gt;Green = Undergraduate&lt;/span&gt;, \n    &lt;span style = 'color:#FDBB30;'&gt;Gold = Graduate&lt;/span&gt;\",\n    caption = \"Sources: Delta Cost Project & IPEDS\", \n    theme = theme(plot.subtitle = element_markdown(face = \"italic\", size = 9)))\nplot_usfenr_all\n\n\n\n\n\n\n\n\n\nQuick note about this plot…I was getting an annoying Error in grid.Call(C_textBounds, as.graphicsAnnot(x\\(label), x\\)x, x$y, : polygon edge not found message trying to run these plots. For the presentation I was using the Calibri font, with this call in the theme() section: text = element_text(family = \"Calibri\"). I removed that & the error went away. But this after trying everything from reinstalling Quartz, shutting down all browser windows, running `dev.off()’ in the console…got rid of the special font & no error.\nAnyway…back to the charts.\nI wanted to show USF undergraduate enrollment indexed over time relative to their Jesuit college peers. The final version of the chart is below, complete with annotations I added in power point. There are ways to do similar annotations on r; I used power point because I could do it quicker, with less fuzting around with annotation placement after rendering and saving the image.\nWe see that USF is in the upper quarter of total enrollment growth. Gonzaga & St. Louis University, two schools well-known thanks to success in the NCAA men’s basketball tournament, showed significant growth in the period. Here you might say “but wait, Georgetown has had NCAA success”, and I’d reply “yes, but their success started before 1987, so within this period didn’t grow as much as Gonzaga & St. Louis”.\n\n\n\nJesuit College Enrollment\n\n\nSo how did I make this chart? How did we get the green line for USF, with all else in gray?\nFirst I created a dataframe of the Jesuit colleges, and indexed changes in enrollment to 1.\n\nenrollindex_jes &lt;-\nipeds_fallenroll_8718 %&gt;%\n    filter(jescoll == 1, level == \"Undergraduate\") %&gt;%\n  mutate(enr_pct_change2 = enr_pct_change / 100) %&gt;%\n    mutate(enr_pct_change2 = ifelse(year == \"Fall 1987\", 1, enr_pct_change2)) %&gt;%\n    arrange(UNITID, year) %&gt;%\n    group_by(UNITID) %&gt;%\n    mutate(index_enr_inst = 1) %&gt;%\n    mutate(index_enr_inst = ifelse(year &gt;= \"Fall 1988\", cumsum(enr_pct_change2),\n                                                                 index_enr_inst)) %&gt;%\n    ungroup() %&gt;%\n    ## fix loyola NO b/c of enroll drop after katrina\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year == \"Fall 2006\"), \n                                                    0.833399497, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2007\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2008\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2009\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2010\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2011\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2012\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2013\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2014\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2015\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2016\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2017\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year &gt;= \"Fall 2018\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %&gt;%\n    select(UNITID, inst_name, year, tot_enr, enr_change, enr_pct_change, \n                 enr_pct_change2, index_enr_inst)\n\nWhy all the manual fixes to Loyola in New Orleans? Well, look at the chart again. See the dip in enrollment around 2005? What might have tanked enrollment at a New Orleans-based college in 2005? Oh right…Hurricane Katrina. To smooth out the drop, I reindexed from the 2006 enrollment point, and added to the index sum after that. For some reason I couldn’t identify, the usual lag from prior year wasn’t working so I just did it manually.\nAs for the plot…to get the green line, I first plotted everything but USF in grey, then plotted USF in green. Saved the plot, then added annotations in power point.\n\nggplot(enrollindex_jes, aes(year, index_enr_inst, group = UNITID)) +\n    geom_line(data = subset(enrollindex_jes, UNITID != \"122612\"), color = \"grey\") +\n    geom_line(data = subset(enrollindex_jes, UNITID == \"122612\"), \n        color = \"#00543C\", size = 1) +\n    scale_y_continuous(limits = c(-.5, 2),\n        breaks = c(-.5, 0, .5, 1, 1.5, 2)) +\n    labs(x = \"\", y = \"\") +\n    theme_minimal() +\n    theme(#text = element_text(family = \"Calibri\"),\n                panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n                axis.text.y = element_text(size = 14))\n\nI had planned to use the Delta Cost/IPEDS tuition & fees data for some charts in the section on affordability, but it worked out better to pull data from the College Board.\nAgain, you can access the presentation in the github repo. If you have questions or comments about the code or the content, you can find me on LinkedIn or Twitter by clicking on the icon links at the bottom of the post or on the main page. Or send me an email if you already know how to find me that way.\n\n\n\nJesuit College Enrollment"
  },
  {
    "objectID": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html",
    "href": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html",
    "title": "Tidy Tuesday, April 07, 2020 - Le Tour! (Stage 1, cleaning the data)",
    "section": "",
    "text": "|"
  },
  {
    "objectID": "posts/tidy-tuesday-apr-07-2020-LeTour-stage2/index.html",
    "href": "posts/tidy-tuesday-apr-07-2020-LeTour-stage2/index.html",
    "title": "Tidy Tuesday, April 07, 2020 - Le Tour! (Stage 2, charts!)",
    "section": "",
    "text": "Back in the saddle for Stage 2 of the Tour de France data ride\nStage 1 ended up being all about wrangling and cleaning the #TidyTuesday Tour de France data. When I first dug into the data I wasn’t sure what I wanted to visualize. It wasn’t until I spent some time living with the data, seeing what was there and looking at the #tidytuesday TdF submissions on Twitter so I didn’t repeat what was done that I decided I wanted to look at results by stage, specifically the gaps between the winners of each stage and the times recorded for the next-best group and the last rider(s) across the line. Charlie Gallagher took a similar approach at the data, using overall race results for the GC riders.\nA quick but important aside - in the Tour, as in most (all?) UCI races, while each rider is accorded a place - 1, 2, 3, etc… - times are calculated by identifiable groups crossing the line. So let’s say you are 2nd to 15th in the 1st group (of 15 total riders) that crosses with barely any daylight between riders; you each get the same time as the winner. But only 1 rider wins the stage. In any stage, there could be only 2 or 3 identifiable time groups, or there could be many groups. Depends on the stage type and other factors - crashes, where in the race the stage took place, etc…\nWhat this means for my project here is I needed to wrangle data so that I was able to identify two time groups apart from the winner; the next best group and the last group. Each group could have more than 1 rider. Download and clean the stage results data and you’ll see what I mean.\nSo let’s look at some code and charts.\nAt the end of Stage 1 we had a number of data frames. I’m joining two for this analysis, one with stage winners (which has important stage characteristic data) and a set of all riders in every stage from 1903 to 2019. We’ll first load the packages we need…\n\n# load packages\nlibrary(tidyverse) # to do tidyverse things\nlibrary(lubridate) # to do things with dates & times\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # to stitch together plots\n\n# create notin operator to help with cleaning & analysis\n`%notin%` <- negate(`%in%`)\n\nThen join the sets. For the purposes of this post I’ll just load an RDS I created (it’s not uploaded to the repo, sorry, but you can recreate it with the code.\n\ntdf_stageall <- merge(tdf_stagedata, tdf_stagewin, by.x = c(\"race_year\", \"stage_results_id\"),\n                      by.y = c(\"race_year\", \"stage_results_id\"), all = T)\n\n\ntdf_stageall <- readRDS(\"data/tdf_stageall.rds\")\nglimpse(tdf_stageall)\n\nRows: 255,807\nColumns: 32\n$ race_year        <dbl> 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903,…\n$ stage_results_id <chr> \"stage-01\", \"stage-01\", \"stage-01\", \"stage-01\", \"stag…\n$ edition          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ split_stage.x    <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ rider            <chr> \"Garin Maurice\", \"Pagie Émile\", \"Georget Léon\", \"Auge…\n$ rider_first      <chr> \"Maurice\", \"Émile\", \"Léon\", \"Fernand\", \"Jean\", \"Marce…\n$ rider_last       <chr> \"Garin\", \"Pagie\", \"Georget\", \"Augereau\", \"Fischer\", \"…\n$ rider_firstlast  <chr> \"Maurice Garin\", \"Émile Pagie\", \"Léon Georget\", \"Fern…\n$ rank2            <chr> \"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008…\n$ time             <Period> 17H 45M 13S, 55S, 34M 59S, 1H 2M 48S, 1H 4M 53S, 1…\n$ elapsed          <Period> 17H 45M 13S, 17H 46M 8S, 18H 20M 12S, 18H 48M 1S, …\n$ points           <int> 100, 70, 50, 40, 32, 26, 22, 18, 14, 10, 8, 6, 4, 2, …\n$ bib_number       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ team             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ age              <int> 32, 32, 23, 20, 36, 37, 25, 33, NA, 22, 26, 28, 21, 2…\n$ rank             <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"1…\n$ stage_num.x      <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n$ stage_ltr.x      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ stage_date       <date> 1903-07-01, 1903-07-01, 1903-07-01, 1903-07-01, 1903…\n$ stage_type       <chr> \"Flat / Plain / Hilly\", \"Flat / Plain / Hilly\", \"Flat…\n$ Type             <chr> \"Plain stage\", \"Plain stage\", \"Plain stage\", \"Plain s…\n$ split_stage.y    <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ Origin           <chr> \"Paris\", \"Paris\", \"Paris\", \"Paris\", \"Paris\", \"Paris\",…\n$ Destination      <chr> \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon…\n$ Distance         <dbl> 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467…\n$ Winner           <chr> \"Maurice Garin\", \"Maurice Garin\", \"Maurice Garin\", \"M…\n$ winner_first     <chr> \"Maurice\", \"Maurice\", \"Maurice\", \"Maurice\", \"Maurice\"…\n$ winner_last      <chr> \"Garin\", \"Garin\", \"Garin\", \"Garin\", \"Garin\", \"Garin\",…\n$ Winner_Country   <chr> \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA…\n$ Stage            <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ stage_ltr.y      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ stage_num.y      <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n\n\nThis set has many columns that we’ll build off of to use in analysis going forward. To get the changes in gaps by stage types, we’ll build another set. Because we want to look both at changes in stage types and gaps between winners and the field, the trick here is to sort out for each stage in each race year who the winners are (easy), who has the slowest time (mostly easy) and who has the 2nd best record time.\nThat last item it tough because of the time & rank method I described above. The script below is commented to show why I did what I did. Much of the code comes from looking at the data and seeing errors, issues, etc. Not including that code here. Also, much of my ability to spot errors comes from knowledge about the race, how it’s timed, some history. Domain knowledge helps a lot when cleaning & analyzing data.\n\nstage_gap <-\ntdf_stageall %>%\n  arrange(race_year, stage_results_id, rank2) %>%\n  #  delete 1995 stage 16 - neutralized due to death in stage 15, all times the same\n  mutate(out = ifelse((race_year == 1995 & stage_results_id == \"stage-16\"),\n                       \"drop\", \"keep\")) %>%\n  filter(out != \"drop\") %>%\n  # delete  missing times\n  filter(!is.na(time)) %>%\n  # remove non-finishers/starters, change outside time limit rank to numeric to keep in set\n  filter(rank %notin% c(\"DF\", \"DNF\", \"DNS\", \"DSQ\", \"NQ\")) %>%\n  filter(!is.na(rank)) %>%\n\n  # OTLs are ejected from the race because they finished outside a time limit. But we need them in the set.\n  mutate(rank_clean = case_when(rank == \"OTL\" ~ \"999\",\n                           TRUE ~ rank)) %>% \n  # sortable rank field\n  mutate(rank_n = as.integer(rank_clean)) %>%\n  # creates total time in minutes as numeric, round it to 2 digits\n  mutate(time_minutes = ifelse(!is.na(elapsed),\n                              day(elapsed)*1440 + hour(elapsed)*60 + minute(elapsed) + second(elapsed)/60,\n                               NA)) %>%\n  mutate(time_minutes = round(time_minutes, 2)) %>%\n  \n  # create field for difference from winner\n  group_by(race_year, stage_results_id) %>% \n  arrange(race_year, stage_results_id, time_minutes, rank2) %>%\n\n  mutate(time_diff = time_minutes - min(time_minutes)) %>%\n  mutate(time_diff_secs = time_diff*60) %>%\n  mutate(time_diff = round(time_diff, 2)) %>%\n  mutate(time_diff_secs = round(time_diff_secs, 0)) %>%\n  mutate(time_diff_period = seconds_to_period(time_diff_secs)) %>%\n  mutate(rank_mins = rank(time_minutes, ties.method = \"first\")) %>%\n  # create rank field to use to select winner, next best, last\n  mutate(compare_grp = case_when(rank_n == 1 ~ \"Winner\",\n                                 (rank_n > 1 & time_diff_secs > 0 & rank_mins != max(rank_mins))\n                                 ~ \"Next best2\",\n                                  rank_mins == max(rank_mins) ~ \"Last\",\n                                 TRUE ~ \"Other\")) %>%\n  ungroup() %>%\n  group_by(race_year, stage_results_id, compare_grp) %>% \n  arrange(race_year, stage_results_id, rank_mins) %>%\n  mutate(compare_grp = ifelse((compare_grp == \"Next best2\" & rank_mins == min(rank_mins)),\n                               \"Next best\", compare_grp)) %>%\n  mutate(compare_grp = ifelse(compare_grp == \"Next best2\", \"Other\", compare_grp)) %>%\n  ungroup() %>%\n  mutate(compare_grp = factor(compare_grp, levels = c(\"Winner\", \"Next best\", \"Last\", \"Other\"))) %>%\n  # create race decade field\n  mutate(race_decade = floor(race_year / 10) * 10) %>%\n  mutate(race_decade = as.character(paste0(race_decade, \"s\"))) %>%\n  # keep only winner, next, last\n  filter(compare_grp != \"Other\") %>%\n  select(race_year, race_decade, stage_results_id, stage_type, rider_firstlast, bib_number, Winner_Country,\n         rank, rank_clean, rank_n, time, elapsed, time_minutes, time_diff, time_diff_secs, time_diff_period, \n         rank_mins, compare_grp) \n\nOk, finally, let’s see what this data looks like. First a chart to show averages and quartile ranges for the gaps by stage type. Create a data object with the values, then the plots. Faceting by stage type didn’t work because the y axis ranges were very different. So we’ll use patchwork to stitch them together in one plot. The medians are the red dots, interquartile ranges at either end of the line, and means are in black. I included both means & medians because the spread for some stage types was so great.\n\n\nShow stage gap charts code\ngapranges <- stage_gap %>%\n  filter(compare_grp != \"Winner\") %>%\n  filter(stage_type %notin% c(\"Other\", \"Time Trial - Team\")) %>%\n  group_by(stage_type, compare_grp) %>%\n  summarise(num = n(), \n            lq = quantile(time_diff_secs, 0.25),\n            medgap = median(time_diff_secs),\n            uq = quantile(time_diff_secs, 0.75),\n            lq_tp = (seconds_to_period(quantile(time_diff_secs, 0.25))),\n            medgap_tp = (seconds_to_period(median(time_diff_secs))),\n            uq_tp = (seconds_to_period(quantile(time_diff_secs, 0.75))),\n            avggap = round(mean(time_diff_secs),2),\n            avggap_tp = round(seconds_to_period(mean(time_diff_secs)), 2))\n\ngapplot1 <-\ngapranges %>%\n  filter(compare_grp == \"Next best\") %>%\n  ggplot(aes(stage_type, medgap, color = avggap)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"#0055A4\") +\n  geom_point(size = 2, color = \"#EF4135\") +\n  geom_point(aes(y = avggap), size = 2, color = \"black\", alpha = .8) +\n  geom_text(aes(label = medgap_tp), \n            size = 3, color = \"#EF4135\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = uq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = lq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.2) +\n  geom_text(aes(label = avggap_tp, y = avggap_tp),\n            size = 3, color = \"black\", alpha = .8, hjust = -.1) +\n  labs(title = \"Time Gap from Stage Winner to Next Best Time\",\n       subtitle = \"Median & Inter-quartile Ranges (avg in black)\",\n       y = \"Time Gap from Winner\", x = \"Stage Type\") +\n  theme_light() +\n  theme(plot.title = element_text(color = \"#0055A4\", size = 9),\n        plot.subtitle = element_text(face = \"italic\", color = \"#EF4135\",\n                                     size = 8),\n        axis.title.x = element_text(color = \"#0055A4\"),\n        axis.title.y = element_text(color = \"#0055A4\"), \n        axis.text.x = element_text(color = \"#0055A4\"),\n        axis.text.y=element_blank())\n\ngapplot2 <-\ngapranges %>%\n  filter(compare_grp == \"Last\") %>%\n  ggplot(aes(stage_type, medgap, color = avggap)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"#0055A4\") +\n  geom_point(size = 2, color = \"#EF4135\") +\n  geom_point(aes(y = avggap), size = 2, color = \"black\", alpha = .8) +\n  geom_text(aes(label = medgap_tp), \n            size = 3, color = \"#EF4135\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = uq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = lq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.1) +\n  geom_text(aes(label = avggap_tp, y = avggap_tp),\n            size = 3, color = \"black\", alpha = .8, hjust = -.1) +\n  labs(title = \"Time Gap from Stage Winner to Slowest Time\",\n       subtitle = \"Median & Inter-quartile Ranges (avg in black)\",\n       y = \"\", x = \"Stage Type\") +\n  theme_light() +\n  theme(plot.title = element_text(color = \"#0055A4\", size = 9),\n        plot.subtitle = element_text(face = \"italic\", color = \"#EF4135\",\n                                     size = 8),\n        axis.title.x = element_text(color = \"#0055A4\", size = 9),\n        axis.text.x = element_text(color = \"#0055A4\", size = 9),\n        axis.text.y=element_blank())\n\ngapplot1 + gapplot2 +\n  plot_annotation(title = \"Tour de France Stages, 1903 to 2019\",\n                  theme = theme(plot.title = \n                                  element_text(color = \"#0055A4\", size = 10)))\n\n\n\n\n\nWhat do these charts tell us? Well unsurprisingly mountain stages tend to have longer gaps between winners and the rest of the field than do flat/plain/hilly stages. Time trials are usually on flat or hilly stages, so they behave more like all other flat/plain/hilly stages. Even looking at the median to smooth for outliers, half of the last men in on mountain stages came in under 36 minutes, half over 36 minutes. The last 25% of mountain-stage riders came in an hour or more after the winner.\nHow has this changed over time? Well let’s facet out by degree decade.\nFirst thing that needs doing is to build a dataframe for analysis - it will have medians my race year and stage type. But for the chart we want to have a decade field. Turns out this was a bit complicated in order to get the chart I wanted. You can see in the code comments why I did what I did.\n\n\nShow df build code\ngaprangesyrdec <- \nstage_gap %>%\n  filter(compare_grp != \"Winner\") %>%\n  filter(stage_type %notin% c(\"Other\", \"Time Trial - Team\")) %>%\n  group_by(stage_type, compare_grp, race_year) %>%\n  summarise(num = n(), \n            lq = quantile(time_diff_secs, 0.25),\n            medgap = median(time_diff_secs),\n            uq = quantile(time_diff_secs, 0.75),\n            lq_tp = (seconds_to_period(quantile(time_diff_secs, 0.25))),\n            medgap_tp = (seconds_to_period(median(time_diff_secs))),\n            uq_tp = (seconds_to_period(quantile(time_diff_secs, 0.75))),\n            avggap = round(mean(time_diff_secs),2),\n            avggap_tp = round(seconds_to_period(mean(time_diff_secs)), 2)) %>%\n  ungroup() %>%\n  # need to hard code in rows so x axis & faceting works in by decade charts\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1915, .before = 13) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1916, .before = 14) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1917, .before = 15) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1918, .before = 16) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1915, .before = 123) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1916, .before = 124) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1917, .before = 125) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1918, .before = 126) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1915, .before = 233) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1916, .before = 234) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1917, .before = 235) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1918, .before = 236) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1915, .before = 343) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1916, .before = 344) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1917, .before = 345) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1918, .before = 346) %>%\n\n    # need field for x axis when faciting by decade\n  mutate(year_n = str_sub(race_year,4,4)) %>%\n  # create race decade field\n  mutate(race_decade = floor(race_year / 10) * 10) %>%\n  mutate(race_decade = as.character(paste0(race_decade, \"s\"))) %>%\n#  mutate(race_decade = ifelse(race_year %in%))\n  arrange(stage_type, compare_grp, race_year) %>%\n  select(stage_type, compare_grp, race_year, year_n, race_decade, everything())\n\n\nNow that we have a dataframe to work from, let’s make a chart. But to do that we have to make a few charts and then put them together with the patchwork package.\nFirst up is changes in the mountain stages and the median gaps between winner and next best recorded time. I grouped into three decade sets. Note that because of changes in the gaps over time, the y axes are a bit different in the early decades of the race. Also note at how I was able to get hours:seconds:minutes to show up on the y axis. The x axis digits are that way because race year would repeat in each facet, so I had to create a proxy year.\n\n\nShow mountain stage gap charts code\n# mountain winner to next best\nplot_dec_mtnb1 <-\ngaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n#  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Mountain\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\" , size = 8),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtnb2 <-\ngaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 420), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\" , size = 8),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtnb3 <-\ngaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 420), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\" , size = 8),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtnb1 / plot_dec_mtnb2 / plot_dec_mtnb3 +\n  plot_annotation(title = \"Gaps Between Winner & Next Best Times are Narrowing\",\n                  subtitle = \"Median gap on mountain stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nWhat does this chart tell us? As you look at it, keep in mind the y axis is different in the 1900s - 1930s chart because in the early years of the race the gaps were much wider.\nMost obviously, and not surprisingly, the gaps between winner and next best time shrank as the race professionalized and sports science got better. There are of course outliers here and there in the last few decades, but the course changes year-to-year, and some years the race organizers have made some years more difficult than other in the mountains.\nWe also see the effect of war. The two world wars not only interrupted the race in those years, but especially in the years immediately after WWII the gaps were larger than in the late 1930s. We can imagine what the war did to the pool of riders. The sport needed time to recover, for riders to train and get back to full fitness.\nOk, now let’s look at the changes in the mountains from the winners to the time for the last rider(s). The only change from the last set of charts is filter(compare_grp == \"Last\")\n\n\nShow mountain stage gap charts code\n# mountain winner to last\nplot_dec_mtla1 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n  #  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Last\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtla2 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 5400), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtla3 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 5400), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtla1 / plot_dec_mtla2 / plot_dec_mtla3  +\n  plot_annotation(title = \"Gaps Between Winner & Last Rider Times Mostly Stable Since 1950s\",\n                  subtitle = \"Median gap on mountain stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nWhat do we see here? Well first, notice that the gaps in the 1900s to 1930s were huge, especially before the 1930s. By the 1930s the gaps was usually around 30-40 minutes, similar to post-WWII years. But in the early years of the race, the last man in sometimes wouldn’t arrive until 10+ hours after the winner!\nBut since then the gaps are mostly around 30+ minutes. And again, I adjusted to include racers who finish outside of the time-stage cut off, and are thus eliminated from the race overall.\nOk, last two charts in this series…this time we’ll look at the flat & hilly stages. The only code changes are to the filters: filter(compare_grp == \"Next best\") or filter(compare_grp == \"Last\") and filter(stage_type == \"Flat / Plain / Hilly\").\n\n\nShow flat/hilly stage gap charts code\n# flat/hilly next best\nplot_dec_flnb1 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n  #  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Mountain\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flnb2 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 300), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flnb3 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 300), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 7),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flnb1 / plot_dec_flnb2 / plot_dec_flnb3 +\n  plot_annotation(title = \"Gaps Between Winner & Next Best Times Mostly < 1 Minute Since 1970s\",\n                  subtitle = \"Median gap on flat & hilly stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nPerhaps the most surprising thing in the Flat/Hilly stage gaps between winners & next best is that the gaps were similar to mountain stages. But then from watching the race all these years I remember that the climbers finish in groups fairly near to each other, even if the mountain stages are so hard.\nNo surprise of course that for many decades now the gaps have been around or under a minute. After the bunch sprints, the next group of riders, those not contesting the win, are right behind that pack.\n\n\nShow flat/hilly stage gap charts code\n### flat / hilly winner to last\nplot_dec_flla1 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n  #  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Mountain\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flla2 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 2340), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flla3 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 2340), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flla1 / plot_dec_flla2 / plot_dec_flla3 +\n  plot_annotation(title = \"Gaps Between Winner & Last Rider Times Very Tight by 1970s, Stabilized to ~ 10 min since\",\n                  subtitle = \"Median gap on flat & hilly stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nThe gap from winner to last was much less than winner-to-last in mountains, which isn’t a surprise. The sprinters tend to suffer in the Alps, Pyrenees and other mountain stages. As long as they come in under the time threshold, they are likely to be well behind on the day. But on flat stages, the only thing that keeps a rider more than a few minutes back is a spill, flat tire, or just having a bad day.\nNow it’s worth noting that I did not normalize for stage distance or elevation gain (for mountain stages) in terms of comparing year to year. I went with the assumption that since I was grouping multiple stages into a year, that even over time this would normalize itself. If this were a more serious analysis I’d do it.\nAnother extension of this analysis would be a model to predict time gaps. Then I’d include stage distance & gain, rider height/weight, and other factors.\nSome shout-outs are in order. First of course to the #tidytuesday crew. For the data here:\n* Alastair Rushworth and his tdf package\n* Thomas Camminady and his Le Tour dataset\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/Random music choice for the new routine/index.html",
    "href": "posts/Random music choice for the new routine/index.html",
    "title": "Random music for the new routine",
    "section": "",
    "text": "Yes, I have a lot of music"
  },
  {
    "objectID": "posts/random-music-choice-for-the-new-routine/index.html",
    "href": "posts/random-music-choice-for-the-new-routine/index.html",
    "title": "Random music for the new routine",
    "section": "",
    "text": "Yes, I have a lot of music.\n\n\n\n\n\nTo know exactly how much, a few years ago I painstakingly catalogued everything into a spreadsheet, and every time I get something new I add to the list. As of October 2023 I have over 2160 records and CDs…more than 900 vinyl albums (LP & EP), more than 1000 CD albums (LP & EP), 100+ 7” singles plus music in other formats.\nAnd yes I had it shipped from San Francisco to Lyon, then Lyon to Copenhagen. Related, my wife is a patient and understanding woman.\nWe know about the paradox of choice, right? Why you can’t choose something to watch from the thousands of things available on the streaming services to which you subscribe?\nYeah, same here.\nThis is a problem I want to solve even more now that I’ll be home more often, as my contract at CIEE has ended and I’m spending time re-skilling and upskilling my data knowledge…getting reacquainted with r, learning some Python and Tableau, and hopefully posting more regularly here, all while looking for my next job (hint-hint, if you need a data analyst/data scientist or know someone who does, let me know).\nI need things to listen to and sometimes I want to randomize the choice so I don’t spend 10 minutes dithering about it and choosing one of the same 20 records I listen to by default.\nSo anyway, what does a data nerd do? Write a quick script to randomize the choice. It’s fairly simple, and I have a couple of ground rules:\n\nWhat comes up must be played. If that means Hüsker Dü Land Speed Record at 8:30am, so be it. If that means Wilco’s Sky Blue Sky in the afternoon when I need a pick-me-up, so be it.\nOnce played, it can’t be played again via the randomizer. This of course doesn’t preclude playing it when I want to hear it.\n\nSo how does it work? Well, let’s see the code…it’s not genius-level but it does the trick.\nFirst we load the packages….\n\n# load packages\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\nNow we load the music library…\n\n# read in music library\nmusiclib &lt;- readxl::read_excel(\"~/Documents/music catalogue.xlsx\") %&gt;%\n    clean_names() %&gt;%\n    rename(format = format_vinyl_cd, type = type_lp_ep_7_single_12)\n#&gt; rename: renamed 2 variables (type, format)\nglimpse(musiclib)\n#&gt; Rows: 2,166\n#&gt; Columns: 11\n#&gt; $ artist        &lt;chr&gt; \"'Til Tuesday\", \"[The] Caseworker\", \"10,000 Maniacs\", \"1…\n#&gt; $ title         &lt;chr&gt; \"Welcome Home\", \"These Weeks Should Be Remembered\", \"Bli…\n#&gt; $ type          &lt;chr&gt; \"LP\", \"LP\", \"LP\", \"LP\", \"LP\", \"LP\", \"LP\", \"LP\", \"LP\", \"L…\n#&gt; $ format        &lt;chr&gt; \"CD\", \"CD\", \"CD\", \"CD\", \"CD\", \"CD\", \"vinyl\", \"vinyl\", \"C…\n#&gt; $ year_issue    &lt;dbl&gt; 1986, 2003, 1989, 1987, 1993, 1995, 2019, 1985, 1992, 19…\n#&gt; $ year_original &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ label_1       &lt;chr&gt; \"Epic Records\", \"Manifesto Records\", \"Elektra Records\", …\n#&gt; $ label_2       &lt;chr&gt; NA, NA, NA, NA, NA, \"Fifty Seven Records\", NA, NA, \"Poly…\n#&gt; $ label_3       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ notes         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ added         &lt;dttm&gt; 2020-12-14, 2020-05-25, 2020-12-14, 2020-12-14, 2020-12…\n\n…and here we load the exclusion list (it will get appended to later)\n\n# input played list\nmusicplayed &lt;- readxl::read_excel(\"~/Documents/musicplayed.xlsx\") %&gt;%\n    clean_names()\n#glimpse(musicplayed)\n\nThen we merge with the library, filter out what’s been played, and get something new to listen to. In this case I’m filtering to just get full LPs or EPs (no singles) and on vinyl or CD (I have some rando flexidiscs).\nAfter which I create a new df to output and append to the played list and overwrite the played list.\n\n# merge with played file,\n# exclude played\n# if desired, filter on format (vinyl, CD, either) and type (LP, EP, etc)\n# output new music to play\n# select variables for output to played file\nplaywhat &lt;- musiclib %&gt;%\n    merge(musicplayed, by = c(\"artist\", \"title\", \"format\", \"type\"), all = T) %&gt;%\n    mutate(played = ifelse(is.na(played), 0, played)) %&gt;%\n    filter(played == 0) %&gt;%\n    filter(format %in% c(\"vinyl\", \"CD\")) %&gt;%  \n    filter(type %in% c(\"LP\", \"EP\")) %&gt;%\n    sample_n(1) %&gt;%\n    select(artist, title, format, type) %&gt;%\n    mutate(date_played = Sys.Date()) %&gt;%\n    mutate(played = 1)\n\nview(playwhat)\n\n# add new music played to what has been played\nplayed2 &lt;- playwhat %&gt;%\n    rbind(musicplayed)\n\n## output that to excel file\nwritexl::write_xlsx(played2, \"~/Documents/musicplayed.xlsx\")\n\nBy some cosmic coincidence, I ran the program this morning to get something to listen to while writing this post, and what came up was Searching for Ray by Copenhagen band El Ray. And I bought this album back in 2018 at Rte 66, an amazing store here in Copenhagen.\nIt turned out to be the perfect music to listen to while getting this post together (and debugging some weird glitch where quarto couldn’t locate r)…I listened to it twice.\n\n\n\n\n\nThey play fun surf-rock…check them out.\nSo that’s it, the data nerd’s way to get around the paradox of choice and work through the tons of music I have on hand."
  },
  {
    "objectID": "posts/exploring-happiness/index.html",
    "href": "posts/exploring-happiness/index.html",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "",
    "text": "What makes us happy?\nA subjective question to be sure. The things that make me happy might not do much for you, and what brings you happiness might not work for me. To each their own? Or are there some basic things that boost our collective happiness, things most of us agree are good.\nThere have been attempts to answer the question in a universal way, by focusing on broad quality of life measures and activities. A perfect question for social scientists to dig into, and so they have.\nAmong the most referenced measures is the World Happiness Report. A yearly multi-chapter report published by Sustainable Development Solutions Network, using responses from the Gallup World Poll.\nFor this post (and at least one, maybe more to come) I want to dig into the data that has been made available. Every year the WHR research team releases data for chapter two, which has composite scores by country based on the ladder score question and some others. They add logged GDP and other data in the full chapter report. GDP is made available in the chapter data for release.\nThe Data  The Chapter 2 data has been consistently offered for download for years now. There are two datasets:\n\nData for Figure 1 includes the three three-year rolling average of the happiness ladder question (a 0-10 scale, described in the statistical appendix) along with related measures, aggregated by country. We also get the ladder score of a hypothetical dystopian country.\nData for Table 1 has the output of the OLS regression model to predict each country’s ladder score.\n\nThe Figure 1 data also includes OLS output in form of the percent of each country’s happiness score that could be attributed to the component variables. Another column in the Figure 1 set includes a column with the dystopia score plus the country’s residual of the actual and predicted ladder scores. In the data loading code below you’ll that added a column separating out the residual.\nBoth the report’s statisitcal appendix (downloads a pdf) and on-line version of Chapter 2 explain everything in more detail so I won’t repeat it here.\nWorking with the data  I’ll be using r for all aspects of the work; importing the data, cleaning, analysing, and visualising. So let’s go…\nFor this post, I want to focus on Exploratory Data Analysis (EDA). It’s the part of the analytical process where you get a broad overview of the data…look for things that need cleaning, look for distributions and relationships. In the past I’d build my own charts and tables, and that took quite a lot of time and mental energy.\nThankfully there are packages to speed up the work. So to get a quick look at this first set of WHR data, I’ll test-drive the EDA packages DataExplorer by Boxuan Cui, and Roland Krasser’s explorer.\nTo start with let’s load packages to import and clean the data. These are the three packages I use for almost every analysis in r.\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\nTo get the WHR data into RStudio you can go two ways. First is to download the sheet to your local machine and read in:\n\n# read in WHR data, fix country names for later merges\nwhr23_fig2_1a &lt;- readxl::read_excel(\"yourfilepath/DataForFigure2.1WHR2023.xls\") %&gt;%\n    clean_names() %&gt;%\n    as_tibble() %&gt;%\n    mutate(residual = dystopia_residual - ladder_score_in_dystopia) %&gt;%\n    select(-residual_is_dystopia_minus_dystopia_plus_residual) %&gt;%\n    mutate(whr_year = 2023) %&gt;%\n    mutate(country_name = case_when(\n        country_name == \"Czechia\" ~ \"Czech Republic\",\n        country_name == \"State of Palestine\" ~ \"Palestinian Territories\",\n        country_name ==  \"Turkiye\" ~ \"Turkey\",\n        TRUE ~ country_name))\n\nYou could also use curl to download straight from the WHR page:\n\nlibrary(readxl)\nurl1 &lt;- \"https://happiness-report.s3.amazonaws.com/2023/DataForFigure2.1WHR2023.xls\"\ndestfile1 &lt;- \"DataForFigure2_1WHR2023.xls\"\ncurl::curl_download(url1, destfile1)\nwhr23_fig2_1a &lt;- readxl::read_excel(destfile1) \n## %&gt;% (and then the same cleaning steps shown above)\n\nThe data from the WHR does not include the world region for each country, something I will want for further analysis. I’m not sure what the source is for the region grouping they are using. I found a file with a region column on Kaggle for the 2021 survey, so downloaded that and merged on country name.\n\n# read in kaggle file with region names\nctreg &lt;- readr::read_csv(\"yourfilepath/world-happiness-report-2021.csv\") %&gt;%\n    as_tibble() %&gt;%\n    clean_names() %&gt;%\n    select (country_name, region = regional_indicator)\n\n# join to whr23 on country, add missing region for Congo (Kinshasa)\nwhr23_fig2_1 &lt;- whr23_fig2_1a %&gt;%\n    merge(ctreg, all = TRUE) %&gt;%\n    as_tibble() %&gt;%\n    select(country_name, region, whr_year, everything()) %&gt;%\n    mutate(region = ifelse(\n        country_name == \"Congo (Kinshasa)\", \"Sub-Saharan Africa\", region))\n\nAnother way to do it is to hard code them. I had to go back to the 2018 report to find a list.\nRegardless, we now have a dataset, so let’s explore it.\nThe DataExplorer package  Let’s start with DataExplorer. The create_report() function runs the full set of native reports and outputs to a directory of your choosing with a filename of your choosing. But for a review I want to go through a few of the individual report elements.\nintroduce() outputs a table showing rows, columns and other information. If you want to see this information in chart form, plot_intro() and plot_missing() do that.\n\n## DataExplorer for EDA\nlibrary(DataExplorer) # EDA tools\n\n# summary of completes, missings\nintroduce(whr23_fig2_1)\n#&gt; # A tibble: 1 × 9\n#&gt;    rows columns discrete_columns continuous_columns all_missing_columns\n#&gt;   &lt;int&gt;   &lt;int&gt;            &lt;int&gt;              &lt;int&gt;               &lt;int&gt;\n#&gt; 1   150      22                2                 20                   0\n#&gt; # ℹ 4 more variables: total_missing_values &lt;int&gt;, complete_rows &lt;int&gt;,\n#&gt; #   total_observations &lt;int&gt;, memory_usage &lt;dbl&gt;\nplot_intro(whr23_fig2_1)\n\n\n\nplot_missing(whr23_fig2_1)\n\n\n\n\nThere are hardly any missing values in the set, which will make analysis easier.\nOk, so how about the distribution of our variables? plot_bar() takes all discrete variables and plot_histogram() runs for the continuous variables. Depending on how many columns of each type your dataset has you will need to play with the nrow and ncol options so that everything renders to the RStudio plot column. For the histograms you can change the number of binsm change the x-axis to log or some other option (the default is continuous). You can also customize the look a bit with passing arguments to the ggtheme = and theme_config() functions.\n\nplot_bar(whr23_fig2_1)\n\n\n\n\nFor the discrete variable bar charts, for this dataset there isn’t much to look at. But for a dataset with demographic varaibles, geographic places, etc. this would be very helpful.\n\nplot_histogram(whr23_fig2_1, nrow = 5L)\n\n\n\n\nThe histograms render in alpha order of the variable name, not order in the dataset. When the plots render, we look through them to see if there are any unusual skews or other things we want to watch out for depending on the type of analyses to be run. In this case there are a few solitary bars in some of the histograms, like values above 0.4 in the generosity column. But nothing too skewed so that if we took a closer look at distributions by way of means or interquartiles that we’d be too worried.\nNow for my favorite part of this package, a correlation matrix! We’ll run plot_correlation() without the year, dystopia ladder score, and whiskers and make sure to only do continuous. We can also adjust things like the type of correlation (default is pearson), the size of the coefficient labels in the chart and other elements.\n\n## correlation...remove some columns, clean NA in pipe, continuous only, change text size\nwhr23_fig2_1 %&gt;%\n    select(-whr_year, -ladder_score_in_dystopia, -upperwhisker, -lowerwhisker) %&gt;%\n    filter(!is.na(residual)) %&gt;%\n    plot_correlation(type = \"continuous\", geom_text_args = list(\"size\" = 3))\n\n\n\n\nThe way my brain works is to look for visual patterns and relationships. So a correlation matrix like this is perfect to give me a broad view of how the continuous variables relate to each other. The matrix heatmap returns positive relationships in red, negative in blue. I first want to look at the relationships of the component variables to the ladder score, and we see positive associations for everything except for perception of corruption, which makes sense because you’d likely report being less happy if you lived in a corrupt country.\nThe weakest association is between generosity, which comes from a question asking “Have you donated to charity in the past month?” So while donations to charity are a good thing, they don’t necessarily move the needle on happiness. At least not in the aggregate. But maybe by country or region? Something to take a look at later. This is why we do EDA…\nWe also see that we could have run this without the “explained by…” columns as they have the same coefficients as the component variables.\nAs much as I love a correlation matrix, I love scatterplots even more. I clearly have a thing for patterns and relationships. The plot_scatterplot function returns plots for all the variables you pass along, against the one you call in the by = argument. Here we want to see the association between the ladder score and component variables from Chapter 2.\n\nplot_scatterplot(\n    whr23_fig2_1 %&gt;% select(ladder_score, social_support:perceptions_of_corruption, dystopia_residual, residual), \n    by = \"ladder_score\", nrow = 3L)\n\n\n\n\nWe know from the correlation heatmap that we don’t need the “explained_by_*” variables as they were redundant to the component variables. The x/y distributions here confirm what we saw in the correlations, including the slightly negative relationship between the ladder score and perceptions of corruption, and that generosity was a weaker relationship.\nWhile the scatterplot function does allow for some plot customization, one I tried but couldn’t get to work was using the geom_point_args() call to color the dots by region, like this using ggplot:\n\nwhr23_fig2_1 %&gt;%\n    ggplot(aes(x = perceptions_of_corruption, y = ladder_score)) +\n    geom_point(aes(color = region)) +\n    theme(legend.position = \"bottom\")\n\n\n\n\nThere are a few other functions offered to do principal component analysis and qq (quantile-quantile) plots, but they did not help much with this dataset.\nOverall there are plenty of helpful features in DataExplorer that make it worthwhile to use for EDA. I’d like the ability to color scatterplots by a discrete variable, or to facet the histograms or scatterplots, but as is, a robust EDA tool.\nThe explore package  The best function here is explore(dataset), which launches a shiny window with four tabs.\nThe “overview” tab shown here, displays a table with mean, min, max, and unique & missing value counts by variable.\n\n\n\n\n\nThe “variable” tab allows you to explore variables on their own… \n…or in relation to one another.  You not only get a chart appropriate to the variable type (categoricals with bars, continuous with area plots), but when you target against another variable you get a scatterplot.\nThe explain tab runs a decision tree against a target variable, and the data tab displays the entire dataset as a table, all rows and all columns. So before launching this you may want to be mindful of running it against too large a dataset.\nIf you don’t want to launch the shiny app, you can output a report in html…\n\n## creates html report of all individual reports\nwhr23_fig2_1 %&gt;%\n    report(output_dir = \"~/data/World Happiness Report\")\n\n…or run select features individually, depending on what you need….\n\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, social_support:perceptions_of_corruption,\n                 explained_by_log_gdp_per_capita:residual) %&gt;%\n    describe_all() \n#&gt; select: dropped 8 variables (country_name, region, whr_year, standard_error_of_ladder_score, upperwhisker, …)\n#&gt; # A tibble: 14 × 8\n#&gt;    variable                          type     na na_pct unique   min  mean   max\n#&gt;    &lt;chr&gt;                             &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 ladder_score                      dbl      13    8.7    138  1.86  5.54  7.8 \n#&gt;  2 social_support                    dbl      13    8.7    138  0.34  0.8   0.98\n#&gt;  3 healthy_life_expectancy           dbl      14    9.3    137 51.5  65.0  77.3 \n#&gt;  4 freedom_to_make_life_choices      dbl      13    8.7    138  0.38  0.79  0.96\n#&gt;  5 generosity                        dbl      13    8.7    138 -0.25  0.02  0.53\n#&gt;  6 perceptions_of_corruption         dbl      13    8.7    138  0.15  0.73  0.93\n#&gt;  7 explained_by_log_gdp_per_capita   dbl      13    8.7    138  0     1.41  2.2 \n#&gt;  8 explained_by_social_support       dbl      13    8.7    138  0     1.16  1.62\n#&gt;  9 explained_by_healthy_life_expect… dbl      14    9.3    137  0     0.37  0.7 \n#&gt; 10 explained_by_freedom_to_make_lif… dbl      13    8.7    138  0     0.54  0.77\n#&gt; 11 explained_by_generosity           dbl      13    8.7    138  0     0.15  0.42\n#&gt; 12 explained_by_perceptions_of_corr… dbl      13    8.7    138  0     0.15  0.56\n#&gt; 13 dystopia_residual                 dbl      14    9.3    137 -0.11  1.78  2.95\n#&gt; 14 residual                          dbl      14    9.3    137 -1.89  0     1.18\n\n\nwhr23_fig2_1 %&gt;%\n    explore(ladder_score)\n\n\n\n\nThe main vignette and reference guide is very robust, so no need to repeat too much here. But there are some fun features like decision trees, and lots of flexibilty to explore multiple variables in relation to each other.\nThe skimr package  Then there is skimr, one of the first EDA packages that I remember seeing. If there’s a feature I like most, it’s the basic skim() function, which returns means & other distributions, as well as little histograms.\n\n\n\nlibrary(skimr)\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, social_support:perceptions_of_corruption, residual) %&gt;%\n    skim()\n#&gt; select: dropped 15 variables (country_name, region, whr_year, standard_error_of_ladder_score, upperwhisker, …)\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n150\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nladder_score\n13\n0.91\n5.54\n1.14\n1.86\n4.72\n5.68\n6.33\n7.80\n▁▂▆▇▃\n\n\nsocial_support\n13\n0.91\n0.80\n0.13\n0.34\n0.72\n0.83\n0.90\n0.98\n▁▂▃▆▇\n\n\nhealthy_life_expectancy\n14\n0.91\n64.97\n5.75\n51.53\n60.65\n65.84\n69.41\n77.28\n▃▃▇▇▂\n\n\nfreedom_to_make_life_choices\n13\n0.91\n0.79\n0.11\n0.38\n0.72\n0.80\n0.87\n0.96\n▁▁▃▇▇\n\n\ngenerosity\n13\n0.91\n0.02\n0.14\n-0.25\n-0.07\n0.00\n0.12\n0.53\n▃▇▅▁▁\n\n\nperceptions_of_corruption\n13\n0.91\n0.73\n0.18\n0.15\n0.67\n0.77\n0.85\n0.93\n▁▁▁▅▇\n\n\nresidual\n14\n0.91\n0.00\n0.50\n-1.89\n-0.22\n0.07\n0.30\n1.18\n▁▂▅▇▂\n\n\n\n\n\n\n\nIt’s especially helpful on small and medium-sized datasets, to get a quick overview and look for outliers.\nHappiness data takeaways  Using these packages for EDA on the happiness data, we learned that:\n\nThere are not many missing values in the data.\nAll of the component variables except generosity have strong positive correlations with the happiness score.\nPerception of corruption has, as expected, a negative association with happiness.\n\nWe also came away wanting to know a bit more about differences by region, so that’s a good starting point for the next post, which will be a slightly deeper dive into the data.\nConclusion  There is no one perfect EDA package that suits all needs for any dataset. DataExplorer has some robust features, particularly in this usecase the correlation heatmap and the scatterplots. I loved the native reports and shiny app in explorer. I had planned to look at correlationfunnel, but it’s only really suited to a use-case with binary outcomes such as customer sign-up, churn, employee retention, college admissions outcomes (admits, yield), student success outcomes like retention and graduation. I’ll have to find another dataset to try that package. Doing these package test-drives reminded me that skimr is also very useful.\nGoing forward I’ll be setting up a more deliberate EDA workflow using parts of each of these packages, depending on the size of the dataset and the main questions I’d have of the data."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html",
    "href": "posts/exploring-happiness-eda/index.html",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "",
    "text": "Our cat Dalias (aka Potato), in a happy moment"
  },
  {
    "objectID": "posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/index.html#using-the-spotify-api-and-spotifyr-package-to-visualize-some-music-ive-made",
    "href": "posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/index.html#using-the-spotify-api-and-spotifyr-package-to-visualize-some-music-ive-made",
    "title": "Sad Songs & Pretty Charts - a Gosta Berling Music Data Visualization",
    "section": "",
    "text": "For this post, I thought I’d focus on music analytics, given that music and data science/analysis are two things I’ve spent most of my waking hours doing for a number of years now.\nOver the years I’ve made a lot of music in a number of different projects. For most of my time living in the Bay Area I’ve played with some friends in a band called Gosta Berling. We’ve released two EPs and a full album (click on the album covers to give listen)\n  \nOur sound could be called melancholy mood-pop. We like melody, but we were raised on brooding post-punk so a minor key vibe is definitely present. The Spotify API has musical features including danceability, energy, and valence (what they call ‘happiness’). I used Charlie Thompson’s spotifyr package to see how we score. spotifyr has a bunch of functions designed to make it easier to navigate Spotify’s JSON data structure.\nOne quick thing…I’m using Spotify data so in effect validating Spotify. While I appreciate the availability of the data for projects like this, Spotify needs to do much better by way of paying artists. We don’t have tons of streams, but as you can see from this account report… \n…artists get f$ck-all per stream. So sure, use Spotify, it’s a great tool for discovering new music. And while artists pressure them to pay more per stream, you can help by purchasing music from artists you like. The pandemic has killed touring income, so sales are all that many artists have to support themselves. Help them out, buy the music you like. Especially if they’re on Bandcamp and you buy 1st Fridays, when Bandcamp waives their revenue share, meaning the artist gets every penny. Did I mention you can buy our music on Bandcamp? :)\nAnyway, soapbox off…first thing, let’s load the packages we’ll be using:\n\n# load packages\nlibrary(spotifyr) # pull data from spotify\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot text\n\nlibrary(httr)\nlibrary(stringr) # work with string data\nlibrary(lubridate) # work with dates\nlibrary(GGally) # correlation plots\nlibrary(PerformanceAnalytics) # correlation plots\nlibrary(corrr)  # correlation plots\n\nTo get access the Spotify data, you need a developer key. Charlie’s explained how to do it on the package page, so I won’t repeat that here. To set up the keys in your .Renviron, run usethis::edit_r_environ() and add (where the xs are your codes):\n\nSPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx'\nSPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx'\n\n# or do\nSys.setenv(SPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx')\n\nThis call sets your access token for the data session\nIf you run into redirect issues, see this stackoverflow thread, specifically this comment\nFirst thing is to search the artist data for audio features. I’m pulling in everything into a dataframe. Now initially I had a filter for artist = 'Gosta Berling'. But that was also pulling in data from a Swedish prog-metal band Gösta Berling’s Saga. So I needed to filter on our artist ID and pull in albums and singles (for some reason our EPs were listed as singles, but whatever)\n\n# gets full range of information for tracks from artist\ngosta_audio1 &lt;- get_artist_audio_features(artist = \"4Vb2yqJJthJTAZxKz4Aryn\", include_groups = c(\"album\", \"single\"))\n\nOh…why more than one band with Gosta Berling in their name? Well, The Saga of Gösta Berling was Greta Garbo’s first feature-length film, and based on an old Swedish novel Gösta Berling’s Saga about a drunkard outcast priest seeking redemption. When, like our band, you’re a bunch of movie nerds, and a couple of you are especially obsessed with silent films & old Hollywood, you name your band Gosta Berling. And so does a Swedish band…anyway…more about the data.\nThe code here gets a dataframe for each record. I also needed to add album titles. Next steps were to merge the album dataframes together, extract the song IDs and pass them to the get_track_features() function as a list.\n\n# get album tracks, add album name could merge on other df, easier to quick fix this way\ntravel &lt;- get_album_tracks(id = \"0vBs7ZtBj3ROrRyac3M47q\")\ntravel$album &lt;- \"Travel\"\nsweetheart &lt;- get_album_tracks(id = \"0dJBaJ3VFxOtdG5L9yzALJ\")\nsweetheart$album &lt;- \"Everybody's Sweetheart\"\nwinterland  &lt;- get_album_tracks(id = \"6CMekiY6lCIuBZpzFDInpf\")\nwinterland$album &lt;- \"Winterland\"\n\n# merge album files, output track ids to use for audio features\ngbtracks &lt;- data.table::rbindlist(list(sweetheart, travel, winterland))\n#copy result from console to paste as vector below\ngbtrackids &lt;- dput(as.character(gbtracks$id)) \n\ngosta_audio2 &lt;- \n  get_track_audio_features(c(\"2SotrXjkvjTZf05XSMKGyp\", \"07cTJ65GZ4Lvr6b1CtgPll\", \"4ooz79IN3la97See8IMNRL\", \"7pgCh68iFO0LNUNKWTFFIP\", \"4ZCesDRgGWKEXwq8iKw5FB\", \"4ZdH5B3tijHjWiwyOErgtf\", \"5GWKeBYgOsv3PKutDIQoet\", \"0XXWRsY6URe2Vx7Bxs6k06\", \"0t3AGVXHyF3dEYuhvAYuNz\", \"4ObsuwrVLKUq5aF8whrFqk\", \"0PnjWfIPwsqBtllMILjzxB\", \n\"7uQtlGsKxXOzsSapKTZRFU\", \"3kQuG44stzA3pQf7g61Ipt\", \n\"0YH9wkimhRhCmstNZyxPgO\", \"7rEbjyNO0dTEK6x8HkLqAz\", \"4VgEAtVQtkwIHzKMOROk6X\", \"5R9M4s6QZljNPVVzxoy98h\", \"1FNtHQ0juoKg2yCf9u4VSg\", \"5NWmfmupE7FEJ9O1e9vizu\"),\nauthorization = get_spotify_access_token())\n\nThis gets a dataframe with most of what I want…just a few tweaks needed. First, since they weren’t pulled from the get_track_audio_features() call, I used the track id, name, and album track number from the gbtracks dataframe. Also, because the song key returned as only the numeric value, I created the letter name and mode (major or minor), and ordered the columns.\n\n# get track number and name, merge from gbtracks -\n# need b/c these fields not returned from get_track_audio_features()\ngbtrack2 &lt;- gbtracks %&gt;%\n  select(id, name, album, track_number) %&gt;%\n  rename(track_name = name)\n\n# merge to complete df. add names for key and mode\ngosta_audio &lt;- left_join(gosta_audio2, gbtrack2) %&gt;%\n  mutate(key_name = case_when(key == 0 ~ \"C\", key == 2 ~ \"D\", key == 4 ~ \"E\", key == 5 ~ \"F\",\n                              key == 7 ~ \"G\", key == 9 ~ \"A\", key == 11 ~ \"B\")) %&gt;%\n  mutate(mode_name = case_when(mode == 0 ~ \"Minor\", mode == 1 ~ \"Major\")) %&gt;%\n  mutate(key_mode = paste(key_name, mode_name, sep = \" \")) %&gt;%\n  rename(track_id = id) %&gt;%\n  select(album, track_name, track_number, key_mode, time_signature, duration_ms, \n         danceability, energy, loudness, tempo, valence, \n         acousticness, instrumentalness, liveness, speechiness,\n         key_name, mode_name, key, mode)\n\nOk, we’ve got a nice tidy dataframe, let’s do some analysis & visualization!\nSpotify’s developer pages have good explanations of the data. Some notes from spotify here about elements:\n\nMost of the audio features are 0-1, 1 being highest. e.g. higher speechiness = higher ratio of words::music. Valence is “happiness”, where higher = happier.\nLoundess in dB, tempo is BPM.\n\nSo let’s look at a quick summary of the audio features for our songs.\n\n#&gt;   duration_ms      danceability        energy          loudness      \n#&gt;  Min.   : 75933   Min.   :0.2500   Min.   :0.0476   Min.   :-21.350  \n#&gt;  1st Qu.:295380   1st Qu.:0.3545   1st Qu.:0.3260   1st Qu.:-12.031  \n#&gt;  Median :350053   Median :0.3920   Median :0.5190   Median : -9.943  \n#&gt;  Mean   :334762   Mean   :0.4105   Mean   :0.5233   Mean   :-10.705  \n#&gt;  3rd Qu.:385634   3rd Qu.:0.4820   3rd Qu.:0.7160   3rd Qu.: -7.537  \n#&gt;  Max.   :522760   Max.   :0.5730   Max.   :0.9360   Max.   : -6.014  \n#&gt;      tempo           valence        acousticness     instrumentalness \n#&gt;  Min.   : 82.15   Min.   :0.0349   Min.   :0.00371   Min.   :0.00881  \n#&gt;  1st Qu.:116.51   1st Qu.:0.1620   1st Qu.:0.12920   1st Qu.:0.50800  \n#&gt;  Median :141.83   Median :0.2940   Median :0.39300   Median :0.69800  \n#&gt;  Mean   :131.06   Mean   :0.3105   Mean   :0.41332   Mean   :0.62883  \n#&gt;  3rd Qu.:149.98   3rd Qu.:0.4405   3rd Qu.:0.63750   3rd Qu.:0.84450  \n#&gt;  Max.   :166.01   Max.   :0.6960   Max.   :0.88600   Max.   :0.94400  \n#&gt;     liveness       speechiness     \n#&gt;  Min.   :0.0703   Min.   :0.02540  \n#&gt;  1st Qu.:0.1020   1st Qu.:0.02810  \n#&gt;  Median :0.1160   Median :0.03060  \n#&gt;  Mean   :0.1333   Mean   :0.03699  \n#&gt;  3rd Qu.:0.1265   3rd Qu.:0.03865  \n#&gt;  Max.   :0.3300   Max.   :0.11600\n\nFirst I wanted to look at basic correlations for the values. There are a number of ways to run and visualize correlations in r…a few examples follow. First thing I needed to do was a subset of the gosta_audio df for easier calls with the various correlation packages.\nLet’s try correlations in base r. You get the coefficients in the console or you can output to a dataframe to hard-code the visualization.\n\ncor(gbcorr)\n#&gt;                  duration_ms danceability      energy    loudness      tempo\n#&gt; duration_ms       1.00000000   0.03575546 -0.09957649  0.16485951 -0.1589364\n#&gt; danceability      0.03575546   1.00000000 -0.10466026  0.09671649 -0.2719148\n#&gt; energy           -0.09957649  -0.10466026  1.00000000  0.85748849  0.5140085\n#&gt; loudness          0.16485951   0.09671649  0.85748849  1.00000000  0.4952005\n#&gt; tempo            -0.15893636  -0.27191484  0.51400852  0.49520052  1.0000000\n#&gt; valence          -0.04414383  -0.10232090  0.72025346  0.48053791  0.5519247\n#&gt; acousticness     -0.19009855   0.11222116 -0.74742026 -0.65043898 -0.3612391\n#&gt; instrumentalness  0.12784620   0.06977532 -0.53088295 -0.49709651 -0.4411810\n#&gt; liveness         -0.30987073  -0.25213421  0.49374017  0.30054882  0.5316901\n#&gt; speechiness      -0.30678610  -0.31639826  0.45449667  0.27298422  0.4217976\n#&gt;                      valence acousticness instrumentalness   liveness\n#&gt; duration_ms      -0.04414383   -0.1900986       0.12784620 -0.3098707\n#&gt; danceability     -0.10232090    0.1122212       0.06977532 -0.2521342\n#&gt; energy            0.72025346   -0.7474203      -0.53088295  0.4937402\n#&gt; loudness          0.48053791   -0.6504390      -0.49709651  0.3005488\n#&gt; tempo             0.55192475   -0.3612391      -0.44118097  0.5316901\n#&gt; valence           1.00000000   -0.7793878      -0.29646550  0.4743309\n#&gt; acousticness     -0.77938779    1.0000000       0.39266796 -0.3261889\n#&gt; instrumentalness -0.29646550    0.3926680       1.00000000 -0.3406087\n#&gt; liveness          0.47433091   -0.3261889      -0.34060869  1.0000000\n#&gt; speechiness       0.41684028   -0.3150009      -0.56643572  0.7459700\n#&gt;                  speechiness\n#&gt; duration_ms       -0.3067861\n#&gt; danceability      -0.3163983\n#&gt; energy             0.4544967\n#&gt; loudness           0.2729842\n#&gt; tempo              0.4217976\n#&gt; valence            0.4168403\n#&gt; acousticness      -0.3150009\n#&gt; instrumentalness  -0.5664357\n#&gt; liveness           0.7459700\n#&gt; speechiness        1.0000000\ngbcorrs1 &lt;- as.data.frame(cor(gbcorr))\n\nOr you could let some packages do the viz work for you. First, the GGally package, which returns a nice matrix visualization that shows which fields are most postively and negatively correlated.\n\nggcorr(gbcorr, label = TRUE)\n\n\n\n\n\n\n\n\nWe see here some strong postive associations with energy::loundess returning a .9 coefficient, and liveness::speechiness and energy::valence each returning at .7 coefficient. The energy::acousticness and loudness::acousticness combinations each return a -.7 coefficient, showing a negative relationship between those music features.\nWith the corrr package I tried a couple of approaches. First a basic matrix that prints to the console, and doesn’t look much different than base r.\n\ngbcorr %&gt;%\n  correlate(use = \"pairwise.complete.obs\", method = \"spearman\")\n#&gt; # A tibble: 10 × 11\n#&gt;    term     duration_ms danceability energy loudness  tempo valence acousticness\n#&gt;    &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 duratio…     NA            0.0799 -0.319   -0.189 -0.439  -0.191      -0.0737\n#&gt;  2 danceab…      0.0799      NA      -0.269   -0.124 -0.323  -0.209       0.128 \n#&gt;  3 energy       -0.319       -0.269  NA        0.872  0.658   0.761      -0.725 \n#&gt;  4 loudness     -0.189       -0.124   0.872   NA      0.574   0.458      -0.595 \n#&gt;  5 tempo        -0.439       -0.323   0.658    0.574 NA       0.665      -0.479 \n#&gt;  6 valence      -0.191       -0.209   0.761    0.458  0.665  NA          -0.770 \n#&gt;  7 acousti…     -0.0737       0.128  -0.725   -0.595 -0.479  -0.770      NA     \n#&gt;  8 instrum…      0.135        0.0333 -0.447   -0.586 -0.416  -0.177       0.339 \n#&gt;  9 liveness     -0.319       -0.321   0.319    0.144  0.479   0.488      -0.103 \n#&gt; 10 speechi…     -0.331       -0.715   0.382    0.283  0.640   0.396      -0.209 \n#&gt; # ℹ 3 more variables: instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, speechiness &lt;dbl&gt;\n\nNext, I used their rplot call and then rendered a network graph using the network_plot() call.\n\ngbcorrs2 &lt;- correlate(gbcorr)\nrplot(gbcorrs2)\n\n\n\n\n\n\n\n   # network graph\ncorrelate(gbcorr) %&gt;% \n  network_plot(min_cor=0.5)\n\n\n\n\n\n\n\n\nAnd finally the `performance analytics’ package, which was the first of the packages to include significance levels in the default output.\n\n\n\n\n\n\n\n\n\nGiven the correlations, I was interested in exploring the relationships a bit more. So I ran a few scatterplots, with song titles as data labels, and dots colored by album name (using primary color from the cover) to see also if any of the albums clustered at all along either axis. The ggrepel package is used to move the labels off of the dots.\nThere is a bit of a relationship between the Energy score and Valence - so our more energetic songs are our happiest songs. Another interesting way to explore this would be to do some sentiment analysis on the lyics and see if there’s a relationship between energy, valence and using words considered to be more positive in nature. That’s a project on my to-do list.\n\ngosta_audio %&gt;%\n  ggplot(aes(energy, valence, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) +\n  ylim(0, 1) +\n  xlim(0, 1) +\n  theme_minimal() +\n  labs(x = \"energy\", y = \"valence (happiness)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n\nNext I wondered if there’s a relationship between song tempo (beats per minute) & happiness. Our average BPM is 131, which isn’t too far the the mid-range of songs on Spotify. The histogram below used to be on the Spotify API page but they don’t seem to have it up anywhere anymore, so found it via the Wayback Machine\nSo let’s see the resulting scatterplot…\n\n\nshow tempo x valence scatterpplot code\ngosta_audio %&gt;%\n  ggplot(aes(tempo, valence, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) + \n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"tempo (bpm)\", y = \"valence (happiness)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nIt’s not until we get to about the 130 BPM range is it that our songs start to get to even a .25 valence (happiness) score, and from there the relationship between tempo & happiness really kicks in.\nFinally, tempo and energy…\n\n\nshow tempo x energy scatterpplot code\ngosta_audio %&gt;%\n  ggplot(aes(tempo, energy, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) +\n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"tempo (bpm)\", y = \"energy\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nSo yes, most of our songs are in the bottom half of the happy scale. And there does seem to be a bit of a relationship between tempo, energy and happiness and of course a relationship between tempo and energy. Going forward, I’d love to explore our song lyrics via text analysis, especially sentiment analysis to see if the songs Spotify classified as our most sad (low valence) had lyrics that were less positive.\nSo if you like slightly melancholy mood-pop that’s in the 130 +/- BPM range (think The National, Radiohead), I think you’ll like us.\nThanks for reading. And again, give us a listen, and maybe buy some music if you like. :)\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/exploring-happiness-analysis/index.html",
    "href": "posts/exploring-happiness-analysis/index.html",
    "title": "Exploring Happiness - Analysis",
    "section": "",
    "text": "Coming soon…for now, enjoy this picture of a happy cat on a balcony\n\n\n\nOur cat Leo, happy in the sun\n\n\nNow that we’re done with basic exploratory analysis on the World Happiness Report\nPredicting the happiness score  Ok, let’s run a couple of models to predict the happiness score and plot the results.\nBut why, you might ask, since the WHR authors already did that? Well, I’ve never really run regression models in r. I did in SAS for one job, and I haven’t had to since then. So join me as I walk through running a model and plotting the results."
  },
  {
    "objectID": "posts/call-me-by-my-name/index.html",
    "href": "posts/call-me-by-my-name/index.html",
    "title": "Call Me By My Name",
    "section": "",
    "text": "The actor Ryan O’Neal died on December 8, 2023. If you’re of a certain age and a film fan you might know him best from a pretty good run in the 1970s of very diverse films from Love Story, to the Peter Bogdonavich gems What’s Up Doc and Paper Moon, and then working with Stanley Kubrick in Barry Lyndon and Richard Attenborough in A Bridge Too Far. Not to mention the under-rated The Driver. If you’re of a certain age but more into celebrity gossip you might know him best from dating Farrah Fawcett and being John McEnroe’s father-in-law.\nIf your name is Ryan, like my friend Ryan Godfrey, you will know him not only from films (Ryan G watches a lot of movies) but for arguably launching the name Ryan into the American babysphere. Pop culture does sometimes influence what we name our kids (do you know anyone with a kid born in the last 10 years who named them Arya? I know at least one), and Ryan (Godfrey’s) post here:\n\n\n\nPopularity of the first name “Ryan”\n\n\n…showed that while correlation may not always be causation, Ryan was mostly a last name until Love Story hit big and made Ryan O’Neal a star. And ironically, Ryan wasn’t even Ryan O’Neal’s given first name…it was Charles.\nYou can go to the US Social Security Administration’s (SSA) baby names page and check for yourself what are the popular names in any given year, or track the popularity of names over time. You can also download data and do your own analysis. Or you can use the babynames package created by Hadley Wickham of Posit (formerly RStudio). Which is what we’ll do here to look at a few names of interest.\nThe SSA data is, as they note, based on applications for Social Security numbers (SSN). The dataset includes only SSN applciations for people born in the US and excludes any name counts below 5 in any given year.\nSo let’s check out how bay names have trended in the US.\nTo start we’ll load packages to import and clean the data. These are the three packages I use for almost every analysis in r, plus the babynames package.\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(babynames) # pacakge with name data from US Social Security Admin\n\nThe package comes with a few datasets. We will use the babynames set, which has year of birth, sex, name, number, and proportion of people of that sex with that name.\nUnlike most of my posts, I won’t be doing any data transformation or fancy visuals…I just want to see how a few names have trended over the years. First, let’s replicate Ryan G’s chart, and look at the name “Ryan”. We’ll start at 1940 since we already know from Ryan G’s chart when it took off.\nBefore we continue, a couple of caveats…\n\nThere was a baby boom from the late 1940s onward, so even with the dip after 1965 there have been many more births and more people getting SSNs than pre WWII. Many older workers born well before Social Security was a thing never applied for numbers so weren’t in the system. So…\nA more nuanced analysis would be proportion of births with the names. But this isn’t meant to be nuanced or statisitically significant, just a quick look at how name popularity has changed over time in the US.\n\nFrom here on in the r code is folded, so click the arrow to the left of the text above the plot to expand the code window.\n\n\nShow code for Ryans since 1940\n# read in babynames data from package as a local set\nnamesdf &lt;- babynames\n\nnamesdf %&gt;%\n    filter(sex == \"M\") %&gt;%\n    filter(name == \"Ryan\") %&gt;%\n    filter(year &gt; 1940) %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    scale_x_continuous(\n        breaks = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010), \n        labels = c(\"1940\", \"1950\", \"1960\", \"1970\", \"1980\", \"1990\", \"2000\", \"2010\")) +\n    scale_y_continuous(\n        breaks = c(0, 5000, 10000, 15000, 20000, 25000, 30000), \n        labels = c(\"0\", \"5000\", \"10000\", \"15000\", \"20000\", \"25000\", \"30000\")) +\n    theme_minimal() +\n    labs(title = \"Ryan becomes a popular boy's name in the 1970s and after\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nSo yes, as Ryan G wrote, the name Ryan had a slight bump from Peyton Place but the big (baby) bump came when Love Story came out in 1970 and continued through the 1970s and 1980s while Ryan O’Neal’s celebrity status was at its peak.\nWe can see wisps of Ryans before 1970, and because the numbers hit almost 30,000 pre-1970 is a bit compressed. So let’s isolate those years…\n\n\nShow code for Ryans before 1970\nnamesdf %&gt;%\n    filter(sex == \"M\") %&gt;%\n    filter(name == \"Ryan\") %&gt;%\n    filter(year &lt; 1970) %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    labs(title = \"Fewer than 200 Ryans prior to late 1950s\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nFewer than 200 up until the late 1950s, then modest increases after O’Neal starred in the night-time sopa opera Peyton Place from 1964 to 1969.\nBut wait, we’re only looking at boys named Ryan. Do you know any women named Ryan? You might…\n\n\nShow code for girl Ryans since 1940\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(year &gt; 1940) %&gt;%\n    filter(name == \"Ryan\") %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"gold\") +\n    theme_minimal() +\n    labs(title = \"Ryan...not just for boys\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nThinking about O’Neal’s effect on the name Ryan, I got to thinking about the name of his daughter Tatum. She won an Oscar for Paper Moon so I wondered if the name became more popular in the early-mid 1970…\n\n\nShow code for Tatums\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(name == \"Tatum\") %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    labs(title = \"More Tatums in the 1970s, lots more in the 2000s\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n…and sure enough it did. Not to the same extent as Ryan, but a definite pop from 1973 on. After Paper Moon she was in The Bad News Bears, International Velvet and Little Darlings. So like her dad had a run of fame through the early 1980s. But why the jump in the name’s popularity in the late 1990s and through the 2000s? Her very public relationship and marriage to tennis pro John McEnroe? She didn’t do much film or TV work until the early-mid 2000s. So that resurgence, combined with a nostaliga bump? Some other famous Tatum I’m overlooking?\nAll this name trending chatter reminded me of my own firmly held belief which is that it’s very statistically likely a Gen X man or woman in the US will have dated at least one Jennifer in their lives. I’ve dated more than one. It’s almost unavoidable. Why? Well let’s look at birth numbers…\n\n\nShow code for Jennifers\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(name == \"Jennifer\") %&gt;%\n    filter(year &gt;= 1940) %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    scale_x_continuous(\n        breaks = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010), \n        labels = c(\"1940\", \"1950\", \"1960\", \"1970\", \"1980\", \"1990\", \"2000\", \"2010\")) +\n    theme_minimal() +\n    labs(title = \"The Love Story effect on Jennifers\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nJennifer had been on the rise from the 1940s onward, slow and steady. But boomed around 1970. And hey, the female lead in Love Story was named Jenny. Coincidence? Or was Love Story more of a cultural juggernaut than we give it credit 50+ years on? I’d say sorry, but…\nLately the name has fallen out of favor. And it’s striking how normally distributed the curve is, given the timeframe.\nAnd finally, in the interest of marital harmony, and because I showed these charts to my wife Ingrid when I made them just for fun, and then she asked what about her name…well I can’t leave her chart out of the post, right? So here’s the name Ingrid over time.\n\n\nShow code for Ingrids\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(name == \"Ingrid\") %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    labs(title = \"Ingrid fluctuates over time\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nSo there you go. There’s much more to do…download the most recent sets, match names to regions and states. Look at other countries either through r packages like ukbabynames or use packages or API calls to national statistical services."
  },
  {
    "objectID": "posts/exploring-happiness-analysis/index.html#i-accidentally-pushed-this-to-github-way-too-early-and-it-published-and-now-for-some-reason-despite-trying-to-delete-i-cant-make-it-disappear-from-the-blog.-ill-finish-the-post-soon-so-until-i-can-either-delete-or-finish-consider-this-a-work-in-progress",
    "href": "posts/exploring-happiness-analysis/index.html#i-accidentally-pushed-this-to-github-way-too-early-and-it-published-and-now-for-some-reason-despite-trying-to-delete-i-cant-make-it-disappear-from-the-blog.-ill-finish-the-post-soon-so-until-i-can-either-delete-or-finish-consider-this-a-work-in-progress",
    "title": "Exploring Happiness - Part 2…Analysis",
    "section": "I accidentally pushed this to github way too early and it published, and now for some reason despite trying to delete I can’t make it disappear from the blog. I’ll finish the post soon, so until I can either delete or finish, consider this a work in progress",
    "text": "I accidentally pushed this to github way too early and it published, and now for some reason despite trying to delete I can’t make it disappear from the blog. I’ll finish the post soon, so until I can either delete or finish, consider this a work in progress\nIn Part 1 I did some exploratory data analysis on the World Happiness Report data for Chapter 2 of the 2023 edition.\nThat post got me thinking a bit about being more deliberate about my own initial workflow. Some people have project templates, but I mostly just need a starter script template. So I added some lines to my template script to really ingrain the habit of using those packages for the EDA phase. It’s below…feel free to borrow and of course modify to work\n\n\ncode for my r script template\n### template for r analysis work. save as w/ appropriate name to project directory\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\n# EDA tools\nlibrary(DataExplorer)\nlibrary(explore)\nlibrary(skimr)\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n# sets theme as default for all plots\ntheme_set(theme_light)\n\n## ggplot helpers - load if necessary\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot text\n\n\n### load data\n\n\n### clean data, redo as necesary after running basic EDA\n\n\n\n### EDA with DataExplorer, explore, skimr\n\n## DataExplorer summary of completes, missings\neda1 &lt;- introduce(DATA)\nview(eda1)\n\n## explorer summary\nwhr23_fig2_1 %&gt;%\n    describe_tbl()\n\n## skimr summary\nDATA %&gt;%\n    select() %&gt;%\n    skim()\n\n## go back and clean if issues seen here ##\n\n## dataexplorer plots\nplot_bar(DATA)\nplot_histogram(DATA, nrow = 5L)\n\n## dataexplorer correlations\nDATA %&gt;%\n    select(or deselect as needed) %&gt;%\n    filter(!is.na(if needed)) %&gt;%\n    plot_correlation(maxcat = 5L, type = \"continuous\", geom_text_args = list(\"size\" = 4))\n\n## dataexplorer scatterplots\nplot_scatterplot(\n    DATA %&gt;% select(), by = \"choose target for y axis\", nrow = 3L)\n\n## explorer shiny app\nexplore(DATA %&gt;%\n                    select())\n\n### continue with deeper analysis here or start new r script\n\n\nBut let’s get back to examining what makes people happy. By way of a quick recap:\n\nThe WHR comes from an annual world-wide survey administered by Gallup and is published by the Sustainable Development Solutions Network.\nData for Chapter 2 are made available every year, and include a 3-year rolling average of the ladder happiness score aggregated by country, other questions from the poll, and logged GDP for the country.\n\nThe EDA in part 1 showed:\n\nThere are not many missing values in the data.\nAll of the component variables except generosity have strong positive correlations with the happiness score.\nPerception of corruption has, as expected, a negative association with happiness.\n\nLeading me to want to explore relationship between the ladder score and components by country and region.\nBut first…I spaced on adding one varaible to the dataset, the GINI index, which measures “the extent to which the distribution of income (or, in some cases, consumption expenditure) among individuals or households within an economy deviates from a perfectly equal distribution….a Gini index of 0 represents perfect equality, while an index of 100 implies perfect inequality.” (from the World Bank GINI data page, click the Details tab)\nIt was referenced in the statistical appendix but was not included in the dataset made avaialble. The Chapter 2 authors used two GINI values:\n\na GINI of household income as reported to the Gallup survey and imputed via a STATA function, and\nthe World Bank’s GINI value, taken as the mean of GINI values from 2000 to 2022 to account for the spotty nature of by-country GINI values.\n\nWe can’t do the Gallup & STATA generated GINI because we can’t get the data and I don’t have STATA. So we’ll get the World Bank values, and add that into the data already created. Then some quick EDA and the analysis we came here to do.\nWe’ll start by loading the packages we’ll use here, as well as the WHR data we already created.\n\n\ncode for loading packages and WHR data\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\n# functions I use often enough to load them regularly...should probably write a personal package\nsource(\"~/Data/r/basic functions.R\")\n\n# load data \nwhr23_fig2_1a &lt;- readRDS(\n    file = \"~/Data/r/World Happiness Report/data/whr23_fig2_1.rds\") %&gt;%\n        filter(!is.na(ladder_score)) %&gt;%\n    rename(region_whr = region)\n\n\nTo get the GINI values you can go to the World Bank’s GINI page and downloaded the latest spreadsheet, or…\n…even better you could use one of the helpful r packages to get World Bank data; Vincent Arel-Bundock’s(http://arelbundock.com) WDI and the Geospatial Science and Human Security at Oak Ridge National Lab’s wbstats. We’ll use the WDI package here.\nI’ll create a GINI average as per the WHR authors and I’ll also extract the latest GINI value from the set. I’m not sure right now which is ultimately the best to use, but that’s what a bit more EDA is for.\n\n\ncode for loading GINI values\n# using the \"extra = TRUE\" option pulls in capital city & latitude longitude, \n# region, and World Bank income & lending indicators. We won't use them in this analysis but it's\n# worth knowing they're there.\nginis = WDI::WDI(indicator='SI.POV.GINI', start=2000, end=2023, extra = TRUE) %&gt;%\n        as_tibble() %&gt;%\n    select(-status, -lastupdated) %&gt;%\n    # fix missing regions \n    mutate(region =\n                    case_when(country == \"Czechia\" ~ \"Europe & Central Asia\",\n                                        country == \"Viet Nam\" ~ \"East Asia & Pacific\",\n                                        TRUE ~ region)) %&gt;%\n    # remove aggregated regions \n    filter(region != \"Aggregates\")%&gt;%\n    arrange(country, year) %&gt;%\n    rename(gini = SI.POV.GINI) %&gt;%\n    # create the latest and average columns\n    mutate(ginifill = gini) %&gt;%\n    group_by(country) %&gt;%\n    mutate(gini_avg = mean(gini, na.rm = TRUE)) %&gt;%\n    fill(ginifill, .direction = \"downup\") %&gt;%\n    ungroup() %&gt;%\n    filter(year == 2022) %&gt;%\n    mutate(gini_latest = ifelse(is.na(gini), ginifill, gini)) %&gt;%\n    select(country:gini, gini_latest, ginifill, gini_avg, everything()) %&gt;%\n    rename(country_name = country) %&gt;%\n    # clean up some country names to match with WHR data\n    mutate(country_name =\n                    case_when(country_name == \"Czechia\" ~ \"Czech Republic\",\n                                        country_name == \"Congo, Dem. Rep.\" ~ \"Congo (Kinshasa)\",\n                                        country_name == \"Congo, Rep.\" ~ \"Congo (Brazzaville)\",\n                                        country_name == \"Cote d'Ivoire\" ~ \"Ivory Coast\",\n                                        country_name == \"Egypt, Arab Rep.\" ~ \"Egypt\",\n                                        country_name == \"Eswatini\" ~ \"Swaziland\",\n                                        country_name == \"Gambia, The\" ~ \"Gambia\",\n                                        country_name == \"Hong Kong SAR, China\" ~ \"Hong Kong S.A.R. of China\",\n                                        country_name == \"Iran, Islamic Rep.\" ~ \"Iran\",\n                                        country_name == \"Korea, Rep.\" ~ \"South Korea\",\n                                        country_name == \"Kyrgyz Republic\" ~ \"Kyrgyzstan\",\n                                        country_name == \"Lao PDR\" ~ \"Laos\",\n                                        country_name == \"Russian Federation\" ~ \"Russia\",\n                                        country_name == \"Slovak Republic\" ~ \"Slovakia\",\n                                        country_name == \"Turkiye\" ~ \"Turkey\",\n                                        country_name == \"Venezuela, RB\" ~ \"Venezuela\",\n                                        country_name == \"Viet Nam\" ~ \"Vietnam\",\n                                        country_name == \"West Bank and Gaza\" ~ \"Palestinian Territories\",\n                                        country_name == \"Yemen, Rep.\" ~ \"Yemen\",\n                                        TRUE ~ country_name)) \n\n\nNow let’s add the GINI numberss to the WHR data we already have…\n\n\ncode for joining GINI to WHR data\nwhr23_fig2_1 &lt;- whr23_fig2_1a %&gt;%\n    merge(ginis, all = TRUE) %&gt;%\n    as_tibble() %&gt;%\n    select(country_name, iso3c, region, region_whr, whr_year:lowerwhisker, logged_gdp_per_capita,\n                 gini_avg, gini_latest, everything()) %&gt;%\n    ## fill in Taiwan, no longer in this set but still available \n    ### at https://pip.worldbank.org/country-profiles/TWN\n    mutate(gini_avg = ifelse(\n        country_name == \"Taiwan Province of China\", 32.09833333, gini_avg)) %&gt;%\n    mutate(gini_latest = ifelse(\n        country_name == \"Taiwan Province of China\", 31.48, gini_latest)) %&gt;%\n    filter(!is.na(ladder_score))\n\n\nN.B…yes, I use merge() and not left/right/full_join()…I learned SAS before SQL and old habits die hard (shrug emoji).\nN.B.2…Taiwan data is no longer avaialble in most WB sets, either in the spreadsheet or via the API the packages access. It is still at the WB’s Poverty & Inequality Platform so I downloaded what I could and hard-coded. Why? Because I’m a little OCD when it comes to trying to minimize missing data.\nAnyway…now we have to do some quick EDA on the GINI values in relation to the data we have. First I want to take a quick look at the by-country difference between the latest GINI value and the mean value for the period since 2000. We subtract the average from the latest value, do a quick skimr check, a density plot on the latest-average difference…\n\n\ncode for EDA skim & density plot\n# a quick skim \nwhr23_fig2_1 %&gt;%\n    mutate(gini_diff = gini_latest - gini_avg) %&gt;%\n    select(gini_latest, gini_avg, gini_diff) %&gt;%\n    skimr::skim()\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n137\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngini_latest\n7\n0.95\n36.73\n7.60\n23.20\n31.54\n35.30\n40.88\n63.00\n▃▇▃▁▁\n\n\ngini_avg\n7\n0.95\n38.09\n7.88\n24.79\n32.26\n36.68\n42.71\n62.40\n▆▇▅▂▁\n\n\ngini_diff\n7\n0.95\n-1.37\n2.32\n-8.91\n-2.85\n-1.00\n0.16\n4.37\n▁▂▇▇▁\n\n\n\n\n\ncode for EDA skim & density plot\n\nwhr23_fig2_1 %&gt;%\n    filter(!is.na(gini_avg)) %&gt;%\n    mutate(gini_diff = gini_latest - gini_avg) %&gt;%\n    select(country_name, gini_latest, gini_avg, gini_diff, region_whr) %&gt;%\n    arrange(gini_diff) %&gt;%\n    ggplot(aes(gini_diff)) +\n                    geom_density(fill = \"blue\") +\n    xlim(-9, 6)\n\n\n\n\n\nQuick EDA observations from the skim and denisty plot:\n\nThe GINI scale is 0 to 100, and the spread in this WHR set is 24.7 to 62.4, almost exactly as the entire panel from the WB GINI dataset.\nThe means, medians, standard deviations, and ranges for average and latest are close enough.\nThe difference (latest - average) is looks on the denisty plot to be clustered just below 0, and the median difference is only -1, so we’ll use the average.\nWe only lose 7 countries from the set by adding GINI.\n\nSo let’s use the average and see how it relates to our WHR variables by doing first a correlation matrix from the DataExplorer package…\n\n\ncode for EDA correlation\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, standard_error_of_ladder_score, gini_avg, logged_gdp_per_capita,\n                 social_support:perceptions_of_corruption,\n                  explained_by_log_gdp_per_capita:residual) %&gt;%\n    filter(!is.na(residual)) %&gt;%\n    filter(!is.na(gini_avg)) %&gt;%\n    DataExplorer::plot_correlation(maxcat = 5L, type = \"continuous\", geom_text_args = list(\"size\" = 3))\n\n\n\n\n\n…and the DataExplorer scatterplots.\n\n\ncode for EDA scatterplots\nDataExplorer::plot_scatterplot(\n    whr23_fig2_1 %&gt;% select(gini_avg, ladder_score, logged_gdp_per_capita,\n                                                    social_support:perceptions_of_corruption), \n    by = \"gini_avg\", nrow = 3L)\n\n\n\n\n\nWe can see from the correlation matrix and the scatterplots that:\n\nthere is a mild but persistent relationship between a lower GINI score (less inequality) and the measures that correlate with more happines…the happiness score itself, life expectancy, freedom to make choices, etc.\nThe one negative relationship was with perceptions of corruption, or the higher the inequality measure in their country, the more likely people answering the survey were to report that they perceived higher levels of corruption.\n\nAlso, interestingly, a lower GINI index correlated to a higher GDP per capita…in other words, it’s better for all to spread the wealth.\nPredicting the happiness score  Ok, let’s run a couple of models to predict the happiness score and plot the results.\nBut why, you might ask, since the WHR authors already did that? Well, I’ve never really run regression models in r. I did in SAS for one job, and I haven’t had to since then. So join me as I walk through running a model and plotting the results.\nI had hoped to pull in a few other indicators to add to the model. The main one I wanted was literacy. Unfortunately there were too many missing values in the UNESCO set…most European countries and a few other key countries. I then thought about public expenditure on education but was worried about colinearity with the GINI index (no, I didn’t test it). So we’ll stick with what we have and not cloud up the model with too many exogenous variables."
  },
  {
    "objectID": "posts/exploring-happiness-eda-pt2/index.html",
    "href": "posts/exploring-happiness-eda-pt2/index.html",
    "title": "Exploring Happiness - EDA Part 2",
    "section": "",
    "text": "Happiness in a cone"
  },
  {
    "objectID": "posts/r-to-tableau-and-quarto/index.html",
    "href": "posts/r-to-tableau-and-quarto/index.html",
    "title": "r to Tableau, then show it in Quarto",
    "section": "",
    "text": "r plays nicely with other outputs\nArtwork by Allison Horst.\ntl/dr - I’m trying to show I have some Tableau skills to go with my r skills to help my chances at employment (hint-hint)."
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html",
    "href": "posts/my-year-of-riding-danishly/index.html",
    "title": "My Year of Riding Danishly",
    "section": "",
    "text": "My Univega bike enjoying the view at Dragør"
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#getdata",
    "href": "posts/my-year-of-riding-danishly/index.html#getdata",
    "title": "My Year of Riding Danishly",
    "section": "Pull the Data",
    "text": "Pull the Data\nStrava offers an API to get data, but at first I thought it would be easier to request the my full archive via my user profile page and use the activity CSV and clean that up. That turned out to be a bit tricky because of how the dates and times were handled in the CSV.\nA week or so after I downloaded the CSV I came across this Bluesky #TidyTuesday post where someone used the rStrava package to access the Strava API. This turned out to be much better for wrangling dates. It had most of the fields you get in the CSV, except for a few interesting ones including calories burned and the average and max grades. You can get them via the API but only when pulling individual activities. It was easy to merge the few fields I wanted from the CSV into the data collected from the API.\nThe code below shows the API pull (not the spreadsheet import) and how I cleaned it and added new fields. I used some of the exact same text in the rStrava vignette with regard to creating the httr-oauth file and the stoken file. For a more detailed explanation, go there.\nMake sure also to read the Strava API guidelines and documentation for information on rate limits, the JSON structures and a data dictionary.\n\n\nShow code for getting data via rStrava\n### this section in a separate file called stoken.r\n# create the  access token \napp_name &lt;- 'myappname' # chosen by user\napp_client_id  &lt;- 'myid' # an integer, assigned by Strava\napp_secret &lt;- 'xxxxxxxx' # an alphanumeric secret, assigned by Strava\n\n# Setting cache = TRUE for strava_oauth will create an authentication file in the working directory. \nstoken &lt;- httr::config(token = strava_oauth(\n    app_name, app_client_id, app_secret, cache = TRUE, app_scope=\"activity:read_all\"))\n###\n\n### Session to pull the data\n# calling the `.httr-oauth` in the directory to create a session-specific token\nstoken &lt;- httr::config(token = readRDS('.httr-oauth')[[1]])\n\n# this call shows up in the console with your Strava ID, name and any bio info you've entered.\nmyinfo &lt;- get_athlete(stoken, id = 'my strava athlete id')\nhead(myinfo)\n\n# pull the data\n # this call pulls all the data into a large list \nmyact &lt;- get_activity_list(stoken)\n\n# convert the data into a dataframe and clean & mutate as needed\nact_data &lt;- compile_activities(myact) %&gt;%\n    as_tibble() %&gt;%\n    mutate(gear_name = case_when(\n        gear_id == \"b6298198\" ~ \"Univega\", \n        gear_id == \"b11963967\" ~ \"Commute bike\", \n        TRUE ~ \"Not a bike ride\")) %&gt;%\n    mutate(activity_date = lubridate::as_datetime(start_date_local)) %&gt;%\n    mutate(activity_date_p = as.Date(start_date_local)) %&gt;%\n    mutate(activity_year = lubridate::year(start_date_local),\n                 activity_month = lubridate::month(start_date_local),\n                 activity_month_t = lubridate::month(start_date_local, label = TRUE, abbr = FALSE),\n                 activity_day = lubridate::day(start_date_local),\n                 activity_md =  paste0(activity_month_t, \" \", activity_day),\n                 activity_wday = wday(activity_date_p, label = TRUE, abbr = FALSE),\n                 activity_hour = lubridate::hour(activity_date),\n                 activity_min = lubridate::minute(activity_date),\n                 activity_hmt = paste0(activity_hour, \":\", activity_min),\n                 activity_hm = hm(activity_hmt),\n                 moving_time_hms = hms::hms(moving_time),\n                 elapsed_time_hms = hms::hms(elapsed_time)) %&gt;%\n    mutate(location_country = case_when(\n        timezone == \"(GMT+01:00) Europe/Copenhagen\" ~ \"Denmark\",\n        timezone == \"(GMT+01:00) Europe/Paris\" ~ \"France\",\n        TRUE ~ \"United States\")) %&gt;%\n## random edits\n    mutate(commute = ifelse((activity_year == 2023 & activity_md == \"June 14\" & name == \"Morning commute\"),\n                                                    TRUE, commute)) %&gt;%\n    mutate(commute = ifelse((activity_year == 2023 & activity_md == \"September 19\" & name == \"Afternoon commute\"),\n                                                    TRUE, commute)) %&gt;%\n    mutate(commute = ifelse((activity_year == 2023 & activity_md == \"October 5\" & name == \"Morning Ride\"),\n                                                    TRUE, commute)) %&gt;%\n    mutate(name = ifelse((activity_year == 2023 & activity_md == \"October 4\" & name == \"Morning Ride\"),\n                                             \"Morning commute\", name)) %&gt;%\n    mutate(name = ifelse((activity_year == 2023 & activity_md == \"October 4\" & name == \"Evening Ride\"),\n                                             \"Evening commute\", name)) %&gt;%\n    mutate(name = ifelse((activity_year == 2023 & activity_md == \"October 5\" & name == \"Morning Ride\"),\n                                             \"Morning commute\", name)) %&gt;%\n    mutate(name = ifelse(name == \"Evening commmute\", \"Evening commute\", name)) %&gt;%\n## adjust studieskolen vesterbro morning rides\n    mutate(name = case_when(\n        (activity_year == 2023 & (name == \"Morning Ride\" | name == \"Rainy Morning Ride\") &\n            activity_md %in% c(\"October 24\", \"October 26\", \"October 31\", \"November 2\", \"November 7\",\n                                                 \"November 9\", \"November 14\", \"November 16\", \"November 21\", \"November 23\",\n                                                 \"November 28\", \"November 30\", \"December 5\", \"December 7\",\n                                                 \"December 12\", \"December 14\"))\n        ~ \"To Studieskolen\", TRUE ~ name)) %&gt;%\n    # adjust studieskolen vesterbro afternoon rides\n    mutate(name = case_when(\n        (activity_year == 2023 & (name == \"Lunch Ride\" | name == \"Afternoon Ride\") &\n            activity_md %in% c(\"October 24\", \"October 26\", \"October 31\", \"November 7\",\n                                                 \"November 9\", \"November 14\", \"November 16\", \"November 21\", \"November 23\",\n                                                 \"November 30\", \"December 5\", \"December 12\", \"December 14\"))\n        ~ \"From Studieskolen\", TRUE ~ name)) %&gt;%\n    mutate(name = ifelse((activity_year == 2023 & name == \"From Studieskolen\" &\n                                                activity_md %in% c(\"November 23\", \"December 14\") & activity_hour &gt; 13),\n                                             \"Afternoon Ride\", name)) %&gt;%\n## adjust studieskolen KVUC rides\n    mutate(name = case_when(\n        (activity_year == 2023 & name == \"Afternoon Ride\" &\n            activity_md %in% c(\"October 9\", \"October 11\",\n                                                 \"October 23\", \"October 25\", \"October 30\",  \"November 1\",\n                                                 \"November 6\", \"November 8\", \"November 13\", \"November 15\",\n                                                 \"November 20\", \"November 22\", \"November 27\", \"November 29\",\n                                                 \"December 4\", \"December 6\", \"December 11\", \"December 13\",\n                                                 \"December 20\")) ~ \"To Studieskolen KVUC\",\n        TRUE ~ name)) %&gt;%\n    mutate(name = case_when(\n        (activity_year == 2023 & name == \"Evening Ride\" &\n            activity_md %in% c(\"October 9\", \"October 11\",\n                                                 \"October 23\", \"October 25\", \"October 30\",  \"November 1\",\n                                                 \"November 6\", \"November 8\", \"November 13\", \"November 15\",\n                                                 \"November 20\", \"November 22\", \"November 27\", \"November 29\",\n                                                 \"December 4\", \"December 6\", \"December 11\", \"December 13\",\n                                                 \"December 20\")) ~ \"From Studieskolen KVUC\",\n        TRUE ~ name)) %&gt;%\n    mutate(name = ifelse(\n        (activity_year == 2023 & name == \"To Studieskolen KVUC\" & activity_md == \"December 20\" & activity_hour == 16),\n        \"From Studieskolen KVUC\", name)) %&gt;%\n    mutate(name = ifelse((commute == \"TRUE\" & grepl(\"Ride\", name)),\n                                                                str_replace(name, \"Ride\", \"commute\"), name)) %&gt;%\n    mutate(ride_type = case_when(\n        commute == \"TRUE\" ~ \"Commute/Studieskolen\",\n        name %in% c(\"To Studieskolen\", \"From Studieskolen\",\n                                                 \"To Studieskolen KVUC\", \"From Studieskolen KVUC\")\n        ~ \"Commute/Studieskolen\",\n        gear_name == \"Univega\" ~ \"Workout\",\n        TRUE ~ \"Other\")) %&gt;%\n    select(activity_id = id, activity_date:activity_wday, activity_hm, activity_hour, activity_min, timezone,\n                 activity_name = name, ride_type, sport_type, commute, gear_name, gear_id, distance_km = distance,\n                 moving_time_hms, moving_time, elapsed_time_hms, elapsed_time, average_speed, max_speed, average_watts, kilojoules,\n                 elevation_high = elev_high, elevation_low = elev_low, elevation_gain = total_elevation_gain, location_country,\n                 lat_start = start_latlng1, lng_start = start_latlng2, lat_end = end_latlng1, lng_end = end_latlng2)\n\n## merge this with CSV data (activity_id, calories, average_grade, max_grade, average_elapsed_speed, elevation_loss) on the activity_id key to get the dataframe \"strava_data\""
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#eda1",
    "href": "posts/my-year-of-riding-danishly/index.html#eda1",
    "title": "My Year of Riding Danishly",
    "section": "EDA with DataExplorer",
    "text": "EDA with DataExplorer\nOk, we have some data, let’s see what it looks like. These two DataExplorer plots show (on the right) a general overview of the dataset…percent of observations missing, percent of discrete & continuous variables, etc and on the left, the percent missing for each variable.\nTo put them next to each other I’ve used a bootstrap css grid system to define the columns and place the plots there. I’ll use the css grids later for the gt tables. They won’t show in this rendered html doc, so go to the .qmd file to see how it works. The code is plot_intro(strava_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe main take-away here is that the average_elapsed_speed variable is missing 27% of observations, so we won’t bother with it in our analysis."
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#eda2",
    "href": "posts/my-year-of-riding-danishly/index.html#eda2",
    "title": "My Year of Riding Danishly",
    "section": "EDA with Scatterplots",
    "text": "EDA with Scatterplots\nWhile DataExplorer does have functionality for scatterplots, each call only allows for one comparison y-axis variable. I could do multiple calls within the package but I recently came across Cedric Scherer’s post on automating plot outputs using functional programming and wanted to give that approach a try, this project being perfect for a test. I ended up copying one of his scatterplot functions with no modification for use here. (I’m still a bit weak on functional programming so didn’t want to mess anything up and have to spend time debugging)\nThis first bit of code creates the plot function:\n\n\nShow code for creating automated plot function\n## plot template as function\nplot_scatter_lm &lt;- function(data, var1, var2, pointsize = 2, transparency = .5, color = \"\") {\n\n    ## check if inputs are valid\n    if (!exists(substitute(data))) stop(\"data needs to be a data frame.\")\n    if (!is.data.frame(data)) stop(\"data needs to be a data frame.\")\n    if (!is.numeric(pull(data[var1]))) stop(\"Column var1 needs to be of type numeric, passed as string.\")\n    if (!is.numeric(pull(data[var2]))) stop(\"Column var2 needs to be of type numeric, passed as string.\")\n    if (!is.numeric(pointsize)) stop(\"pointsize needs to be of type numeric.\")\n    if (!is.numeric(transparency)) stop(\"transparency needs to be of type numeric.\")\n    if (color != \"\") { if (!color %in% names(data)) stop(\"Column color needs to be a column of data, passed as string.\") }\n\n    g &lt;-\n        ggplot(data, aes(x = !!sym(var1), y = !!sym(var2))) +\n        geom_point(aes(color = !!sym(color)), size = pointsize, alpha = transparency) +\n        geom_smooth(aes(color = !!sym(color), color = after_scale(prismatic::clr_darken(color, .3))),\n                                method = \"lm\", se = FALSE) +\n        theme_minimal() +\n        theme(panel.grid.minor = element_blank(),\n                    legend.position = \"top\")\n\n    if (color != \"\") {\n        if (is.numeric(pull(data[color]))) {\n            g &lt;- g + scale_color_viridis_c(direction = -1, end = .85) +\n                guides(color = guide_colorbar(\n                    barwidth = unit(12, \"lines\"), barheight = unit(.6, \"lines\"), title.position = \"top\"\n                ))\n        } else {\n            g &lt;- g + scale_color_brewer(palette = \"Set2\")\n        }\n    }\n\n    return(g)\n}\n\n\nAnd here we call the plot_scatter_lm function. In this first instance I’m plotting some variables against ride distance.\n\n## data extract\nstrava_activities_rides &lt;- strava_data %&gt;%\n    filter(activity_year == 2023)\n\n## 1st plot call - distance as y axis\npatchwork::wrap_plots(\n    map2(c(\"elapsed_time\", \"moving_time\", \"average_speed\",\"average_watts\", \"calories\", \"kilojoules\"), \n             c(\"distance_km\", \"distance_km\", \"distance_km\", \"distance_km\", \"distance_km\", \"distance_km\"), \n             ~plot_scatter_lm(data = strava_activities_rides, var1 = .x, var2 = .y, pointsize = 3.5) + \n                theme(plot.margin = margin(rep(15, 4)))))\n\n\n\n\n\n\n\n\nThis first set of plots confirms what we saw in the correlation heatmap while also displaying how the ride data points are distributed. We see the positive and almost 1:1 relationships between distance and both time measures, elapsed and moving. Elapsed time is the total time from when you start the ride until you end and save it in the app. Moving time is Strava’s calculation of how much time you actually spent in motion. Strava provides ride time in seconds, which is best for this kind of plotting, and of course you can use lubridate to convert it to hours & minutes.\nWe also see the negative association with watts that we saw in the correlations. I’m making a note to take a closer look at how much an effect watts has later on in the regression section.\nNote the outlier ride of 60km and an elapsed time of more than 15,000 seconds. That was the ride where I was hit by the car…the elapsed time ended up at 18,242 seconds, or more than 5 hours. Moving time was only 2 hours & 44 minutes. I guess turning off the app and saving the ride wasn’t top of my to-do list while laying on the street with broken bones.\nThis next group of plots has moving time as the comparison variable. I didn’t plot distance on the x-axis as we saw that relationship already.\n\n\nshow code for moving time scatterplots\npatchwork::wrap_plots(\n    map2(c(\"average_speed\", \"elevation_gain\", \"average_grade\", \"average_watts\", \"calories\", \"kilojoules\"), \n             c(\"moving_time\", \"moving_time\", \"moving_time\", \"moving_time\", \"moving_time\", \"moving_time\"), \n             ~plot_scatter_lm(data = strava_activities_rides, var1 = .x, var2 = .y, pointsize = 3.5) + \n                theme(plot.margin = margin(rep(15, 4)))))\n\n\n\n\n\n\n\n\n\nWe see average speed decreasing a bit as ride time goes up, which makes some sense.\nWe also see the longer rides had more elevation gain, though to be clear, there’s not much elevation to be gained here in Denmark…it’s a bit flat. But the longer workout rides tended to be to northern parts of Sjælland, where it can get a bit hilly at times. I tried adding ride_type as a color aesthetic for the plots and while I won’t display that here, it did confirm my suspicion that the longer rides up north were responsible for the relationship between ride time and elevation.\nAs expected I also expended more energy (calories & kilojoues) the more time I was riding.\nNext we plot average speed in y axis, omitting distance and time, as we’ve already seen that relationship.\n\n\nshow code for average speed scatterplots\npatchwork::wrap_plots(\n    map2(c(\"elevation_gain\", \"average_grade\", \"max_grade\", \"average_watts\", \"calories\", \"kilojoules\"), \n             c(\"average_speed\", \"average_speed\", \"average_speed\", \"average_speed\", \"average_speed\", \"average_speed\"), \n             ~plot_scatter_lm(data = strava_activities_rides, var1 = .x, var2 = .y, pointsize = 3.5) +\n                theme(plot.margin = margin(rep(15, 4)))))\n\n\n\n\n\n\n\n\n\nThe strongest relationship is with watts, confirming what we saw in the correlation heatmap.\nThere’s an oddly slightly positive relationship with elevation, but I wonder if that’s a result of my longer workout rides to the northern part of Sjælland also having not only more elevation but also a bit more open road (especially in the early morning hours when I tend to ride) to ride faster. Again I ran it with ride_type as a color aesthetic which more or less confirmed my intuition.\nI’m a bit surprised to see that energy output isn’t as positively associated with average speed, but perhaps here where it’s flat there’s only so high a level of energy burn I can get to.\nAnd finally, kilojoules as the comparison variable. Why use kilojoules and not calories? Accoring to this Garmin FAQ, calories expended are total energy expended in the time it took to do the workout, while kilojoules is the energy burned by the workout. The formula for kilojoules is (watts x seconds) x 1000.\nSince we’ve already plotted kilojoules against the distance, time, and speed, no need to repeat.\n\n\nshow code for kilojoule scatterplots\n    patchwork::wrap_plots(\n        map2(c(\"average_watts\", \"elevation_gain\", \"average_grade\"), \n                 c(\"kilojoules\", \"kilojoules\", \"kilojoules\"), \n                 ~plot_scatter_lm(data = strava_activities_rides, var1 = .x, var2 = .y, pointsize = 3.5) + \n                    theme(plot.margin = margin(rep(15, 4)))))"
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#tables",
    "href": "posts/my-year-of-riding-danishly/index.html#tables",
    "title": "My Year of Riding Danishly",
    "section": "Analysis Pt 1 - gt tables",
    "text": "Analysis Pt 1 - gt tables\nNow that we have a broad overview of the data, let’s get a bit more specific, first with some tables using the gt package. To prep the data for the tables I created a summary-level subset of the data with means, medians, etc. No need to show that code, as it’s fairly basic.\nFirst, a top-line summary:\n\n\nShow code for summary table\nsumtable %&gt;%\n    select(rides, km_total, elev_total, time_total1, time_total2, cal_total, kiloj_total) %&gt;%\n    gt() %&gt;%\n    fmt_number(columns = c(km_total, elev_total, cal_total, kiloj_total), decimals = 0) %&gt;%\n    cols_label(rides = \"Total Rides\", km_total = \"Total Kilometers\",\n                         elev_total = md(\"Total Elevation *(meters)*\"),\n                         time_total1 = md(\"Total Time *(hours/min/sec)*\"),\n                         time_total2 = md(\"Total Time *(days/hours/min/sec)*\"),\n                         cal_total = \"Total Calories\", kiloj_total = \"Total Kilojoules\") %&gt;%\n    cols_align(align = \"center\", columns = everything()) %&gt;%\n    tab_style(\n        style = cell_text(align = \"center\"),\n        locations = cells_column_labels(\n            columns = c(rides, km_total, elev_total, time_total1, time_total2, cal_total, kiloj_total))) %&gt;%\n    tab_header(title = md(\"My Year of Riding Danishly&lt;br&gt;*Ride Totals*\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Year of Riding Danishly\nRide Totals\n\n\nTotal Rides\nTotal Kilometers\nTotal Elevation (meters)\nTotal Time (hours/min/sec)\nTotal Time (days/hours/min/sec)\nTotal Calories\nTotal Kilojoules\n\n\n\n\n446\n2,562\n5,372\n133:26:42\n5d 13H 26M 42S\n68,578\n62,737\n\n\n\n\n\n\n\n\nFor the year, more than 440 rides covering 2,500 kilometers. I spent the equivalent of more than 5 days on the bike, and burned 60,000+ units of energy. Which means on average, every day I did 1.2 rides,and went about 7 km, a few km more than the average Copenhagener. (It occurs to me know that I didn’t make a count for how many days of the year I rode…an edit to come perhaps…)\nI won’t be showing code for the rest of the tables, they’re very basic gt calls. Let’s see the ranges for distance, time, elevation gain, and energy expended.\n\n\n\n\n\n\n\n\n\n\n\nRide Statistics - Distance (in km)\n\n\nAverage\nMedian\nShortest\nLongest\n\n\n\n\n5.74\n4.23\n1.06\n60.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRide Statistics - Time\n\n\nAverage\nMedian\nShortest\nLongest\n\n\n\n\n17M 57S\n12M 40S\n4M 23S\n2H 44M 26S\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRide Statistics - Elevation (meters)\n\n\nAverage\nMedian\nLowest\nHighest\n\n\n\n\n12.04\n8.35\n0\n328.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRide Statistics - Energy\n\n\nCalories Burned\nKilojoules Burned\n\n\nAverage\nLeast\nMost\nAverage\nLeast\nMost\n\n\n\n\n153.76\n38.66\n1136.32\n140.67\n34.7\n1019.1\n\n\n\n\n\n\n\n\n\n\nThe rides spanned 1 km to 60 km, with the average & median ride around 4-5km, which makes sense given that my work commute was a bit over 4km and school rides between 4km - 6km. The elevation stats are what you’d expect for Denmark, and the average ride burned 140-150 units of energy."
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#plots",
    "href": "posts/my-year-of-riding-danishly/index.html#plots",
    "title": "My Year of Riding Danishly",
    "section": "Analysis Pt 2 Plots",
    "text": "Analysis Pt 2 Plots\nNow onto some fun with ggplot2. We’ll see how the rides spread out across months, days of the week, and subsetted by type of ride. For type, I grouped all rides to work and Danish classes (at one of two Studieskolen campuses) into one group, workouts into another, and errands and other random rides into a third group.\nThese first two show by month and by type of ride. To put the charts side-by-side I used patchwork. For the rides by type, I used percentages for the bar and dynamic labelling to put the ride-by-type n in the x-axis label. This is where I used the {. -&gt;&gt; tmp} call to pass the temproary data thru to calls within the plot code. It’s a neat trick, just remember to call rm(tmp) to remove the temporary set from the environment.\n\n\nShow code for charts by month and type\n# by month\nridesplot1 &lt;-\nrides_mth_type %&gt;%\n    distinct(activity_month_t, .keep_all = TRUE) %&gt;%\n    select(activity_month_abbv, rides_by_month) %&gt;%\n    ggplot(aes(activity_month_abbv, rides_by_month)) +\n    geom_col(fill = \"#C8102E\") +\n    geom_text(aes(label= rides_by_month),\n                        color = \"white\", size = 5, vjust = 1.5) +\n    labs(x = \"\", y = \"\", title = \"Spring & Summer Weather = More Rides\",\n             subtitle = glue::glue(\"*Average Rides / Month = {round(mean(rides_mth_type$rides_by_month, 3))}*\")) +\n    theme_minimal() +\n    theme(panel.grid = element_blank(), plot.title = element_text(hjust = 0.5),\n                plot.subtitle = element_markdown(hjust = 0.5),\n                axis.text.y = element_blank())\n\n\n# by type\nridesplot2 &lt;-\nrides_mth_type %&gt;%\n    select(ride_type, ride_type_n) %&gt;%\n    group_by(ride_type) %&gt;%\n    mutate(rides_by_type = sum(ride_type_n)) %&gt;%\n    ungroup() %&gt;%\n    select(-ride_type_n) %&gt;%\n    distinct(rides_by_type, .keep_all = TRUE) %&gt;%\n    mutate(ride_type_pct = rides_by_type / sum(rides_by_type)) %&gt;%\n    {. -&gt;&gt; tmp} %&gt;%\n    ggplot(aes(ride_type, ride_type_pct)) +\n    geom_col(fill = \"#C8102E\") +\n    scale_x_discrete(labels = paste0(tmp$ride_type, \"&lt;br&gt;Total Rides = \", tmp$rides_by_type, \"\")) +\n    geom_text(data = subset(tmp, ride_type != \"Workout\"),\n        aes(label= scales::percent(round(ride_type_pct, 2))),\n                        color = \"white\", size = 5, vjust = 1.5) +\n    geom_text(data = subset(tmp, ride_type == \"Workout\"),\n                        aes(label= scales::percent(round(ride_type_pct, 2))),\n                        color = \"#C8102E\", size = 5, vjust = -.5) +\n    labs(x = \"\", y = \"\", title = \"Lots of Riding to Work or Danish Class\") +\n    theme_minimal() +\n    theme(panel.grid = element_blank(), plot.title = element_text(hjust = 0.5),\n                axis.text.y = element_blank(), axis.text.x = element_markdown())\n    rm(tmp)\n    \nridesplot1 + ridesplot2 \n\n\n\n\n\n\n\n\n\nAs the weather got better I ride more often. My work contract ended in early October and from them on it was 4 days a week to Danish classes. Those commute and school rides accounted for just about 70% of all my rides for the year.\nNext let’s plot the percent of ride type by month…\n\n\nShow code for ride-type-by-month chart\n\nrides_mth_type %&gt;%\n    ggplot(aes(activity_month_t, ride_type_pct, fill = ride_type)) +\n    geom_bar(stat = \"identity\") +\n    geom_text(data = subset(rides_mth_type, ride_type != \"Workout\"),\n                        aes(label = scales::percent(round(ride_type_pct, 2))),\n                        position = position_stack(vjust = 0.5),\n                        color= \"white\", vjust = 1, size = 5) +\n    labs(x = \"\", y = \"\", title = \"Most Rides Each Month Were Commutes to/from Work or Danish Class\") +\n    scale_fill_manual(values = c(\"#0072B2\", \"#E69F00\", \"#CC79A7\"),\n                                        labels = c(\"Commute/&lt;br&gt;Studieskolen\", \"Other\", \"Workout\")) +\n    theme_minimal()+\n    theme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'),\n                legend.text = element_markdown(),\n                legend.key.width = unit(1.5, 'cm'), legend.title = element_blank(),\n                axis.text.y = element_blank(), plot.title = element_text(hjust = 0.5),\n        panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    guides(fill = guide_legend(label.position = \"bottom\"))\n\n\n\n\n\n\n\n\n\nA bit of a drop in the commute/school percentage after my work contract ended, but that’s expected.\nThe next two plots display by day of the week and ride type by day of the week. For overall rides by day, I again did the percentage for the bars and added the number of rides into the x-axis labels. Ride-type by day follows.\n\n\nShow code for ride by day chart\n# day of the week and type\nstrava_data %&gt;%\n    filter(activity_year == 2023) %&gt;%\n    group_by(activity_wday) %&gt;%\n    summarise(rides_by_wday = n()) %&gt;%\n    mutate(rides_wday_pct = rides_by_wday / sum(rides_by_wday)) %&gt;%\n    mutate(rides_day_avg = round(mean(rides_by_wday), 0)) %&gt;%\n    ungroup() %&gt;%\n    mutate(total_rides = sum(rides_by_wday)) %&gt;%\n    {. -&gt;&gt; tmp} %&gt;%\n    ggplot(aes(activity_wday, rides_by_wday)) +\n    geom_col(fill = \"#C8102E\") +\n    scale_x_discrete(labels = paste0(tmp$activity_wday, \"&lt;br&gt;Total Rides = \", tmp$rides_by_wday, \"\")) +\n    geom_text(aes(label = scales::percent(round(rides_wday_pct, 2))),\n                        color = \"white\", size = 5, vjust = 1.5) +\n    labs(x = \"\", y = \"\", title = \"More Rides on Weekdays, Especially Tues -&gt; Thurs\",\n             subtitle = glue::glue(\"*Total Rides = {tmp$total_rides} &lt;br&gt; Average Rides / Day of the Week = {tmp$rides_day_avg}*\")) +\n    theme_minimal() +\n    theme(panel.grid = element_blank(), plot.title = element_text(hjust = 0.5),\n                plot.subtitle = element_markdown(hjust = 0.5),\n                axis.text.x = element_markdown(),\n                axis.text.y = element_blank())\n\n\n\n\n\n\n\n\n\nWeekdays in general had the highest percentage of rides. Fridays had fewer rides because we tended to work-from-home on Fridays and when the work contract ended I did not have class on Fridays.\n\n\nShow code for ride-type by day chart\nstrava_data %&gt;%\n    filter(activity_year == 2023) %&gt;%\n    group_by(activity_wday, ride_type)  %&gt;%\n    summarise(ride_type_n = n()) %&gt;%\n    mutate(ride_type_pct = ride_type_n / sum(ride_type_n)) %&gt;%\n    ungroup() %&gt;%\n    ggplot(aes(activity_wday, ride_type_pct, fill = ride_type)) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = scales::percent(round(ride_type_pct, 2))),\n                        position = position_stack(vjust = 0.5),\n                        color= \"white\", size = 5) +\n    labs(x = \"\", y = \"\", title = \"Weekdays Were for Getting to/from Work & Danish Class\",\n             subtitle = \"Weekends for Errands and Workouts\") +\n    scale_fill_manual(values = c(\"#0072B2\", \"#E69F00\", \"#CC79A7\"),\n                                        labels = c(\"Commute/&lt;br&gt;Studieskolen\", \"Other\", \"Workout\")) +\n    theme_minimal() +\n    theme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'),\n                legend.text = element_markdown(),\n                legend.key.width = unit(1.5, 'cm'), legend.title = element_blank(),\n                axis.text.y = element_blank(),\n                plot.title = element_text(hjust = 0.5),\n                plot.subtitle = element_text(hjust = 0.5, size = 14),\n                panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    guides(fill = guide_legend(label.position = \"bottom\"))\n\n\n\n\n\n\n\n\n\nI wanted to see what time of day I started my rides and wanted to display the data in the form of a clock rather than a bar chart. This explainer was helpful to get the visualisation to work, with just some minor tweaking to get the data labels showing correctly.\nThe first “clock” represents ride starts times by the hour of the day, in 24-hour format. The second shows which minutes of the hour were most common for starting a ride.\n\n\nShow code for circular time plots\nstrava_data %&gt;%\n    filter(activity_year == 2023) %&gt;%\n    count(ride_type, activity_hour) %&gt;%\n    {. -&gt;&gt; tmp} %&gt;%\n    ggplot(aes(activity_hour, y = n, fill = ride_type)) +\n    geom_bar(stat = \"identity\") +\n    scale_x_continuous(limits = c(0, 24), breaks = seq(0, 24)) +\n    geom_text(data = subset(tmp, ride_type == \"Commute/Studieskolen\" & n &gt; 20),\n        aes(label= n), color = \"white\", size = 4) +\n    coord_polar(start = 0) +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"#0072B2\", \"#E69F00\", \"#CC79A7\"),\n                                        labels = c(\"Commute/&lt;br&gt;Studieskolen\", \"Other\", \"Workout\")) +\n    labs(x = \"\", y = \"\",\n             title = \"Most Rides During Morning and Evening Commuting Hours\",\n             subtitle = \"*Numbers Correspond to Hour of Day on a 24 hr clock*\") +\n    theme(legend.text = element_markdown(),\n                axis.text.y = element_blank(),\n                legend.title = element_blank(),\n                plot.title = element_text(size = 10, hjust = 0.5),\n                plot.subtitle = element_markdown(hjust = 0.5, size = 9))\n\n\n\n\n\n\n\n\n\nShow code for circular time plots\nrm(tmp)\n\nactivty_ampm %&gt;%\n    ggplot(aes(activity_min, y = activity_min_n, fill = ampm)) +\n    geom_col(position = position_stack(reverse = TRUE)) +\n    scale_x_continuous(limits = c(-1, 60), breaks = seq(0, 59), labels = seq(0, 59)) +\n    geom_text(data = subset(activty_ampm, activity_min_n &gt; 5),\n                        aes(label= activity_min_n), color = \"white\", size = 4, position = position_nudge(y = -1)) +\n    coord_polar(start = 0) +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"#E57A77\", \"#7CA1CC\"),\n                                        labels = c(\"AM\", \"PM\")) +\n    labs(x = \"\", y = \"\",\n             title = \"Most Morning Rides Started Between 12 & 30 Past the Hour &lt;br&gt;\n             Evening Rides More Evenly Spaced Through the Hour\",\n             subtitle = \"*Numbers Correspond to  Minutes of the Hour*\") +\n    theme(legend.text = element_markdown(),\n                axis.text.y = element_blank(),\n                legend.title = element_blank(),\n                plot.title = element_markdown(size = 10, hjust = 0.5),\n                plot.subtitle = element_markdown(hjust = 0.5, size = 9))\n\n\n\n\n\n\n\n\n\nBecause most of my rides were commuting to work or going to morning Danish classes, the hour between 7am-8am was the most common for me to start a ride. The afternoon & evening rides were spaced a bit more between 16 (4pm) to 18 (6pm).\nFor the minute-of-the-hour plot I added a fill aesthetic to differentiate between AM and PM rides. Lots of my AM rides were between 12 & 25 past the hour, which corresponds to when I’d usually leave for work or class. The PM rides were spread out a bit more around the hour.\nBut kind of cool to visualise time this way, right?"
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#models",
    "href": "posts/my-year-of-riding-danishly/index.html#models",
    "title": "My Year of Riding Danishly",
    "section": "Analysis Pt 3 Regression",
    "text": "Analysis Pt 3 Regression\nHaving seen the relationships between the variables in the EDA correlation and scatterplots, I wanted to run some simple OLS regressions to see the effect of the variables on explaining how long a ride took, how many average watts I was producing, and how much energy I burned.\nThe modelsummary package is very helpful for wrapping a bunch of functions together to help display regression statistics.I especially liked the ability to put multiple models together and display results side-by-side for easy comparison. It is a common way to present regression results when you run multiple models, but it’s not that easy to do without lots of manual work. But voila, pass the models into a list, and modelsummary does the rest when you call the object.\n\n\nShow code for regression models\nstrava_models &lt;- strava_data %&gt;%\n    filter(activity_year == 2023) \n\nride_models &lt;- list(\n    \"Time\" = lm(moving_time ~ distance_km + average_speed + elevation_gain + average_grade + average_watts,\n                            data = strava_models),\n    \"Watts\" = lm(average_watts ~ moving_time + distance_km +average_speed + elevation_gain + average_grade + kilojoules,\n                             data = strava_models),\n    \"Kilojoules\" = lm(kilojoules ~ moving_time + distance_km +average_speed + elevation_gain + average_grade + average_watts,\n                                            data = strava_models))\n\nmodelsummary(ride_models, stars = TRUE, gof_omit = \"IC|Adj|F|RMSE|Log\", output = \"gt\")\n\n\n\n\n\n\n\n\n\n\nTime\nWatts\nKilojoules\n\n\n\n\n(Intercept)\n1538.565***\n9.618*\n-45.198***\n\n\n\n(77.665)\n(4.595)\n(7.763)\n\n\ndistance_km\n189.001***\n-3.828***\n1.418*\n\n\n\n(3.969)\n(0.369)\n(0.715)\n\n\naverage_speed\n-121.421***\n6.459***\n-3.803***\n\n\n\n(7.119)\n(0.249)\n(0.667)\n\n\nelevation_gain\n-1.573+\n0.066+\n0.299***\n\n\n\n(0.861)\n(0.037)\n(0.063)\n\n\naverage_grade\n-136.183*\n25.688***\n-21.569***\n\n\n\n(60.772)\n(2.307)\n(4.441)\n\n\naverage_watts\n5.881***\n\n1.089***\n\n\n\n(0.850)\n\n(0.065)\n\n\nmoving_time\n\n-0.021***\n0.087***\n\n\n\n\n(0.003)\n(0.003)\n\n\nkilojoules\n\n0.358***\n\n\n\n\n\n(0.021)\n\n\n\nNum.Obs.\n446\n446\n446\n\n\nR2\n0.968\n0.880\n0.981\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nShow code for regression models\nmodelplot(ride_models, coef_omit = \"Interc\")\n\n\n\n\n\n\n\n\n\nSo what do the models tell us? Each extra kilometer per ride added almost 190 seconds, or 3 minutes, to the ride. Each km/hour or average speed took about 2 minutes off a ride. It’s a bit counter-intuitive however that steeper average grades resulted in shorter rides. I’d need to dig deeper into the data to see why that might be happening.\nFor the “watts” model the largest effect size was average_grade, which makes sense…the steeper the ride the more power I needed to do it. Though oddly, grade had a negative effect on kilojoules burned.\nOverall the models were robust, each explaining well over 80% of variance.\nBut before we’re fully satisfied with the models, let’s do a quick check for colinearity, I used the vif function from the car package and stack from base r in one call to create objects I could then pass to gt for better display. I won’t display all the code, as it runs, but the main calls are:\n\n# create the data obejct for the time model in the list\ncolin_time &lt;- stack(car::vif(ride_models$Time))\n\n# show the results using gt\ncolin_time %&gt;%\n    gt() %&gt;%\n    tab_header(title = \"Colinearity - Time Model\")\n\n\n\n\n\n\n\n\n\n\n\n\nColinearity - Time Model\n\n\nvalues\nind\n\n\n\n\n6.829803\ndistance_km\n\n\n3.419076\naverage_speed\n\n\n5.357780\nelevation_gain\n\n\n1.404015\naverage_grade\n\n\n4.580368\naverage_watts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColinearity - Watts Model\n\n\nvalues\nind\n\n\n\n\n67.991341\nmoving_time\n\n\n34.068257\ndistance_km\n\n\n2.404495\naverage_speed\n\n\n5.636265\nelevation_gain\n\n\n1.166868\naverage_grade\n\n\n31.624813\nkilojoules\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColinearity - Kilojoules Model\n\n\nvalues\nind\n\n\n\n\n31.177648\nmoving_time\n\n\n42.036058\ndistance_km\n\n\n5.679692\naverage_speed\n\n\n5.398402\nelevation_gain\n\n\n1.420039\naverage_grade\n\n\n5.078179\naverage_watts\n\n\n\n\n\n\n\n\n\n\nAs we can see, there’s some problematic colinearity with moving_time and distance_km in the “watts” and “kilojoules” models, so let’s redo models removing the variables with most colinearity. We add the new versions to the list we already created so we can see the output side-by-side.\nWon’t run the plot of estimates as we can see they didn’t change significantly enough. It also shows that though the colinearity numbers were a bit concerning, ultimately the models were not negatively affected.\n\n\nShow code for regression models\nride_models &lt;- list(\n    \"time\" = lm(moving_time ~ distance_km + average_speed + elevation_gain + average_grade + average_watts,\n                            data = strava_models),\n    \"watts\" = lm(average_watts ~ moving_time + distance_km +average_speed + elevation_gain + average_grade + kilojoules,\n                             data = strava_models),\n    \"watts2\" = lm(average_watts ~ distance_km +average_speed + elevation_gain + average_grade,\n                             data = strava_models),\n    \"kilojoules\" = lm(kilojoules ~ moving_time + distance_km +average_speed + elevation_gain + average_grade + average_watts,\n                                            data = strava_models),\n  \"kilojoules2\" = lm(kilojoules ~ moving_time + average_speed + elevation_gain + average_grade + average_watts,\n                                            data = strava_models))\nmodelsummary(ride_models, stars = TRUE, gof_omit = \"IC|Adj|F|RMSE|Log\", output = \"gt\")\n\n\n\n\n\n\n\n\n\n\ntime\nwatts\nwatts2\nkilojoules\nkilojoules2\n\n\n\n\n(Intercept)\n1538.565***\n9.618*\n16.547***\n-45.198***\n-54.034***\n\n\n\n(77.665)\n(4.595)\n(4.277)\n(7.763)\n(6.376)\n\n\ndistance_km\n189.001***\n-3.828***\n-2.537***\n1.418*\n\n\n\n\n(3.969)\n(0.369)\n(0.187)\n(0.715)\n\n\n\naverage_speed\n-121.421***\n6.459***\n7.014***\n-3.803***\n-2.846***\n\n\n\n(7.119)\n(0.249)\n(0.218)\n(0.667)\n(0.461)\n\n\nelevation_gain\n-1.573+\n0.066+\n0.285***\n0.299***\n0.353***\n\n\n\n(0.861)\n(0.037)\n(0.046)\n(0.063)\n(0.057)\n\n\naverage_grade\n-136.183*\n25.688***\n30.120***\n-21.569***\n-20.768***\n\n\n\n(60.772)\n(2.307)\n(3.086)\n(4.441)\n(4.438)\n\n\naverage_watts\n5.881***\n\n\n1.089***\n1.025***\n\n\n\n(0.850)\n\n\n(0.065)\n(0.057)\n\n\nmoving_time\n\n-0.021***\n\n0.087***\n0.093***\n\n\n\n\n(0.003)\n\n(0.003)\n(0.001)\n\n\nkilojoules\n\n0.358***\n\n\n\n\n\n\n\n(0.021)\n\n\n\n\n\nNum.Obs.\n446\n446\n446\n446\n446\n\n\nR2\n0.968\n0.880\n0.782\n0.981\n0.981\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nAnd finally, just for fum. let’s plot the predicted vs actual observations for ride times, watts generated and kilojoules burned.\nFirst we create the data frames….\n\nride_models_time &lt;- data.frame(Predicted = predict(ride_models$time),\n                                                             Observed = strava_models$moving_time)\nride_models_watts &lt;- data.frame(Predicted = predict(ride_models$watts2),\n                                                             Observed = strava_models$average_watts)\nride_models_joules &lt;- data.frame(Predicted = predict(ride_models$kilojoules2),\n                                                                Observed = strava_models$kilojoules)\n\n…now we render the plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere’s not too much distance from the smoothing lines, so in general the model is predicting near enough to what was in the actual ride data.\n\n\nSo there you have it, a year of riding Danishly…on average more than 1 ride per day, more than 7 km per day. Riding to work and school for errands, for fitness. Even though the last ride of the year was both the best and worst, and even though the injuries mean far fewer rides in 2024, I’m looking forward to getting back on the road.\n\n\n\nMy Univega bike enjoying the view at Dragør\nnot my childhood bike but similar\nnot my Panasonic 12-speed but similar\nthe everyday ride here in Copenhagen"
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#introduction",
    "href": "posts/my-year-of-riding-danishly/index.html#introduction",
    "title": "My Year of Riding Danishly",
    "section": "Introduction",
    "text": "Introduction\nI like playing with data and I like riding bikes, so here’s a post (my longest yet, I think) where I look at my own cycling data from the Strava app. I’ve used Strava since 2019. In San Francisco I mostly did workout rides, though there was a period where I rode to work once or twice a week. Since I knew I’d be riding just about everywhere here in Denmark, I made sure to track every ride this year, not just workouts, so I’d have a complete record of rides in 2023 and could see how many rides and kilometers I’d do in a year of riding like a Dane.\nSo let’s explore my year of riding Danishly. We’ll cover how to get data, what you need to do to clean it, and do some quick analysis. In putting this together I learned a bunch of new things, which I’ll explain in more detail as I go. These new things include:\n\nGetting data from my profile section on the Strava webpage and from the Strava API via the rStrava package.\nGetting gt tables to render next to each other by using div classes to create columns.\nUsing functional programming to make it a bit easier to render multiple plots.\nUsing a {. -&gt;&gt; tmp} call to pipe in a temporary dataframe within a data transformation -&gt; ggplot sequence, and call rm(tmp) to remove it from the workflow.\nUsing the modelsummary and car packages to visualize regression model output and plot predicted vs observed values.\n\nBut first…"
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#my-life-with-bikes",
    "href": "posts/my-year-of-riding-danishly/index.html#my-life-with-bikes",
    "title": "My Year of Riding Danishly",
    "section": "My Life with Bikes ",
    "text": "My Life with Bikes \nEver since I was a young boy I’ve loved riding bicycles. My first bike was a birthday present when I was 8 or 9 years old…a yellow and black road runner bike with a banana seat, coaster brakes, similar to this one here. I rode that thing for many years. In 10th grade I saved money from various jobs to get a Panasonic 12-speed. I rode that through grad school, and for some reason didn’t take it with me when I moved cities for work. Though it likely would have been the bike that got stolen instead of the bike I bought after the move.\n\n\n\n\n\n\n\n\n\nnot my childhood bike but similar\n\n\n\n\n\n\n\nnot my Panasonic 12-speed but similar\n\n\n\n\n\nIn San Francisco I bought the used red Univega road bike pictured at the top, and loved that so much I had it shipped on our moves to France and then here to Denmark.\nAmong the many things I was looking forward to when moving back to Copenhagen was finally living in a city with great bicycle infrastructure and culture. After all, US bicycle advocacy organizations like the SF Bike Coalition constantly use Copenhagen and Amsterdam as model cities when pushing for improvements to cycling infrastructure. San Francisco is good, but bike infrastructure here in Denmark has much better support from the government, leading to a much more deeply ingrained bike culture.\nAccording to statistics compiled by Visit Denmark via the Copenhagen Municipality, The Cycling Embassy of Denmark, DTU Center for Transport Analytics and The Ministry of Transportation, Copenhagen has more than 380 km of bike lanes. The average Copenhagener cycles 3 km per day, and combined, all Danish cyclists ride over 8 million km per year.\nDespite the good bike culture here, theft is a thing, especially for decent road bikes. So to prevent theft and the Univega from getting beat up by riding it everywhere everyday, soon after getting settled I got a basic commuter bike to go with the Univega. I found this refursbished beauty at Buddah Bikes in Nørrebro.\n\n\n\nthe everyday ride here in Copenhagen\n\n\nFrom the end of January on, I rode the commuter bike as often as I could…to work, Danish class, running errands, going to shows, visiting friends and family…even family who live 25km north of me. I also did a bunch of workout rides on the Univega, going all over Amager and points north and west. For the year, more than 440 rides.\nThe last ride of the year was quite eventful - on the tail end of a lovely workout ride that was supposed to be 60km, I was hit by a car about 8k from home. Result was a broken leg & shoulder. That meant two surgeries, two weeks in hospital, some physical therapy, and worst of all, no bike rides until at least this summer. On the bright side, socialized medicine FTW; I had excellent care and haven’t once have to haggle with an insurance company trying to deny treatment to boost profits. But that’s perhaps a subject for another post.\nSo anyway, let’s get on with it. The plan is:\n\nPull the Data\n\nShow the code where I pulled the data from the API and cleaned it. It won’t run here and to see it you’ll need to   un-fold it. I’ll be loading the data quietly for use in the analyses.\n\nEDA with DataExplorer\n\nShow and run code for exploratory analysis (EDA) using the DataExplorer package.\n\nEDA with Automated Scatterplots\n\nShow and run code for EDA using Cedric Scherer’s tutorial on automating plots.\n\nTables with gt\n\nShow and run code for the tables, including how to align gt tables next to each other.\n\nCreate Charts to Describe My Ride Data\n\nShow and run the ggplot code to make some pretty charts.\n\nRegression Models\n\nRun a few regression models to explain ride outcomes.\n\n\nFirst we’ll load some packages…\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n# EDA tools\nlibrary(skimr)\nlibrary(DataExplorer) \n# analysis tools\nlibrary(gt) # for making tables\nlibrary(ggtext) # to help make ggplot text look good\nlibrary(patchwork) # to combine plots\nlibrary(modelsummary) # for regression outputs"
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#intro",
    "href": "posts/my-year-of-riding-danishly/index.html#intro",
    "title": "My Year of Riding Danishly",
    "section": "Introduction",
    "text": "Introduction\nI like playing with data and I like riding bikes, so here’s a post (my longest yet, I think) where I look at my own cycling data from the Strava app. I’ve used Strava since 2019. In San Francisco I mostly did workout rides, though there was a period where I rode to work once or twice a week. Since I knew I’d be riding just about everywhere here in Denmark, I made sure to track every ride this year, not just workouts, so I’d have a complete record of rides in 2023 and could see how many rides and kilometers I’d do in a year of riding like a Dane.\nSo let’s explore my year of riding Danishly. We’ll cover how to get data, what you need to do to clean it, and do some quick analysis. In putting this together I learned a bunch of new things, which I’ll explain in more detail as I go. These new things include:\n\nGetting data from my profile section on the Strava webpage and from the Strava API via the rStrava package.\nGetting gt tables to render next to each other by using div classes to create columns.\nUsing functional programming to make it a bit easier to render multiple plots.\nUsing a {. -&gt;&gt; tmp} call to pipe in a temporary dataframe within a data transformation -&gt; ggplot sequence, and call rm(tmp) to remove it from the workflow.\nUsing the modelsummary and car packages to visualize regression model output and plot predicted vs observed values.\n\nBut first…"
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#mybikes",
    "href": "posts/my-year-of-riding-danishly/index.html#mybikes",
    "title": "My Year of Riding Danishly",
    "section": "My Life with Bikes",
    "text": "My Life with Bikes\nEver since I was a young boy I’ve loved riding bicycles. My first bike was a birthday present when I was 8 or 9 years old…a yellow and black road runner bike with a banana seat, coaster brakes, similar to this one here. I rode that thing for many years. In 10th grade I saved money from various jobs to get a Panasonic 12-speed. I rode that through grad school, and for some reason didn’t take it with me when I moved cities for work. Though it likely would have been the bike that got stolen instead of the bike I bought after the move.\n\n\n\n\n\n\n\n\n\nnot my childhood bike but similar\n\n\n\n\n\n\n\nnot my Panasonic 12-speed but similar\n\n\n\n\n\nIn San Francisco I bought the used red Univega road bike pictured at the top, and loved that so much I had it shipped on our moves to France and then here to Denmark.\nAmong the many things I was looking forward to when moving back to Copenhagen was finally living in a city with great bicycle infrastructure and culture. After all, US bicycle advocacy organizations like the SF Bike Coalition constantly use Copenhagen and Amsterdam as model cities when pushing for improvements to cycling infrastructure. San Francisco is good, but bike infrastructure here in Denmark has much better support from the government, leading to a much more deeply ingrained bike culture.\nAccording to statistics compiled by Visit Denmark via the Copenhagen Municipality, The Cycling Embassy of Denmark, DTU Center for Transport Analytics and The Ministry of Transportation, Copenhagen has more than 380 km of bike lanes. The average Copenhagener cycles 3 km per day, and combined, all Danish cyclists ride over 8 million km per year.\nDespite the good bike culture here, theft is a thing, especially for decent road bikes. So to prevent theft and the Univega from getting beat up by riding it everywhere everyday, soon after getting settled I got a basic commuter bike to go with the Univega. I found this refursbished beauty at Buddah Bikes in Nørrebro.\n\n\n\nthe everyday ride here in Copenhagen\n\n\nFrom the end of January on, I rode the commuter bike as often as I could…to work, Danish class, running errands, going to shows, visiting friends and family…even family who live 25km north of me. I also did a bunch of workout rides on the Univega, going all over Amager and points north and west. For the year, more than 440 rides.\nThe last ride of the year was quite eventful - on the tail end of a lovely workout ride that was supposed to be 60km, I was hit by a car about 8k from home. Result was a broken leg & shoulder. That meant two surgeries, two weeks in hospital, some physical therapy, and worst of all, no bike rides until at least this summer. On the bright side, socialized medicine FTW; I had excellent care and haven’t once have to haggle with an insurance company trying to deny treatment to boost profits. But that’s perhaps a subject for another post.\nSo anyway, let’s get on with it. The plan is:\n\nPull the Data\n\nShow the code where I pulled the data from the API and cleaned it. It won’t run here and to see it you’ll need to   un-fold it. I’ll be loading the data quietly for use in the analyses.\n\nEDA with DataExplorer\n\nShow and run code for exploratory analysis (EDA) using the DataExplorer package.\n\nEDA with Automated Scatterplots\n\nShow and run code for EDA using Cedric Scherer’s tutorial on automating plots.\n\nTables with gt\n\nShow and run code for the tables, including how to align gt tables next to each other.\n\nCreate Charts to Describe My Ride Data\n\nShow and run the ggplot code to make some pretty charts.\n\nRegression Models\n\nRun a few regression models to explain ride outcomes.\n\n\nFirst we’ll load some packages…\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n# EDA tools\nlibrary(skimr)\nlibrary(DataExplorer) \n# analysis tools\nlibrary(gt) # for making tables\nlibrary(ggtext) # to help make ggplot text look good\nlibrary(patchwork) # to combine plots\nlibrary(modelsummary) # for regression outputs"
  },
  {
    "objectID": "posts/call-me-by-my-name/index.html#intro",
    "href": "posts/call-me-by-my-name/index.html#intro",
    "title": "Call Me By My Name",
    "section": "Introduction",
    "text": "Introduction\nThe actor Ryan O’Neal died on December 8, 2023. If you’re of a certain age and a film fan you might know him best from a pretty good run in the 1970s of very diverse films from Love Story, to the Peter Bogdonavich gems What’s Up Doc and Paper Moon, and then working with Stanley Kubrick in Barry Lyndon and Richard Attenborough in A Bridge Too Far. Not to mention the under-rated The Driver. If you’re of a certain age but more into celebrity gossip you might know him best from dating Farrah Fawcett and being John McEnroe’s father-in-law.\nIf your name is Ryan, like my friend Ryan Godfrey, you will know him not only from films (Ryan G watches a lot of movies) but for arguably launching the name Ryan into the American babysphere. Pop culture does sometimes influence what we name our kids (do you know anyone with a kid born in the last 10 years who named them Arya? I know at least one), and Ryan (Godfrey’s) post here:\n\n\n\nPopularity of the first name “Ryan”\n\n\n…showed that while correlation may not always be causation, Ryan was mostly a last name until Love Story hit big and made Ryan O’Neal a star. And ironically, Ryan wasn’t even Ryan O’Neal’s given first name…it was Charles."
  },
  {
    "objectID": "posts/call-me-by-my-name/index.html#data",
    "href": "posts/call-me-by-my-name/index.html#data",
    "title": "Call Me By My Name",
    "section": "Getting Baby Name Data",
    "text": "Getting Baby Name Data\nYou can go to the US Social Security Administration’s (SSA) baby names page and check for yourself what are the popular names in any given year, or track the popularity of names over time. You can also download data and do your own analysis. Or you can use the babynames package created by Hadley Wickham of Posit (formerly RStudio). Which is what we’ll do here to look at a few names of interest.\nThe SSA data is, as they note, based on applications for Social Security numbers (SSN). The dataset includes only SSN applciations for people born in the US and excludes any name counts below 5 in any given year.\nSo let’s check out how bay names have trended in the US.\nTo start we’ll load packages to import and clean the data. These are the three packages I use for almost every analysis in r, plus the babynames package.\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(babynames) # pacakge with name data from US Social Security Admin\n\nThe package comes with a few datasets. We will use the babynames set, which has year of birth, sex, name, number, and proportion of people of that sex with that name."
  },
  {
    "objectID": "posts/call-me-by-my-name/index.html#ryans",
    "href": "posts/call-me-by-my-name/index.html#ryans",
    "title": "Call Me By My Name",
    "section": "Counting Ryans",
    "text": "Counting Ryans\nUnlike most of my posts, I won’t be doing any data transformation or fancy visuals…I just want to see how a few names have trended over the years. First, let’s replicate Ryan G’s chart, and look at the name “Ryan”. We’ll start at 1940 since we already know from Ryan G’s chart when it took off.\nBefore we continue, a couple of caveats…\n\nThere was a baby boom from the late 1940s onward, so even with the dip after 1965 there have been many more births and more people getting SSNs than pre WWII. Many older workers born well before Social Security was a thing never applied for numbers so weren’t in the system. So…\nA more nuanced analysis would be proportion of births with the names. But this isn’t meant to be nuanced or statisitically significant, just a quick look at how name popularity has changed over time in the US.\n\nFrom here on in the r code is folded, so click the arrow to the left of the text above the plot to expand the code window.\n\n\nShow code for Ryans since 1940\n# read in babynames data from package as a local set\nnamesdf &lt;- babynames\n\nnamesdf %&gt;%\n    filter(sex == \"M\") %&gt;%\n    filter(name == \"Ryan\") %&gt;%\n    filter(year &gt; 1940) %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    scale_x_continuous(\n        breaks = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010), \n        labels = c(\"1940\", \"1950\", \"1960\", \"1970\", \"1980\", \"1990\", \"2000\", \"2010\")) +\n    scale_y_continuous(\n        breaks = c(0, 5000, 10000, 15000, 20000, 25000, 30000), \n        labels = c(\"0\", \"5000\", \"10000\", \"15000\", \"20000\", \"25000\", \"30000\")) +\n    theme_minimal() +\n    labs(title = \"Ryan becomes a popular boy's name in the 1970s and after\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nSo yes, as Ryan G wrote, the name Ryan had a slight bump from Peyton Place but the big (baby) bump came when Love Story came out in 1970 and continued through the 1970s and 1980s while Ryan O’Neal’s celebrity status was at its peak.\nWe can see wisps of Ryans before 1970, and because the numbers hit almost 30,000 pre-1970 is a bit compressed. So let’s isolate those years…\n\n\nShow code for Ryans before 1970\nnamesdf %&gt;%\n    filter(sex == \"M\") %&gt;%\n    filter(name == \"Ryan\") %&gt;%\n    filter(year &lt; 1970) %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    labs(title = \"Fewer than 200 Ryans prior to late 1950s\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nFewer than 200 up until the late 1950s, then modest increases after O’Neal starred in the night-time sopa opera Peyton Place from 1964 to 1969.\nBut wait, we’re only looking at boys named Ryan. Do you know any women named Ryan? You might…\n\n\nShow code for girl Ryans since 1940\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(year &gt; 1940) %&gt;%\n    filter(name == \"Ryan\") %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"gold\") +\n    theme_minimal() +\n    labs(title = \"Ryan...not just for boys\", x= \"\", y = \"\")"
  },
  {
    "objectID": "posts/call-me-by-my-name/index.html#tatums",
    "href": "posts/call-me-by-my-name/index.html#tatums",
    "title": "Call Me By My Name",
    "section": "Counting Tatums",
    "text": "Counting Tatums\nThinking about O’Neal’s effect on the name Ryan, I got to thinking about the name of his daughter Tatum. She won an Oscar for Paper Moon so I wondered if the name became more popular in the early-mid 1970…\n\n\nShow code for Tatums\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(name == \"Tatum\") %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    labs(title = \"More Tatums in the 1970s, lots more in the 2000s\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n…and sure enough it did. Not to the same extent as Ryan, but a definite pop from 1973 on. After Paper Moon she was in The Bad News Bears, International Velvet and Little Darlings. So like her dad had a run of fame through the early 1980s. But why the jump in the name’s popularity in the late 1990s and through the 2000s? Her very public relationship and marriage to tennis pro John McEnroe? She didn’t do much film or TV work until the early-mid 2000s. So that resurgence, combined with a nostaliga bump? Some other famous Tatum I’m overlooking?"
  },
  {
    "objectID": "posts/call-me-by-my-name/index.html#jennifers",
    "href": "posts/call-me-by-my-name/index.html#jennifers",
    "title": "Call Me By My Name",
    "section": "Counting Jennifers",
    "text": "Counting Jennifers\nAll this name trending chatter reminded me of my own firmly held belief which is that it’s very statistically likely a Gen X man or woman in the US will have dated at least one Jennifer in their lives. I’ve dated more than one. It’s almost unavoidable. Why? Well let’s look at birth numbers…\n\n\nShow code for Jennifers\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(name == \"Jennifer\") %&gt;%\n    filter(year &gt;= 1940) %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    scale_x_continuous(\n        breaks = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010), \n        labels = c(\"1940\", \"1950\", \"1960\", \"1970\", \"1980\", \"1990\", \"2000\", \"2010\")) +\n    theme_minimal() +\n    labs(title = \"The Love Story effect on Jennifers\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nJennifer had been on the rise from the 1940s onward, slow and steady. But boomed around 1970. And hey, the female lead in Love Story was named Jenny. Coincidence? Or was Love Story more of a cultural juggernaut than we give it credit 50+ years on? I’d say sorry, but…\nLately the name has fallen out of favor. And it’s striking how normally distributed the curve is, given the timeframe.\nAnd finally, in the interest of marital harmony, and because I showed these charts to my wife Ingrid when I made them just for fun, and then she asked what about her name…well I can’t leave her chart out of the post, right? So here’s the name Ingrid over time.\n\n\nShow code for Ingrids\nnamesdf %&gt;%\n    filter(sex == \"F\") %&gt;%\n    filter(name == \"Ingrid\") %&gt;%\n    ggplot(aes(x = year, y = n)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    labs(title = \"Ingrid fluctuates over time\", x= \"\", y = \"\")\n\n\n\n\n\n\n\n\n\nSo there you go. There’s much more to do…download the most recent sets, match names to regions and states. Look at other countries either through r packages like ukbabynames or use packages or API calls to national statistical services."
  },
  {
    "objectID": "posts/r-to-tableau-and-quarto/index.html#intro",
    "href": "posts/r-to-tableau-and-quarto/index.html#intro",
    "title": "r to Tableau, then show it in Quarto",
    "section": "Introduction",
    "text": "Introduction\nTableau has long been on my list to add to my data analysis skillset. I’ve tried in fits and starts over the years, but hadn’t been able to sustain it due to work and life.\nI had hoped to get to it in the fall, after my CIEE contract ended and I knew I’d be without work for a bit. But four Danish classes a week and doing a few r projects for the blog took up some time. Then in late December I was on the wrong end of a bike-car collision (more on that later in bike and healthcare-related posts), and after I got back from 2 weeks in the hospital and started to feel better, I decided it was time to take it on.\nI started by replicating the superstore data exec dashboard, then wanted to do an original project. Initially the idea was to do a dashboard with some other data, and see if I could embed it in a blog post using Quarto. While working on that I somehow got the idea I wanted to make the visualisation be a paramterized user-choice analysis.\nI wasn’t nuts about the older data suggested by Tableau, and given I’m using the free Tableau Public product and I don’t yet know how to connect to data and clean in Tableau, I decided on this analysis workflow for this 1st project:\n\nUse r to source and clean the data, export to CSV.\nBuild a visualisation in Tableau, on-line\nWrite a blog post wherein I attempt to embed the Tableau viz.\n\nI’ve seen the embedding done in Jon Boeckenstedt’s college enrollment trends blog (I think he uses blogger) and hope it works here. (spoiler alert…it does)"
  },
  {
    "objectID": "posts/r-to-tableau-and-quarto/index.html#data",
    "href": "posts/r-to-tableau-and-quarto/index.html#data",
    "title": "r to Tableau, then show it in Quarto",
    "section": "Getting & cleaning some data",
    "text": "Getting & cleaning some data\nSo first, let’s source and clean some data. I’ve long wanted to try the worldfootballr package, so we’ll use that to get data for the English Premier League’s 2022-23 season. I ultimately want a dataset with total points and goals, expected points and goals, actual minus expected, and these items also for home and away matches.\n\n\ncode to get and clean the data and write to csv\n### template for r analysis work. save as w/ appropriate name to project directory\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\n# EDA tools\nlibrary(DataExplorer)\nlibrary(explore)\nlibrary(skimr)\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n# get 2022-23 match result data from understat via worldfootballr\nepl_results2223 &lt;- understat_league_match_results(league = \"EPL\", season_start_year = 2022) %&gt;%\n    select(-isResult)\n\n# because of the way the data comes, need to separate out into home and away dfs\nepl_2223_byteam_home &lt;- epl_results2223 %&gt;%\n    select(home_team, home_goals, away_goals, home_xG, away_xG, forecast_win, forecast_draw, forecast_loss) %&gt;%\n    group_by(home_team) %&gt;%\n    mutate(ga_home = away_goals) %&gt;%\n    mutate(xga_home = away_xG) %&gt;%\n    # actual points using match result\n    mutate(points_home = case_when(home_goals &gt; away_goals ~ 3,\n                                                                 home_goals &lt; away_goals ~ 0,\n                                                                 home_goals == away_goals ~ 1)) %&gt;%\n    # expected points using match probability...it's a crude measure but works for our purposes\n    mutate(points_exp_home = case_when(((forecast_win &gt; forecast_draw) & (forecast_win &gt; forecast_loss)) ~ 3,\n                                                                         ((forecast_draw &gt; forecast_win) & (forecast_draw &gt; forecast_loss)) ~ 1,\n                                                                         TRUE ~ 0)) %&gt;%\n    # create sum for each column and bind that row to the df\n    mutate(Total = rowSums(across(where(is.numeric)))) %&gt;%\n    bind_rows(summarize(., description.y = \"Total\", across(where(is.numeric), sum))) %&gt;%\n    # select only the row with totals\n    filter(description.y == \"Total\") %&gt;%\n    # create a few more variables\n    mutate(goals_minus_xg_home = home_goals - home_xG) %&gt;%\n    mutate(ga_minus_xga_home = ga_home - xga_home) %&gt;%\n    mutate(points_minus_points_exp_home = points_home - points_exp_home) %&gt;%\n    ungroup() %&gt;%\n    select(team = home_team, goals_home = home_goals, xg_home = home_xG, goals_minus_xg_home,\n                 ga_home, xga_home, ga_minus_xga_home,\n                 points_home, points_exp_home, points_minus_points_exp_home)\n\n# the same steps but for away team...note the differences in coding actual & expected points\nepl_2223_byteam_away &lt;- epl_results2223 %&gt;%\n    select(away_team, home_goals, away_goals, away_xG, home_xG, forecast_win, forecast_draw, forecast_loss) %&gt;%\n    group_by(away_team) %&gt;%\n    mutate(ga_away = home_goals) %&gt;%\n    mutate(xga_away = home_xG) %&gt;%\n    mutate(points_away = case_when(home_goals &lt;  away_goals ~ 3,\n                                                                 home_goals &gt; away_goals ~ 0,\n                                                                 home_goals == away_goals ~ 1)) %&gt;%\n    mutate(points_exp_away = case_when(((forecast_loss &gt; forecast_draw) & (forecast_win &lt; forecast_loss)) ~ 3,\n                                                                         ((forecast_draw &gt; forecast_win) & (forecast_draw &gt; forecast_loss)) ~ 1,\n                                                                         TRUE ~ 0)) %&gt;%\n    mutate(Total = rowSums(across(where(is.numeric)))) %&gt;%\n    bind_rows(summarize(., description.y = \"Total\", across(where(is.numeric), sum))) %&gt;%\n    filter(description.y == \"Total\") %&gt;%\n    mutate(goals_minus_xg_away = away_goals - away_xG) %&gt;%\n    mutate(ga_minus_xga_away = ga_away - xga_away) %&gt;%\n    mutate(points_minus_points_exp_away = points_away - points_exp_away) %&gt;%\n    ungroup() %&gt;%\n    select(team = away_team, goals_away = away_goals, xg_away = away_xG, goals_minus_xg_away,\n                 ga_away, xga_away, ga_minus_xga_away,\n                 points_away, points_exp_away, points_minus_points_exp_away)\n\n## bring in league table info...note, this is from FB Ref, and XG formula is different from understat. FB Ref uses opta,\n ## understat has their own formula. we'll use the understat xG in the final dataset\nepltable_2223 &lt;- fb_season_team_stats(country = \"ENG\", gender = \"M\", season_end_year = \"2023\", tier = \"1st\",\n                                                                            stat_type = \"league_table\") %&gt;%\n    rename(team = Squad) %&gt;%\n    # fix a few team names that FBRef has in a different format\n    mutate(team = case_when(team == \"Nott'ham Forest\" ~ \"Nottingham Forest\",\n                                                    team == \"Manchester Utd\" ~ \"Manchester United\",\n                                                    team == \"Newcastle Utd\" ~ \"Newcastle United\",\n                                                    TRUE ~ team))\n\n## merge all together, fixing team names, creating new fields.\nepl_2223_byteam_all &lt;- epl_2223_byteam_home %&gt;%\n    merge(epl_2223_byteam_away) %&gt;%\n    mutate(team = case_when(team == \"Wolverhampton Wanderers\" ~ \"Wolves\",\n                                                     team == \"Leeds\" ~ \"Leeds United\",\n                                                     team == \"Leicester\" ~ \"Leicester City\",\n                                                     TRUE ~ team)) %&gt;%\n    merge(epltable_2223) %&gt;%\n    mutate(goals_total = goals_home + goals_away) %&gt;%\n    mutate(xg_total = xg_home + xg_away) %&gt;%\n    mutate(goals_minus_xg_total = goals_total - xg_total) %&gt;%\n    mutate(ga_total = ga_home + ga_away) %&gt;%\n    mutate(xga_total = xga_home + xga_away) %&gt;%\n    mutate(ga_minus_xga_total = ga_total - xga_total) %&gt;%\n    mutate(points_total = points_home + points_away) %&gt;%\n    mutate(points_exp_total = points_exp_home + points_exp_away) %&gt;%\n    mutate(points_minus_points_exp_total = points_total - points_exp_total) %&gt;%\n    select(team, rank = Rk, W:L, Pts, Pts.MP, points_total, points_home, points_away,\n                 points_exp_total, points_exp_home, points_exp_away,\n                 points_minus_points_exp_total, points_minus_points_exp_home, points_minus_points_exp_away,\n                 goals_total, goals_home, goals_away,\n                 xg_total, xg_home, xg_away, goals_minus_xg_total, goals_minus_xg_home, goals_minus_xg_away,\n                 ga_total, ga_home, ga_away, xga_total, xga_home, xga_away, ga_minus_xga_total,\n                 GF:GD, xG:xGD.90)\n\n# write to a CSV we'll import to Tableau\nwrite_csv(epl_2223_byteam_all, \"~/Data/football data files/epl_2223_byteam.csv\")"
  },
  {
    "objectID": "posts/r-to-tableau-and-quarto/index.html#tableau",
    "href": "posts/r-to-tableau-and-quarto/index.html#tableau",
    "title": "r to Tableau, then show it in Quarto",
    "section": "Embed the Tableau workbook",
    "text": "Embed the Tableau workbook\nOk…CSV written, uploaded to the Tableau workbook. I used this guide to build the parameters, and used other tips from the superstore exec summary how-to vids.\nTo embed the Tableau workbook here I copied the embed code and placed in a code chunk with {=html} after the first three tick marks, then the embed code below the html tag.\n                   \nYou can play with the data here on this page or you can go to the book on my Tableau profile. If the data tell any story it’s that teams who maximize chances (convert xG, expected goals) to actual goals, have a better chance of turning expected points into actual points. Not groundbreaking analysis to be sure.\nIt’s not necessarily a final version - there are more variables to add to expand user choice, but it’s enough now to get the main point, which was parameterized user choice.\nMore to Tableau visualisations to come as I get more familiar with it and want to try new chart types and visualisations.\nBut hoooray, I proved the concept! Injury-induced downtime FTW!"
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html#intro",
    "href": "posts/exploring-happiness-eda/index.html#intro",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "Introduction",
    "text": "Introduction\nWhat makes us happy?\nA subjective question to be sure. The things that make me happy might not do much for you, and what brings you happiness might not work for me. To each their own? Or are there some basic things that boost our collective happiness, things most of us agree are good?\nThere have been attempts to answer the question in a universal way, by focusing on broad quality of life measures and activities. A perfect question for social scientists to dig into, and so they have.\nAmong the most referenced measures is the World Happiness Report. A yearly multi-chapter report published by Sustainable Development Solutions Network, using responses from the Gallup World Poll.\nFor this post (and at least one, maybe more to come) I want to dig into the data that has been made available. Every year the WHR research team releases data for chapter two, which has composite scores by country based on the ladder score question, along with related questions from the survey. They add logged GDP and other data in the full chapter report. GDP is made available in the chapter data for release."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html#data",
    "href": "posts/exploring-happiness-eda/index.html#data",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "The Data",
    "text": "The Data\n The Chapter 2 data has been consistently offered for download for years now. There are two datasets:\n\nData for Figure 1 includes the three three-year rolling average of the happiness ladder question (a 0-10 scale, described in the statistical appendix) along with related measures, aggregated by country. We also get the ladder score of a hypothetical dystopian country.\nData for Table 1 has the output of the OLS regression model to predict each country’s ladder score.\n\nThe Figure 1 data also includes OLS output in form of the percent of each country’s happiness score that could be attributed to the component variables. Another column in the Figure 1 set includes a column with the dystopia score plus the country’s residual of the actual and predicted ladder scores. In the data loading code below you’ll see that I added a column separating out the residual.\nBoth the report’s statisitcal appendix (downloads a pdf) and on-line version of Chapter 2 explain everything in more detail so I won’t repeat it here.\nI’ll be using r for all steps of the workflow; importing the data, cleaning, analysing, and visualising. So let’s go…\nFor this post, I want to focus on Exploratory Data Analysis (EDA). It’s the part of the analytical process where you get a broad overview of the data…look for things that need cleaning, look for distributions and relationships. In the past I’d build my own charts and tables, and that took quite a lot of time and mental energy.\nThankfully there are packages to speed up the work. So to get a quick look at this first set of WHR data, I’ll test-drive the EDA packages DataExplorer by Boxuan Cui, and Roland Krasser’s explorer.\nTo start with let’s load packages to import and clean the data. These are the three packages I use for almost every analysis in r.\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\nTo get the WHR data into RStudio you can go two ways. First is to download the sheet to your local machine and read in:\n\n# read in WHR data, fix country names for later merges\nwhr23_fig2_1a &lt;- readxl::read_excel(\"yourfilepath/DataForFigure2.1WHR2023.xls\") %&gt;%\n    clean_names() %&gt;%\n    as_tibble() %&gt;%\n    mutate(residual = dystopia_residual - ladder_score_in_dystopia) %&gt;%\n    select(-residual_is_dystopia_minus_dystopia_plus_residual) %&gt;%\n    mutate(whr_year = 2023) %&gt;%\n    mutate(country_name = case_when(\n        country_name == \"Czechia\" ~ \"Czech Republic\",\n        country_name == \"State of Palestine\" ~ \"Palestinian Territories\",\n        country_name ==  \"Turkiye\" ~ \"Turkey\",\n        TRUE ~ country_name))\n\nYou could also use curl to download straight from the WHR page:\n\nlibrary(readxl)\nurl1 &lt;- \"https://happiness-report.s3.amazonaws.com/2023/DataForFigure2.1WHR2023.xls\"\ndestfile1 &lt;- \"DataForFigure2_1WHR2023.xls\"\ncurl::curl_download(url1, destfile1)\nwhr23_fig2_1a &lt;- readxl::read_excel(destfile1) \n## %&gt;% (and then the same cleaning steps shown above)\n\nThe data from the WHR does not include the world region for each country, something I will want for further analysis. I’m not sure what the source is for the region grouping they are using. I found a file with a region column on Kaggle for the 2021 survey, so downloaded that and merged on country name.\n\n# read in kaggle file with region names\nctreg &lt;- readr::read_csv(\"yourfilepath/world-happiness-report-2021.csv\") %&gt;%\n    as_tibble() %&gt;%\n    clean_names() %&gt;%\n    select (country_name, region = regional_indicator)\n\n# join to whr23 on country, add missing region for Congo (Kinshasa)\nwhr23_fig2_1 &lt;- whr23_fig2_1a %&gt;%\n    merge(ctreg, all = TRUE) %&gt;%\n    as_tibble() %&gt;%\n    select(country_name, region, whr_year, everything()) %&gt;%\n    mutate(region = ifelse(\n        country_name == \"Congo (Kinshasa)\", \"Sub-Saharan Africa\", region))\n\nAnother way to do it is to hard code them. I had to go back to the 2018 report to find a list.\nRegardless, we now have a dataset, so let’s explore it."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html#dataexplorer",
    "href": "posts/exploring-happiness-eda/index.html#dataexplorer",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "EDA with DataExplorer - Overview",
    "text": "EDA with DataExplorer - Overview\nLet’s start with DataExplorer. The create_report() function runs the full set of native reports and outputs to a directory of your choosing with a filename of your choosing. But for a review I want to go through a few of the individual report elements.\nintroduce() outputs a table showing rows, columns and other information. If you want to see this information in chart form, plot_intro() and plot_missing() do that.\n\n## DataExplorer for EDA\nlibrary(DataExplorer) # EDA tools\n\n# summary of completes, missings\nintroduce(whr23_fig2_1)\n#&gt; # A tibble: 1 × 9\n#&gt;    rows columns discrete_columns continuous_columns all_missing_columns\n#&gt;   &lt;int&gt;   &lt;int&gt;            &lt;int&gt;              &lt;int&gt;               &lt;int&gt;\n#&gt; 1   137      35               10                 25                   0\n#&gt; # ℹ 4 more variables: total_missing_values &lt;int&gt;, complete_rows &lt;int&gt;,\n#&gt; #   total_observations &lt;int&gt;, memory_usage &lt;dbl&gt;\nplot_intro(whr23_fig2_1)\n\n\n\n\n\n\n\nplot_missing(whr23_fig2_1)\n\n\n\n\n\n\n\n\nThere are hardly any missing values in the set, which will make analysis easier."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html#dataexplorer2",
    "href": "posts/exploring-happiness-eda/index.html#dataexplorer2",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "EDA with DataExplorer - Distributions",
    "text": "EDA with DataExplorer - Distributions\nOk, so how about the distribution of our variables? plot_bar() takes all discrete variables and plot_histogram() runs for the continuous variables. Depending on how many columns of each type your dataset has you will need to play with the nrow and ncol options so that everything renders to the RStudio plot column. For the histograms you can change the number of binsm change the x-axis to log or some other option (the default is continuous). You can also customize the look a bit with passing arguments to the ggtheme = and theme_config() functions.\n\nplot_bar(whr23_fig2_1)\n\n\n\n\n\n\n\n\nFor the discrete variable bar charts, for this dataset there isn’t much to look at. But for a dataset with demographic variables, geographic places, etc. this would be very helpful.\n\nplot_histogram(whr23_fig2_1, nrow = 5L)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histograms render in alpha order of the variable name, not order in the dataset. When the plots render, we look through them to see if there are any unusual skews or other things we want to watch out for depending on the type of analyses to be run. In this case there are a few solitary bars in some of the histograms, like values above 0.4 in the generosity column. But nothing too skewed so that if we took a closer look at distributions by way of means or interquartiles that we’d be too worried."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html#dataexplorer3",
    "href": "posts/exploring-happiness-eda/index.html#dataexplorer3",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "EDA with DataExplorer - Correlations",
    "text": "EDA with DataExplorer - Correlations\nNow for my favorite part of this package, a correlation matrix! We’ll run plot_correlation() without the year, dystopia ladder score, and whiskers and make sure to only do continuous. We can also adjust things like the type of correlation (default is pearson), the size of the coefficient labels in the chart and other elements.\n\n## correlation...remove some columns, clean NA in pipe, continuous only, change text size\nwhr23_fig2_1 %&gt;%\n    select(-whr_year, -ladder_score_in_dystopia, -upperwhisker, -lowerwhisker, -year,\n                 -ginifill, -gini, -gini_latest, -gini_avg) %&gt;%\n    filter(!is.na(residual)) %&gt;%\n    plot_correlation(type = \"continuous\", geom_text_args = list(\"size\" = 3))\n\n\n\n\n\n\n\n\nThe way my brain works is to look for visual patterns and relationships. So a correlation matrix like this is perfect to give me a broad view of how the continuous variables relate to each other. The matrix heatmap returns positive relationships in red, negative in blue. I first want to look at the relationships of the component variables to the ladder score, and we see positive associations for everything except for perception of corruption, which makes sense because you’d likely report being less happy if you lived in a corrupt country.\nThe weakest association is between generosity, which comes from a question asking “Have you donated to charity in the past month?” So while donations to charity are a good thing, they don’t necessarily move the needle on happiness. At least not in the aggregate. But maybe by country or region? Something to take a look at later. This is why we do EDA…\nWe also see that we could have run this without the “explained by…” columns as they have the same coefficients as the component variables."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html#dataexplorer4",
    "href": "posts/exploring-happiness-eda/index.html#dataexplorer4",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "EDA with DataExplorer - Scatterplots",
    "text": "EDA with DataExplorer - Scatterplots\nAs much as I love a correlation matrix, I love scatterplots even more. I clearly have a thing for patterns and relationships. The plot_scatterplot function returns plots for all the variables you pass along, against the one you call in the by = argument. Here we want to see the association between the ladder score and component variables from Chapter 2.\n\nplot_scatterplot(\n    whr23_fig2_1 %&gt;% select(ladder_score, social_support:perceptions_of_corruption, dystopia_residual, residual), \n    by = \"ladder_score\", nrow = 3L)\n\n\n\n\n\n\n\n\nWe know from the correlation heatmap that we don’t need the “explained_by_*” variables as they were redundant to the component variables. The x/y distributions here confirm what we saw in the correlations, including the slightly negative relationship between the ladder score and perceptions of corruption, and that generosity was a weaker relationship.\nWhile the scatterplot function does allow for some plot customization, one I tried but couldn’t get to work was using the geom_point_args() call to color the dots by region, like this using ggplot:\n\nwhr23_fig2_1 %&gt;%\n    ggplot(aes(x = perceptions_of_corruption, y = ladder_score)) +\n    geom_point(aes(color = region)) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThere are a few other functions offered to do principal component analysis and qq (quantile-quantile) plots, but they did not help much with this dataset.\nOverall there are plenty of helpful features in DataExplorer that make it worthwhile to use for EDA. I’d like the ability to color scatterplots by a discrete variable, or to facet the histograms or scatterplots, but as is, a robust EDA tool."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html#explore",
    "href": "posts/exploring-happiness-eda/index.html#explore",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "EDA with Explore - Interactive EDA",
    "text": "EDA with Explore - Interactive EDA\nThe explore package  The best function here is explore(dataset), which launches a shiny window with four tabs.\nThe “overview” tab shown here, displays a table with mean, min, max, and unique & missing value counts by variable.\n\n\n\n\n\nThe “variable” tab allows you to explore variables on their own… \n…or in relation to one another.  You not only get a chart appropriate to the variable type (categoricals with bars, continuous with area plots), but when you target against another variable you get a scatterplot.\nThe explain tab runs a decision tree against a target variable, and the data tab displays the entire dataset as a table, all rows and all columns. So before launching this you may want to be mindful of running it against too large a dataset.\nIf you don’t want to launch the shiny app, you can output a report in html…\n\n## creates html report of all individual reports\nwhr23_fig2_1 %&gt;%\n    report(output_dir = \"~/data/World Happiness Report\")\n\n…or run select features individually, depending on what you need….\n\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, social_support:perceptions_of_corruption,\n                 explained_by_log_gdp_per_capita:residual) %&gt;%\n    describe_all() \n#&gt; select: dropped 21 variables (country_name, iso3c, region, region_whr, whr_year, …)\n#&gt; # A tibble: 14 × 8\n#&gt;    variable                          type     na na_pct unique   min  mean   max\n#&gt;    &lt;chr&gt;                             &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 ladder_score                      dbl       0    0      137  1.86  5.54  7.8 \n#&gt;  2 social_support                    dbl       0    0      137  0.34  0.8   0.98\n#&gt;  3 healthy_life_expectancy           dbl       1    0.7    137 51.5  65.0  77.3 \n#&gt;  4 freedom_to_make_life_choices      dbl       0    0      137  0.38  0.79  0.96\n#&gt;  5 generosity                        dbl       0    0      137 -0.25  0.02  0.53\n#&gt;  6 perceptions_of_corruption         dbl       0    0      137  0.15  0.73  0.93\n#&gt;  7 explained_by_log_gdp_per_capita   dbl       0    0      137  0     1.41  2.2 \n#&gt;  8 explained_by_social_support       dbl       0    0      137  0     1.16  1.62\n#&gt;  9 explained_by_healthy_life_expect… dbl       1    0.7    137  0     0.37  0.7 \n#&gt; 10 explained_by_freedom_to_make_lif… dbl       0    0      137  0     0.54  0.77\n#&gt; 11 explained_by_generosity           dbl       0    0      137  0     0.15  0.42\n#&gt; 12 explained_by_perceptions_of_corr… dbl       0    0      137  0     0.15  0.56\n#&gt; 13 dystopia_residual                 dbl       1    0.7    137 -0.11  1.78  2.95\n#&gt; 14 residual                          dbl       1    0.7    137 -1.89  0     1.18\n\n\nwhr23_fig2_1 %&gt;%\n    explore(ladder_score)\n\n\n\n\n\n\n\n\nThe main vignette and reference guide are both very comprehensive, so no need to repeat too much here. But there are some fun features like decision trees, and lots of flexibilty to explore multiple variables in relation to each other."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html#skimr",
    "href": "posts/exploring-happiness-eda/index.html#skimr",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "EDA with Skimr",
    "text": "EDA with Skimr\nThen there is skimr, one of the first EDA packages that I remember seeing. If there’s a feature I like most, it’s the basic skim() function, which returns means & other distributions, as well as little histograms.\n\n\n\nlibrary(skimr)\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, social_support:perceptions_of_corruption, residual) %&gt;%\n    skim()\n#&gt; select: dropped 28 variables (country_name, iso3c, region, region_whr, whr_year, …)\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n137\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nladder_score\n0\n1.00\n5.54\n1.14\n1.86\n4.72\n5.68\n6.33\n7.80\n▁▂▆▇▃\n\n\nsocial_support\n0\n1.00\n0.80\n0.13\n0.34\n0.72\n0.83\n0.90\n0.98\n▁▂▃▆▇\n\n\nhealthy_life_expectancy\n1\n0.99\n64.97\n5.75\n51.53\n60.65\n65.84\n69.41\n77.28\n▃▃▇▇▂\n\n\nfreedom_to_make_life_choices\n0\n1.00\n0.79\n0.11\n0.38\n0.72\n0.80\n0.87\n0.96\n▁▁▃▇▇\n\n\ngenerosity\n0\n1.00\n0.02\n0.14\n-0.25\n-0.07\n0.00\n0.12\n0.53\n▃▇▅▁▁\n\n\nperceptions_of_corruption\n0\n1.00\n0.73\n0.18\n0.15\n0.67\n0.77\n0.85\n0.93\n▁▁▁▅▇\n\n\nresidual\n1\n0.99\n0.00\n0.50\n-1.89\n-0.22\n0.07\n0.30\n1.18\n▁▂▅▇▂\n\n\n\n\n\n\n\nIt’s especially helpful on small and medium-sized datasets, to get a quick overview and look for outliers."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html#summary",
    "href": "posts/exploring-happiness-eda/index.html#summary",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "Happiness data takeaways",
    "text": "Happiness data takeaways\nUsing these packages for EDA on the happiness data, we learned that:\n\nThere are not many missing values in the data.\nAll of the component variables except generosity have strong positive correlations with the happiness score.\nPerception of corruption has, as expected, a negative association with happiness.\n\nWe also came away wanting to know a bit more about differences by region, so that’s a good starting point for the next post, which will be a slightly deeper dive into the data."
  },
  {
    "objectID": "posts/exploring-happiness-eda/index.html#conclusion",
    "href": "posts/exploring-happiness-eda/index.html#conclusion",
    "title": "Exploring Happiness - Part 1…EDA",
    "section": "Conclusion",
    "text": "Conclusion\nThere is no one perfect EDA package that suits all needs for any dataset. DataExplorer has some robust features, particularly in this usecase the correlation heatmap and the scatterplots. I loved the native reports and shiny app in explorer. I had planned to look at correlationfunnel, but it’s only really suited to a use-case with binary outcomes such as customer sign-up, churn, employee retention, college admissions outcomes (admits, yield), student success outcomes like retention and graduation. I’ll have to find another dataset to try that package. Doing these package test-drives reminded me that skimr is also very useful.\nGoing forward I’ll be setting up a more deliberate EDA workflow using parts of each of these packages, depending on the size of the dataset and the main questions I’d have of the data."
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#eda3",
    "href": "posts/my-year-of-riding-danishly/index.html#eda3",
    "title": "My Year of Riding Danishly",
    "section": "EDA with Scatterplots",
    "text": "EDA with Scatterplots\nWhile DataExplorer does have functionality for scatterplots, each call only allows for one comparison y-axis variable. I could do multiple calls within the package but I recently came across Cedric Scherer’s post on automating plot outputs using functional programming and wanted to give that approach a try, this project being perfect for a test. I ended up copying one of his scatterplot functions with no modification for use here. (I’m still a bit weak on functional programming so didn’t want to mess anything up and have to spend time debugging)\nThis first bit of code creates the plot function:\n\n\nShow code for creating automated plot function\n## plot template as function\nplot_scatter_lm &lt;- function(data, var1, var2, pointsize = 2, transparency = .5, color = \"\") {\n\n    ## check if inputs are valid\n    if (!exists(substitute(data))) stop(\"data needs to be a data frame.\")\n    if (!is.data.frame(data)) stop(\"data needs to be a data frame.\")\n    if (!is.numeric(pull(data[var1]))) stop(\"Column var1 needs to be of type numeric, passed as string.\")\n    if (!is.numeric(pull(data[var2]))) stop(\"Column var2 needs to be of type numeric, passed as string.\")\n    if (!is.numeric(pointsize)) stop(\"pointsize needs to be of type numeric.\")\n    if (!is.numeric(transparency)) stop(\"transparency needs to be of type numeric.\")\n    if (color != \"\") { if (!color %in% names(data)) stop(\"Column color needs to be a column of data, passed as string.\") }\n\n    g &lt;-\n        ggplot(data, aes(x = !!sym(var1), y = !!sym(var2))) +\n        geom_point(aes(color = !!sym(color)), size = pointsize, alpha = transparency) +\n        geom_smooth(aes(color = !!sym(color), color = after_scale(prismatic::clr_darken(color, .3))),\n                                method = \"lm\", se = FALSE) +\n        theme_minimal() +\n        theme(panel.grid.minor = element_blank(),\n                    legend.position = \"top\")\n\n    if (color != \"\") {\n        if (is.numeric(pull(data[color]))) {\n            g &lt;- g + scale_color_viridis_c(direction = -1, end = .85) +\n                guides(color = guide_colorbar(\n                    barwidth = unit(12, \"lines\"), barheight = unit(.6, \"lines\"), title.position = \"top\"\n                ))\n        } else {\n            g &lt;- g + scale_color_brewer(palette = \"Set2\")\n        }\n    }\n\n    return(g)\n}\n\n\nAnd here we call the plot_scatter_lm function. In this first instance I’m plotting some variables against ride distance.\n\n## data extract\nstrava_activities_rides &lt;- strava_data %&gt;%\n    filter(activity_year == 2023)\n\n## 1st plot call - distance as y axis\npatchwork::wrap_plots(\n    map2(c(\"elapsed_time\", \"moving_time\", \"average_speed\",\"average_watts\", \"calories\", \"kilojoules\"), \n             c(\"distance_km\", \"distance_km\", \"distance_km\", \"distance_km\", \"distance_km\", \"distance_km\"), \n             ~plot_scatter_lm(data = strava_activities_rides, var1 = .x, var2 = .y, pointsize = 3.5) + \n                theme(plot.margin = margin(rep(15, 4)))))\n\n\n\n\n\n\n\n\nThis first set of plots confirms what we saw in the correlation heatmap while also displaying how the ride data points are distributed. We see the positive and almost 1:1 relationships between distance and both time measures, elapsed and moving. Elapsed time is the total time from when you start the ride until you end and save it in the app. Moving time is Strava’s calculation of how much time you actually spent in motion. Strava provides ride time in seconds, which is best for this kind of plotting, and of course you can use lubridate to convert it to hours & minutes.\nWe also see the negative association with watts that we saw in the correlations. I’m making a note to take a closer look at how much an effect watts has later on in the regression section.\nNote the outlier ride of 60km and an elapsed time of more than 15,000 seconds. That was the ride where I was hit by the car…the elapsed time ended up at 18,242 seconds, or more than 5 hours. Moving time was only 2 hours & 44 minutes. I guess turning off the app and saving the ride wasn’t top of my to-do list while laying on the street with broken bones.\nThis next group of plots has moving time as the comparison variable. I didn’t plot distance on the x-axis as we saw that relationship already.\n\n\nshow code for moving time scatterplots\npatchwork::wrap_plots(\n    map2(c(\"average_speed\", \"elevation_gain\", \"average_grade\", \"average_watts\", \"calories\", \"kilojoules\"), \n             c(\"moving_time\", \"moving_time\", \"moving_time\", \"moving_time\", \"moving_time\", \"moving_time\"), \n             ~plot_scatter_lm(data = strava_activities_rides, var1 = .x, var2 = .y, pointsize = 3.5) + \n                theme(plot.margin = margin(rep(15, 4)))))\n\n\n\n\n\n\n\n\n\nWe see average speed decreasing a bit as ride time goes up, which makes some sense.\nWe also see the longer rides had more elevation gain, though to be clear, there’s not much elevation to be gained here in Denmark…it’s a bit flat. But the longer workout rides tended to be to northern parts of Sjælland, where it can get a bit hilly at times. I tried adding ride_type as a color aesthetic for the plots and while I won’t display that here, it did confirm my suspicion that the longer rides up north were responsible for the relationship between ride time and elevation.\nAs expected I also expended more energy (calories & kilojoues) the more time I was riding.\nNext we plot average speed in y axis, omitting distance and time, as we’ve already seen that relationship.\n\n\nshow code for average speed scatterplots\npatchwork::wrap_plots(\n    map2(c(\"elevation_gain\", \"average_grade\", \"max_grade\", \"average_watts\", \"calories\", \"kilojoules\"), \n             c(\"average_speed\", \"average_speed\", \"average_speed\", \"average_speed\", \"average_speed\", \"average_speed\"), \n             ~plot_scatter_lm(data = strava_activities_rides, var1 = .x, var2 = .y, pointsize = 3.5) +\n                theme(plot.margin = margin(rep(15, 4)))))\n\n\n\n\n\n\n\n\n\nThe strongest relationship is with watts, confirming what we saw in the correlation heatmap.\nThere’s an oddly slightly positive relationship with elevation, but I wonder if that’s a result of my longer workout rides to the northern part of Sjælland also having not only more elevation but also a bit more open road (especially in the early morning hours when I tend to ride) to ride faster. Again I ran it with ride_type as a color aesthetic which more or less confirmed my intuition.\nI’m a bit surprised to see that energy output isn’t as positively associated with average speed, but perhaps here where it’s flat there’s only so high a level of energy burn I can get to.\nAnd finally, kilojoules as the comparison variable. Why use kilojoules and not calories? Accoring to this Garmin FAQ, calories expended are total energy expended in the time it took to do the workout, while kilojoules is the energy burned by the workout. The formula for kilojoules is (watts x seconds) x 1000.\nSince we’ve already plotted kilojoules against the distance, time, and speed, no need to repeat.\n\n\nshow code for kilojoule scatterplots\n    patchwork::wrap_plots(\n        map2(c(\"average_watts\", \"elevation_gain\", \"average_grade\"), \n                 c(\"kilojoules\", \"kilojoules\", \"kilojoules\"), \n                 ~plot_scatter_lm(data = strava_activities_rides, var1 = .x, var2 = .y, pointsize = 3.5) + \n                    theme(plot.margin = margin(rep(15, 4)))))"
  },
  {
    "objectID": "posts/my-year-of-riding-danishly/index.html#eda-with-correlations",
    "href": "posts/my-year-of-riding-danishly/index.html#eda-with-correlations",
    "title": "My Year of Riding Danishly",
    "section": "EDA with Correlations",
    "text": "EDA with Correlations\nNow for one of my favorite DataExplorer functions, plot_correlation to produce a correlation heatmap. The deeper the shade of red, the stronger the correlation.\n\n\n\n\n\n\n\n\n\nSo what do we see here?\n\nMost of the relationships are positive, some with expectedly near 1:1 relationships, such as distance ridden and total time for the ride.\nAverage speed is positively correlated with distance but the relationship is only at 0.14, the weakest of all positive associations with distance. Average speed correlations are low…near 0, for total elevation gain and negative the higher the average grade of the ride.\nAverge watts(i), or the power output for the ride, has mostly negative correlations. The longer the rides went in time or distance, the lower the average power per ride segment.\n\nWe’ll keep these correlations in mind when looking at the scatterplots and then later considering the regression results.\n\nStrava provides a weighted average watts for the entire ride"
  },
  {
    "objectID": "posts/exploring-happiness-eda-pt2/index.html#introduction",
    "href": "posts/exploring-happiness-eda-pt2/index.html#introduction",
    "title": "Exploring Happiness - EDA Part 2",
    "section": "Introduction",
    "text": "Introduction\nIn Part 1 I did some exploratory data analysis on the World Happiness Report data for Chapter 2 of the 2023 edition.\nThat post got me thinking a bit about being more deliberate about my own initial workflow. Some people have project templates, but I mostly just need a starter script template. So I added some lines to my template script to really ingrain the habit of using those packages for the EDA phase. It’s below…feel free to borrow and of course modify to work\n\n\ncode for my r script template\n### template for r analysis work. save as w/ appropriate name to project directory\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\n# EDA tools\nlibrary(DataExplorer)\nlibrary(explore)\nlibrary(skimr)\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n# sets theme as default for all plots\ntheme_set(theme_light)\n\n## ggplot helpers - load if necessary\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot text\n\n\n### load data\n\n\n### clean data, redo as necesary after running basic EDA\n\n\n\n### EDA with DataExplorer, explore, skimr\n\n## DataExplorer summary of completes, missings\neda1 &lt;- introduce(DATA)\nview(eda1)\n\n## explorer summary\nwhr23_fig2_1 %&gt;%\n    describe_tbl()\n\n## skimr summary\nDATA %&gt;%\n    select() %&gt;%\n    skim()\n\n## go back and clean if issues seen here ##\n\n## dataexplorer plots\nplot_bar(DATA)\nplot_histogram(DATA, nrow = 5L)\n\n## dataexplorer correlations\nDATA %&gt;%\n    select(or deselect as needed) %&gt;%\n    filter(!is.na(if needed)) %&gt;%\n    plot_correlation(maxcat = 5L, type = \"continuous\", geom_text_args = list(\"size\" = 4))\n\n## dataexplorer scatterplots\nplot_scatterplot(\n    DATA %&gt;% select(), by = \"choose target for y axis\", nrow = 3L)\n\n## explorer shiny app\nexplore(DATA %&gt;%\n                    select())\n\n### continue with deeper analysis here or start new r script"
  },
  {
    "objectID": "posts/exploring-happiness-eda-pt2/index.html#recap-of-1st-eda-post",
    "href": "posts/exploring-happiness-eda-pt2/index.html#recap-of-1st-eda-post",
    "title": "Exploring Happiness - EDA Part 2",
    "section": "Recap of 1st EDA post",
    "text": "Recap of 1st EDA post\nBut let’s get back to examining what makes people happy. By way of a quick recap:\n\nThe WHR comes from an annual world-wide survey administered by Gallup and is published by the Sustainable Development Solutions Network.\nData for Chapter 2 are made available every year, and include a 3-year rolling average of the ladder happiness score aggregated by country, other questions from the poll, and logged GDP for the country.\n\nThe EDA in part 1 showed:\n\nThere are not many missing values in the data.\nAll of the component variables except generosity have strong positive correlations with the happiness score.\nPerception of corruption has, as expected, a negative association with happiness.\n\nBut…I spaced on adding one varaible to the dataset, the GINI index, which measures “the extent to which the distribution of income (or, in some cases, consumption expenditure) among individuals or households within an economy deviates from a perfectly equal distribution….a Gini index of 0 represents perfect equality, while an index of 100 implies perfect inequality.” (from the World Bank GINI data page, click the Details tab)\nIt was referenced in the statistical appendix but was not included in the dataset made available. The Chapter 2 authors used two GINI values:\n\na GINI of household income as reported to the Gallup survey and imputed via a STATA function, and\nthe World Bank’s GINI value, taken as the mean of GINI values from 2000 to 2022 to account for the spotty nature of by-country GINI values.\n\nWe can’t do the Gallup & STATA generated GINI because we can’t get the data and I don’t have STATA. So we’ll get the World Bank values, and add that into the data already created. Then we’ll do some more EDA looking at how GINI relates to variables in the set. I was going to do some more in-depth analysis in this post, but I want to keep posts short and focused. So analysis will be in a separate post.\nWe’ll start by loading the packages we’ll use here, as well as the WHR data we already created.\n\n\ncode for loading packages and WHR data\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\n\n# functions I use often enough to load them regularly...should probably write a personal package\nsource(\"~/Data/r/basic functions.R\")\n\n# load data \nwhr23_fig2_1a &lt;- readRDS(\n    file = \"~/Data/r/World Happiness Report/data/whr23_fig2_1.rds\") %&gt;%\n        filter(!is.na(ladder_score))"
  },
  {
    "objectID": "posts/exploring-happiness-eda-pt2/index.html#adding-gini",
    "href": "posts/exploring-happiness-eda-pt2/index.html#adding-gini",
    "title": "Exploring Happiness - EDA Part 2",
    "section": "Adding GINI",
    "text": "Adding GINI\nTo get the GINI values you can go to the World Bank’s GINI page and downloaded the latest spreadsheet, or…\n…even better you could use one of the helpful r packages to get World Bank data; Vincent Arel-Bundock’s(http://arelbundock.com) WDI and the Geospatial Science and Human Security at Oak Ridge National Lab’s wbstats. We’ll use the WDI package here.\nI’ll create a GINI average as per the WHR authors and I’ll also extract the latest GINI value from the set. I’m not sure right now which is ultimately the best to use, but that’s what a bit more EDA is for.\n\n\ncode for loading GINI values\n# using the \"extra = TRUE\" option pulls in capital city & latitude longitude, \n# region, and World Bank income & lending indicators. We won't use them in this analysis but it's\n# worth knowing they're there.\nginis = WDI::WDI(indicator='SI.POV.GINI', start=2000, end=2023, extra = TRUE) %&gt;%\n        as_tibble() %&gt;%\n    select(-status, -lastupdated) %&gt;%\n    # fix missing regions \n    mutate(region =\n                    case_when(country == \"Czechia\" ~ \"Europe & Central Asia\",\n                                        country == \"Viet Nam\" ~ \"East Asia & Pacific\",\n                                        TRUE ~ region)) %&gt;%\n    # remove aggregated regions \n    filter(region != \"Aggregates\")%&gt;%\n    arrange(country, year) %&gt;%\n    rename(gini = SI.POV.GINI) %&gt;%\n    # create the latest and average columns\n    mutate(ginifill = gini) %&gt;%\n    group_by(country) %&gt;%\n    mutate(gini_avg = mean(gini, na.rm = TRUE)) %&gt;%\n    fill(ginifill, .direction = \"downup\") %&gt;%\n    ungroup() %&gt;%\n    filter(year == 2022) %&gt;%\n    mutate(gini_latest = ifelse(is.na(gini), ginifill, gini)) %&gt;%\n    select(country:gini, gini_latest, ginifill, gini_avg, everything()) %&gt;%\n    rename(country_name = country) %&gt;%\n    # clean up some country names to match with WHR data\n    mutate(country_name =\n                    case_when(country_name == \"Czechia\" ~ \"Czech Republic\",\n                                        country_name == \"Congo, Dem. Rep.\" ~ \"Congo (Kinshasa)\",\n                                        country_name == \"Congo, Rep.\" ~ \"Congo (Brazzaville)\",\n                                        country_name == \"Cote d'Ivoire\" ~ \"Ivory Coast\",\n                                        country_name == \"Egypt, Arab Rep.\" ~ \"Egypt\",\n                                        country_name == \"Eswatini\" ~ \"Swaziland\",\n                                        country_name == \"Gambia, The\" ~ \"Gambia\",\n                                        country_name == \"Hong Kong SAR, China\" ~ \"Hong Kong S.A.R. of China\",\n                                        country_name == \"Iran, Islamic Rep.\" ~ \"Iran\",\n                                        country_name == \"Korea, Rep.\" ~ \"South Korea\",\n                                        country_name == \"Kyrgyz Republic\" ~ \"Kyrgyzstan\",\n                                        country_name == \"Lao PDR\" ~ \"Laos\",\n                                        country_name == \"Russian Federation\" ~ \"Russia\",\n                                        country_name == \"Slovak Republic\" ~ \"Slovakia\",\n                                        country_name == \"Turkiye\" ~ \"Turkey\",\n                                        country_name == \"Venezuela, RB\" ~ \"Venezuela\",\n                                        country_name == \"Viet Nam\" ~ \"Vietnam\",\n                                        country_name == \"West Bank and Gaza\" ~ \"Palestinian Territories\",\n                                        country_name == \"Yemen, Rep.\" ~ \"Yemen\",\n                                        TRUE ~ country_name)) \n\n\nI had hoped to pull in a few other indicators to add to the model. The main one I wanted was literacy. Unfortunately there were too many missing values in the UNESCO set…most European countries and a few other key countries. I then thought about public expenditure on education but was worried about colinearity with the GINI index (no, I didn’t test it). So we’ll stick with what we have and not cloud up the model and other analysis with too many exogenous variables. If this were a\nSo let’s add the GINI numbers to the WHR data we already have…\n\n\ncode for joining GINI to WHR data\nwhr23_fig2_1 &lt;- whr23_fig2_1a %&gt;%\n    merge(ginis, all = TRUE) %&gt;%\n    as_tibble() %&gt;%\n    select(country_name, iso3c, region, region_whr, whr_year:lowerwhisker, logged_gdp_per_capita,\n                 gini_avg, gini_latest, everything()) %&gt;%\n    ## fill in Taiwan, no longer in this set but still available \n    ### at https://pip.worldbank.org/country-profiles/TWN\n    mutate(gini_avg = ifelse(\n        country_name == \"Taiwan Province of China\", 32.09833333, gini_avg)) %&gt;%\n    mutate(gini_latest = ifelse(\n        country_name == \"Taiwan Province of China\", 31.48, gini_latest)) %&gt;%\n    filter(!is.na(ladder_score))\n\n\nN.B…yes, I use merge() and not left/right/full_join()…I learned SAS before SQL and old habits die hard (shrug emoji).\nN.B.2…Taiwan data is no longer available in most WB sets, either in the spreadsheet or via the API the packages access. It is still at the WB’s Poverty & Inequality Platform so I downloaded what I could and hard-coded. Why? Because I’m a little OCD when it comes to trying to minimize missing data."
  },
  {
    "objectID": "posts/exploring-happiness-eda-pt2/index.html#redo-the-eda",
    "href": "posts/exploring-happiness-eda-pt2/index.html#redo-the-eda",
    "title": "Exploring Happiness - EDA Part 2",
    "section": "Redo the EDA",
    "text": "Redo the EDA\nAnyway…now we have to do some quick EDA on the GINI values in relation to the data we have. First I want to take a quick look at the by-country difference between the latest GINI value and the mean value for the period since 2000. We subtract the average from the latest value, do a quick skimr check, a density plot on the latest-average difference…\n\n\ncode for EDA skim & density plot\n# a quick skim \nwhr23_fig2_1 %&gt;%\n    mutate(gini_diff = gini_latest - gini_avg) %&gt;%\n    select(gini_latest, gini_avg, gini_diff) %&gt;%\n    skimr::skim()\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n137\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngini_latest\n7\n0.95\n36.73\n7.60\n23.20\n31.54\n35.30\n40.88\n63.00\n▃▇▃▁▁\n\n\ngini_avg\n7\n0.95\n38.09\n7.88\n24.79\n32.26\n36.68\n42.71\n62.40\n▆▇▅▂▁\n\n\ngini_diff\n7\n0.95\n-1.37\n2.32\n-8.91\n-2.85\n-1.00\n0.16\n4.37\n▁▂▇▇▁\n\n\n\n\n\ncode for EDA skim & density plot\n\nwhr23_fig2_1 %&gt;%\n    filter(!is.na(gini_avg)) %&gt;%\n    mutate(gini_diff = gini_latest - gini_avg) %&gt;%\n    select(country_name, gini_latest, gini_avg, gini_diff, region_whr) %&gt;%\n    arrange(gini_diff) %&gt;%\n    ggplot(aes(gini_diff)) +\n                    geom_density(fill = \"blue\") +\n    xlim(-9, 6)\n\n\n\n\n\n\n\n\n\nQuick EDA observations from the skim and denisty plot:\n\nThe GINI scale is 0 to 100, and the spread in this WHR set is 24.7 to 62.4, almost exactly as the entire panel from the WB GINI dataset.\nThe means, medians, standard deviations, and ranges for average and latest are close enough.\nThe difference (latest - average) looks on the denisty plot to be clustered just below 0, and the median difference is only -1, so we’ll use the average.\nWe only lose 7 countries from the set by adding GINI.\n\nSo let’s use the average and see how it relates to our WHR variables by doing first a correlation matrix from the DataExplorer package…\n\n\ncode for EDA correlation\nwhr23_fig2_1 %&gt;%\n    select(ladder_score, standard_error_of_ladder_score, gini_avg, logged_gdp_per_capita,\n                 social_support:perceptions_of_corruption,\n                  explained_by_log_gdp_per_capita:residual) %&gt;%\n    filter(!is.na(residual)) %&gt;%\n    filter(!is.na(gini_avg)) %&gt;%\n    DataExplorer::plot_correlation(maxcat = 5L, type = \"continuous\", geom_text_args = list(\"size\" = 3))\n\n\n\n\n\n\n\n\n\n…and the DataExplorer scatterplots.\n\n\ncode for EDA scatterplots\nDataExplorer::plot_scatterplot(\n    whr23_fig2_1 %&gt;% select(gini_avg, ladder_score, logged_gdp_per_capita,\n                                                    social_support:perceptions_of_corruption), \n    by = \"gini_avg\", nrow = 3L)\n\n\n\n\n\n\n\n\n\nWe can see from the correlation matrix and the scatterplots that:\n\nThere is a mild but persistent relationship between a lower GINI score (less inequality) and the measures that correlate with more happiness…the happiness ladder score itself, life expectancy, freedom to make choices, etc.\nThe one negative relationship was with perceptions of corruption - that is, the higher the inequality measure in their country the more likely people answering the survey were to report that they perceived higher levels of corruption.\nAnd interestingly, a lower GINI index correlated to a higher GDP per capita…so maybe it’s better for all to spread the wealth?\n\nThat’s it for basic EDA on the data. The next post will dig a bit deeper…look at some differences by region, and I’ll do a quick regression to try and predict happiness score and see how close I can get to the WHR model."
  },
  {
    "objectID": "posts/quarto-blogs-and-gdpr/index.html",
    "href": "posts/quarto-blogs-and-gdpr/index.html",
    "title": "Quick Note on Quarto Blogs and GDPR",
    "section": "",
    "text": "To some minor chagrin, thanks to someone raising an issue in the github repo for the blog, I realised that I was out of compliance with GDPR rules about website cookies.\nI was offering people a more onerous opt-out choice (on the left) when I should have been offering them a clear way to decline (on the right).\n\n\n\n\n\n\n\n\n\nnot compliant with GDPR\n\n\n\n\n\n\n\ncompliant with GDPR\n\n\n\n\n\nA quick search and I found the correction I needed to make, changing the _quarto.yml file section on cookie consent from the default. A simple fix, in the website: section change type: implied (the default) to type: express.\nSimple, right?\nWell…after pushing the changes multiple times, the cookie consent pop-up didn’t change.\nSome more searching and I came across this helpful thread in the quarto dev github repo that touched on my issue. Declan Naughton who started the issue offered a solution using another privacy option, Cookie Consent.\nWhile looking into that, downloading the files and planning carefully how to implement it without breaking anything, I got distracted and redid a post to add a table of contents on the right, using toc: true in the post yaml. I pushed that, and huh…the GDPR-compliant cookie consent window popped up.\nThis does beg the question as to why the default in quarto is the non-compliant consent option. Regardless, if you have blog readers in the EU and want to make sure you’re compliant, do the following:\n\nChange the _quarto-yml file so that cookie-consent: is type: express\nAdd a new post or edit, re-render and push an old post\nCheck and double-check that the compliant cookie pop-up shows\nIf all this talk of cookies makes you hungry, eat cookie!\n\n\n\n\nnom nom nom\n\n\nn.b. - cookie image in post header from this wikipedia entry on cookies"
  },
  {
    "objectID": "posts/quarto-blogs-and-gdpr/index.html#introduction",
    "href": "posts/quarto-blogs-and-gdpr/index.html#introduction",
    "title": "Quick Note on Quarto Blogs and GDPR",
    "section": "",
    "text": "To some minor chagrin, thanks to someone raising an issue in the github repo for the blog, I realised that I was out of compliance with GDPR rules about opting in to cookies.\nI was offering people a more onerus opt-out choice (on the left) when I should have been offering them a clear way to decline (on the right).\n\n\n\n\n\n\n\n\n\nnot compliant with GDPR\n\n\n\n\n\n\n\ncompliant with GDPR\n\n\n\n\n\nA quick search and I found the correction I needed to make, changing the _quarto.yml file section on cookie consent from the default. A simple fix, in the website: section change type: implied (the default) to type: express.\nSimple, right?\nWell…after pushing the changes multiple times, the cookie consent pop-up didn’t change.\nSome more searching and I came across this helpful thread in the quarto dev github repo that touched on my issue. Declan Naughton who started the issue offered a solution using another privacy option, Cookie Consent.\nWhile looking into that, downloading the files and planning carefully how to implement it without breaking anything, I got distracted and redid a post to add a table of contents on the right, using toc: true in the post yaml. I pushed that, and huh…the GDPR-compliant cookie consent window popped up.\nThis does beg the question as to why the default in quarto is the non-compliant consent option. Regardless, if you have blog readers in the EU and want to make sure you’re compliant, do the following:\n\nChange the _quarto-yml file so that cookie-consent: is type: express\nAdd a new post or edit, re-render and push an old post\nCheck and double-check that the compliant cookie pop-up shows\nIf all this talk of cookies makes you hungry, eat cookie!\n\n\n\n\nnom nom nom"
  },
  {
    "objectID": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html#introduction",
    "href": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html#introduction",
    "title": "Tidy Tuesday, April 07, 2020 - Le Tour! (Stage 1, cleaning the data)",
    "section": "Introduction",
    "text": "Introduction\nHaving looked at hiking trails in Washington state and bridges in Maryland I poked around the #TidyTuesday repo and saw this trove of data from back in April on The Tour de France. I love this race, I cycle for exercise, and I love the Kraftwerk album, so of course I had to dig in.\nSo I don’t bury the lede, this is a two-part post. Why? Because there was a lot of data munging & cleaning needed to get the data into shape for what I wanted to do. So this post is all about what I needed to do on that end. The analysis post will come soon. Also, I’m trying to work out how to do a code show/hide thing in hugo academic so bear with me that the code takes up lots of pixels.\n(update - migrating to ‘Quarto’ means a native code-fold feature…hooray!)\nSo let’s dig in…first we’ll load packages and create a ’%notin% operator…\n\n# load packages\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(tdf) # to get original stage results file\n\n# create notin operator to help with cleaning & analysis\n`%notin%` &lt;- negate(`%in%`)"
  },
  {
    "objectID": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html#data1",
    "href": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html#data1",
    "title": "Tidy Tuesday, April 07, 2020 - Le Tour! (Stage 1, cleaning the data)",
    "section": "Getting the Data & Initial Cleaning",
    "text": "Getting the Data & Initial Cleaning\nThere’s a ton of data here, sourced from the tdf package from Alastair Rushworth and (Thomas Camminady’s data set) (https://github.com/camminady/LeTourDataSet), via Kaggle\nThere are three distinct sets to work thru, each going back to the first run of the race in 1903:\n- A dataframe of overall (General Classification, or Yellow Jersey / maillot jaune) winners from 1903 to 2019 comes from the Tidy Tuesday frame.\n- A dataframe with stage winners for races 1903 to 2017, also in the Tidy Tuesday set, sourced from Kaggle.\n- A frame of overall stage results, sourced from the tdf pacakge due to issues with date conversion in the data included in the Tidy Tuesday set.\nThe stage winner set needs a bit of mungung…I created a stage_results_id column similar to the one in the stage results set. But it needs leading zeros for stages 1-9 so it sorts properly.\nI then got it in my head I wanted results through 2020, so I grabbed them from wikipedia; but the hard way, with copy-paste since my scraping skills aren’t there & I just wanted it done. Data is uploaded to my github repo if you want to use it. (yes, it’s in an excel file…)\n\n\nShow tdf data cleaning pt1\n# load main file from tt repo\ntt_tdf &lt;- tidytuesdayR::tt_load('2020-04-07')\n\n\n\n    Downloading file 1 of 3: `stage_data.csv`\n    Downloading file 2 of 3: `tdf_stages.csv`\n    Downloading file 3 of 3: `tdf_winners.csv`\n\n\nShow tdf data cleaning pt1\n# create race winners set. comes from tdf package. includes up to 2019\ntdf_winners &lt;- as_tibble(tt_tdf$tdf_winners)\n\n# create stage winner set. in tt file, comes from kaggle, includes up to 2017\ntdf_stagewin1 &lt;- tt_tdf$tdf_stages %&gt;%\n  mutate_if(is.character, str_trim)\n  \n# pulled 2018 - 2020 from wikipedia\n# read in excel - need to separate route field to Origin & Destination\ntdf_stagewin2 &lt;- readxl::read_excel(\"data/tdf_stagewinners_2018-20.xlsx\") %&gt;%\n  mutate(Stage = as.character(Stage)) %&gt;%\n  mutate(Date = lubridate::as_date(Date)) %&gt;% \n  separate(Course, c(\"Origin\", \"Destination\"), \"to\", extra = \"merge\") %&gt;%\n  mutate_if(is.character, str_trim) %&gt;%\n  select(Stage, Date, Distance, Origin, Destination, Type, Winner, Winner_Country = Winner_country)\n\n# join with rbind (since I made sure to put 2018-2020 data in same shape as tt set)\n# clean up a bit\ntdf_stagewin &lt;- rbind(tdf_stagewin1, tdf_stagewin2) %&gt;%\n  mutate(race_year = lubridate::year(Date)) %&gt;% \n  mutate(Stage = ifelse(Stage == \"P\", \"0\", Stage)) %&gt;%\n  mutate(stage_ltr = case_when(str_detect(Stage, \"a\") ~ \"a\",\n                               str_detect(Stage, \"b\") ~ \"b\",\n                               str_detect(Stage, \"c\") ~ \"c\",\n                               TRUE ~ \"\")) %&gt;%\n  mutate(stage_num = str_remove_all(Stage, \"[abc]\")) %&gt;%\n  mutate(stage_num = stringr::str_pad(stage_num, 2, side = \"left\", pad = 0)) %&gt;% \n  mutate(stage_results_id = paste0(\"stage-\", stage_num, stage_ltr)) %&gt;%\n  mutate(split_stage = ifelse(stage_ltr %in% c(\"a\", \"b\", \"c\"), \"yes\", \"no\")) %&gt;%\n  \n  # extract first and last names from winner field\n  mutate(winner_first = str_match(Winner, \"(^.+)\\\\s\")[, 2]) %&gt;%\n  mutate(winner_last= gsub(\".* \", \"\", Winner)) %&gt;%\n\n  # clean up stage types, collapse into fewer groups\n  mutate(stage_type = case_when(Type %in% c(\"Flat cobblestone stage\", \"Flat stage\", \"Flat\",\n                                            \"Flat Stage\", \"Hilly stage\", \"Plain stage\", \n                                            \"Plain stage with cobblestones\") \n                                ~ \"Flat / Plain / Hilly\",\n                                Type %in% c(\"High mountain stage\", \"Medium mountain stage\",\n                                            \"Mountain stage\", \"Mountain Stage\", \"Stage with mountain\",\n                                            \"Stage with mountain(s)\", \"Transition stage\")\n                                ~ \"Mountain\",\n                                Type %in% c(\"Individual time trial\", \"Mountain time trial\") \n                                ~ \"Time Trail - Indiv\",\n                                Type == \"Team time trial\" ~ \"Time Trail - Team\",\n                                TRUE ~ \"Other\")) %&gt;% \n  mutate_if(is.character, str_trim) %&gt;%\n  arrange(desc(race_year), stage_results_id) %&gt;%\n  select(race_year, stage_results_id, stage_date = Date, stage_type, Type, split_stage,\n         Origin, Destination, Distance, Winner, winner_first, winner_last,\n         Winner_Country, everything())\n\n# take a look at this awesome dataset\nglimpse(tdf_stagewin)\n\n\nRows: 2,299\nColumns: 16\n$ race_year        &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020,…\n$ stage_results_id &lt;chr&gt; \"stage-01\", \"stage-02\", \"stage-03\", \"stage-04\", \"stag…\n$ stage_date       &lt;date&gt; 2020-08-29, 2020-08-30, 2020-08-31, 2020-09-01, 2020…\n$ stage_type       &lt;chr&gt; \"Flat / Plain / Hilly\", \"Mountain\", \"Flat / Plain / H…\n$ Type             &lt;chr&gt; \"Flat stage\", \"Medium mountain stage\", \"Flat stage\", …\n$ split_stage      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ Origin           &lt;chr&gt; \"Nice\", \"Nice\", \"Nice\", \"Sisteron\", \"Gap\", \"Le Teil\",…\n$ Destination      &lt;chr&gt; \"Nice\", \"Nice\", \"Sisteron\", \"Orcières-Merlette\", \"Pri…\n$ Distance         &lt;dbl&gt; 156.0, 186.0, 198.0, 160.5, 183.0, 191.0, 168.0, 141.…\n$ Winner           &lt;chr&gt; \"Alexander Kristoff\", \"Julian Alaphilippe\", \"Caleb Ew…\n$ winner_first     &lt;chr&gt; \"Alexander\", \"Julian\", \"Caleb\", \"Primož\", \"Wout van\",…\n$ winner_last      &lt;chr&gt; \"Kristoff\", \"Alaphilippe\", \"Ewan\", \"Roglič\", \"Aert\", …\n$ Winner_Country   &lt;chr&gt; \"NOR\", \"FRA\", \"AUS\", \"SLO\", \"BEL\", \"KAZ\", \"BEL\", \"FRA…\n$ Stage            &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"1…\n$ stage_ltr        &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ stage_num        &lt;chr&gt; \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\",…"
  },
  {
    "objectID": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html#data2",
    "href": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html#data2",
    "title": "Tidy Tuesday, April 07, 2020 - Le Tour! (Stage 1, cleaning the data)",
    "section": "More Data Cleaning",
    "text": "More Data Cleaning\nStage data in CSV from the tidy tuesday repository seems to have truncated the times, leaving only the seconds in a character field. To get complete results we need to pull from tdf package using the cleaning script from the Tidy Tuesday page. Some operations will take a while, so best to run as a background job if you want to do something else while it runs. Or go get a cup of coffee.\nIn terms of cleaning:\n- The stage_results_id & rank fields needs leading zeros.\n- The rank field needs a bit of clean-up to fix the 1000s codes.\n- Since rider names were last-first, I wanted to separate out first and last, and also make a field with the full name, but first name in front. Stackoverlflow was my regex friend here.\n- Other minor fixes\nIn the process of cleaning and comparing to the stage winners set, I noticed there were some problems in years where individual stages were split into 2 or 3 legs (A, B & C). Either while it was scraped or combined, the A leg results ended up repeating to the B leg, and in some cases the C leg wasn’t reported. I put it in as an issue in the github repo. But that shouldn’t take away from what’s an amazing dataset to work with. In the analysis section I’ll work around the problems with those stages.\n\n\nShow tdf data cleaning pt2\nall_years &lt;- tdf::editions %&gt;%\n  unnest_longer(stage_results) %&gt;%\n  mutate(stage_results = map(stage_results, ~ mutate(.x, rank = as.character(rank)))) %&gt;%\n  unnest_longer(stage_results)\n\nstage_all &lt;- all_years %&gt;%\n  select(stage_results) %&gt;%\n  flatten_df()\n\ncombo_df &lt;- bind_cols(all_years, stage_all) %&gt;%\n  select(-stage_results)\n\ntdf_stagedata &lt;- as_tibble(combo_df %&gt;%\n  select(edition, start_date,stage_results_id:last_col()) %&gt;%\n  mutate(race_year = lubridate::year(start_date)) %&gt;%\n  rename(age = age...25) %&gt;%\n\n  # to add leading 0 to stage, extract num, create letter, add 0s to num, paste\n  mutate(stage_num = str_replace(stage_results_id, \"stage-\", \"\")) %&gt;%\n  mutate(stage_ltr = case_when(str_detect(stage_num, \"a\") ~ \"a\",\n                               str_detect(stage_num, \"b\") ~ \"b\",\n                               TRUE ~ \"\"))) %&gt;%\n  mutate(stage_num = str_remove_all(stage_num, \"[ab]\")) %&gt;%\n  mutate(stage_num = stringr::str_pad(stage_num, 2, side = \"left\", pad = 0)) %&gt;%\n  mutate(stage_results_id2 = paste0(\"stage-\", stage_num, stage_ltr)) %&gt;%\n  mutate(split_stage = ifelse(stage_ltr %in% c(\"a\", \"b\"), \"yes\", \"no\")) %&gt;%\n\n  # fix 1000s rank. change to DNF\n  mutate(rank = ifelse(rank %in% c(\"1003\", \"1005\", \"1006\"), \"DNF\", rank)) %&gt;%\n  mutate(rank2 = ifelse(rank %notin% c(\"DF\", \"DNF\", \"DNS\", \"DSQ\",\"NQ\",\"OTL\"),\n                        stringr::str_pad(rank, 3, side = \"left\", pad = 0), rank)) %&gt;%\n\n  # extract first and last names from rider field\n  mutate(rider_last = str_match(rider, \"(^.+)\\\\s\")[, 2]) %&gt;%\n  mutate(rider_first= gsub(\".* \", \"\", rider)) %&gt;%\n  mutate(rider_firstlast = paste0(rider_first, \" \", rider_last)) %&gt;%\n  select(-stage_results_id, -start_date, ) %&gt;%\n\n  # fix 1967 & 1968\n  mutate(stage_results_id2 = ifelse((race_year %in% c(1967, 1968) & stage_results_id2 == \"stage-00\"),\n         \"stage-01a\", stage_results_id2)) %&gt;%\n  mutate(stage_results_id2 = ifelse((race_year %in% c(1967, 1968) & stage_results_id2 == \"stage-01\"),\n         \"stage-01b\", stage_results_id2)) %&gt;%\n  mutate(split_stage = ifelse((race_year %in% c(1967, 1968) &\n                                 stage_results_id2 %in% c(\"stage-01a\", \"stage-01b\")),\n                              \"yes\", split_stage)) %&gt;%\n\n  select(edition, race_year, stage_results_id = stage_results_id2, split_stage,\n         rider, rider_first, rider_last, rider_firstlast, rank2,\n         time, elapsed, points, bib_number, team, age, everything())\n\nsaveRDS(tdf_stagedata, \"data/tdf_stagedata.rds\")\n\n\n\ntdf_stagedata &lt;- readRDS(\"data/tdf_stagedata.rds\")\nglimpse(tdf_stagedata)\n\nRows: 255,752\nColumns: 18\n$ edition          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ race_year        &lt;dbl&gt; 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903,…\n$ stage_results_id &lt;chr&gt; \"stage-01\", \"stage-01\", \"stage-01\", \"stage-01\", \"stag…\n$ split_stage      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ rider            &lt;chr&gt; \"Garin Maurice\", \"Pagie Émile\", \"Georget Léon\", \"Auge…\n$ rider_first      &lt;chr&gt; \"Maurice\", \"Émile\", \"Léon\", \"Fernand\", \"Jean\", \"Marce…\n$ rider_last       &lt;chr&gt; \"Garin\", \"Pagie\", \"Georget\", \"Augereau\", \"Fischer\", \"…\n$ rider_firstlast  &lt;chr&gt; \"Maurice Garin\", \"Émile Pagie\", \"Léon Georget\", \"Fern…\n$ rank2            &lt;chr&gt; \"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008…\n$ time             &lt;Period&gt; 17H 45M 13S, 55S, 34M 59S, 1H 2M 48S, 1H 4M 53S, 1…\n$ elapsed          &lt;Period&gt; 17H 45M 13S, 17H 46M 8S, 18H 20M 12S, 18H 48M 1S, …\n$ points           &lt;int&gt; 100, 70, 50, 40, 32, 26, 22, 18, 14, 10, 8, 6, 4, 2, …\n$ bib_number       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ team             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ age              &lt;int&gt; 32, 32, 23, 20, 36, 37, 25, 33, NA, 22, 26, 28, 21, 2…\n$ rank             &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"1…\n$ stage_num        &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n$ stage_ltr        &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…"
  },
  {
    "objectID": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html#next",
    "href": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html#next",
    "title": "Tidy Tuesday, April 07, 2020 - Le Tour! (Stage 1, cleaning the data)",
    "section": "What’s Next",
    "text": "What’s Next\nPoking around the Kaggle site referenced above I found these datasets of final results for all riders in all races since 1903. A few different fields than in the tidy tuesday winners set.\nNow this is a ton of data to work with, and I won’t use it all. Figured I’d include the code to get it all in case you get inspired to grab it and take a look.\nOk…that’s it for cleaning & prepping…charts and tables in Stage 2.\nThis post was last updated on 2024-02-20"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html",
    "href": "posts/30-day-chart-challenge-2024/index.html",
    "title": "30 Day Chart Challenge 2024",
    "section": "",
    "text": "If you regularly scroll through dataviz social media you probably see lots of posts in April where people show off their work for the 30 Day Chart Challenge. I never participate in these chart or map challenges because usually they take me by surprise and it’s alreayd a week in before I notice and then I figure I’m too late.\nWell not this year. Even though I’m (checks calendar) three weeks late to the party, I’m going to try and add a few charts to the challenge."
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html#plan",
    "href": "posts/30-day-chart-challenge-2024/index.html#plan",
    "title": "30 Day Chart Challenge 2024",
    "section": "My Plan for the Challenge",
    "text": "My Plan for the Challenge\nIf you regularly scroll through dataviz social media you probably see lots of posts in April where people show off their work for the 30 Day Chart Challenge. I never participate in these chart or map challenges because usually they take me by surprise and it’s already a week in before I notice and then I figure I’m too late.\nWell not this year. Even though I’m (checks calendar) three weeks late to the party, I’m going to try and add a few charts to the challenge. I figure it’s a good way to keep me consistently creating new visualisations, using the prompts to spur me to think about different data sources and dataviz approaches. My plan is to:\n\nProduce charts for as many prompts as possible, working in prompt-day order.\nPost them to social media on my Bluesky, Mastodon, Twitter, and LinkedIn accounts.\nAdd them to this post, meaning the posting date will refresh to the date I’ve added the chart.So If I get to alot or even all of them, this will be a long post.\nUse the table of contents to the right to navigate to a specific chart.\n\nI may not get to every prompt, but I’ll do what I can. So let’s get going."
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html#prompt1",
    "href": "posts/30-day-chart-challenge-2024/index.html#prompt1",
    "title": "30 Day Chart Challenge 2024",
    "section": "Prompt #1 - Part-to-Whole",
    "text": "Prompt #1 - Part-to-Whole\nI chose to interpret this prompt by doing a stacked bar chart displaying educational attainment as a proportion of populations by regions in the UK. It doubles as a late-to-the-party entry for the January 23, 2024 Tidy Tuesday challenge.\nThe original data is from the UK Office of National Statistics, in a report they produced looking at educational attainment by socio-economic factors.\nMy plan for this was to knock out the chart in about an hour, but I needed up having to do a bit more data transformation than I thought. I also had to read up a bit on UK education levels and stages. In the end, it took a few hours to get it as I wanted.\nFirst we’ll get and clean the data…\n\n\nShow code for getting and cleaning data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidytuesdayR) #to get data from tidy tuesday repo\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(ggtext) # to help make ggplot text look good\n\n# load tidy tuesday data\nukeduc1 &lt;- tt_load(\"2024-01-23\")\n\n# create tibble from csv, clean data, add some fields for the chart\nukeduc &lt;- as_tibble(ukeduc1$english_education) %&gt;%\n    mutate(town11nm = ifelse(town11nm == \"Outer london BUAs\", \"Outer London BUAs\", town11nm)) %&gt;%\n    mutate(ttwa_classification = ifelse(\n        town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"), \n        \"Majority conurbation\", ttwa_classification)) %&gt;%\n    mutate(ttwa11nm = ifelse(\n        town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"), \"London\", ttwa11nm)) %&gt;%\n    mutate(ttwa11cd = ifelse(\n        town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"), \"E30000234\", ttwa11cd)) %&gt;%\n    mutate(across(26:29, ~ifelse(is.na(.),0,.))) %&gt;%\n    mutate(level_sum = rowSums(.[c(26:29)])) %&gt;%\n    # to get the number of students in the achievement groups we need to multiply the cohort\n    # by the percentage and divide by 100 as in the original data they were displayed as \n    # full numbers, eg 17.2 instead of 0.172.\n    mutate(highest_level_qualification_achieved_by_age_22_na = 100 - level_sum) %&gt;%\n    mutate(n_lesslev1_age22 =\n            round(highest_level_qualification_achieved_by_age_22_less_than_level_1 * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev1to2_age22 =\n            round(highest_level_qualification_achieved_by_age_22_level_1_to_level_2 * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev3to5_age22 =\n            round(highest_level_qualification_achieved_by_age_22_level_3_to_level_5 * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev6plus_age22 =\n            round(highest_level_qualification_achieved_by_age_22_level_6_or_above * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev_na_age22 =\n            round(highest_level_qualification_achieved_by_age_22_na * (ks4_2012_2013_counts/100) ,0))\n\n\nOk, so we have data, let’s make the chart. It’s a horizontal bar chart showing the eudcational attainment at age 22 of a cohort of 15-16 year old students who sat for college qualifying exams (GCSEs) in the 2012-13 school year. Meaning the ending time point here is 2018-19. Attainment is plotted by level and by region. Because the original data is by town, we have to group by region to get what we want.\n\n\nShow code for making the chart\n# need to create cohort population by region\nukeduc %&gt;%\n    rename(region =rgn11nm ) %&gt;%\n    # group by regions and create region totals \n    group_by(region) %&gt;%\n    summarise(region_cohort = sum(ks4_2012_2013_counts),\n                    region_levless1_age22 = sum(n_lesslev1_age22),\n                    region_lev1to2_age22 = sum(n_lev1to2_age22),\n                    region_lev3to5_age22 = sum(n_lev3to5_age22),\n                    region_lev6plus_age22 = sum(n_lev6plus_age22),\n                    region_lev_na_age22 = sum(n_lev_na_age22)) %&gt;%\n    # pivot longer and make the education attainment field into factors. \n    # factor labels for the chart\n    pivot_longer(cols = ends_with(\"age22\"), \n                        names_to = \"ed_ettain_age22\", \n                        values_to = \"ed_attain_n\") %&gt;%\n    mutate(\n        ed_ettain_age22 = \n            factor(ed_ettain_age22, \n                    levels = c(\"region_lev_na_age22\", \"region_levless1_age22\", \"region_lev1to2_age22\", \n                            \"region_lev3to5_age22\",  \"region_lev6plus_age22\"), \n                    labels = c(\"No data\", \"Level &lt;1\", \"Level = 1-2\", \"Level = 3-5\", \"Level = 6+\"))) %&gt;%\n    mutate(ed_attain_pct = ed_attain_n / region_cohort) %&gt;%\n    mutate(ed_attain_pct2 = round(ed_attain_pct*100, 1)) %&gt;%\n    ungroup() %&gt;%\n    mutate(region = factor(region)) %&gt;%\n    filter(!is.na(region)) %&gt;%\n    # pass this temporary set thru to the ggplot call\n    {. -&gt;&gt; tmp} %&gt;%\n    ggplot(aes(ed_attain_pct, fct_rev(region), fill = fct_rev(ed_ettain_age22))) +\n    geom_bar(stat = \"identity\") +\n    scale_x_continuous(\n        expand = c(0,0), \n        breaks = c(0, 0.25, 0.50, 0.75, 1), \n        labels = c(\"0\", \"25%\", \"50%\", \"75%\", \"100%\")) +\n    geom_text(data = subset(tmp, ed_attain_pct &gt;0.025),\n        aes(label = scales::percent(round(ed_attain_pct , 2))),\n                    position = position_stack(vjust = 0.5),\n                    color= \"white\", vjust = 0.5, size = 5) +\n    labs(title = \"Students in London most likely to have at least 4-year degree by Age 22\",\n        subtitle = \"Sixth Year Educational Outcomes for Level 4 2012-13 Cohort by UK Region\",\n        caption = \"*Tidy Tuesday data 01/23/2024, from UK Office of National Statistics*\",\n        x = \"\", y = \"Region\") +\n    scale_fill_brewer(palette = \"Set2\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'), \n            legend.key.width = unit(1.5, 'cm'), legend.margin=margin(-10, 0, 0, 0),\n            plot.title = element_text(hjust = 1), \n            plot.caption = element_markdown(),\n            panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    guides(fill = guide_legend(\n        label.position = \"bottom\", reverse = TRUE, \n        title = \"Cohort at Age 22\", title.position = \"top\"))\n\n\n\n\n\n\n\n\n\nWe see from this chart that most in regions the rate of 4-year college degree completion (level 6+) was in the mid-high 20s, with students from the London region having the highest rate at 36%.\ncreated and posted April 21, 2024"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html#prompt2",
    "href": "posts/30-day-chart-challenge-2024/index.html#prompt2",
    "title": "30 Day Chart Challenge 2024",
    "section": "Prompt #2 - Neo",
    "text": "Prompt #2 - Neo\nThe way I chose to interpret “neo” was by using births by country, or new people by country. The data come from EuroStat, via the eurostat package. I’ll use total live births and births per 1000 people for the year 2022 (most recent year available). I’ll use patchwork to combine the plots. Unlike prompt 1, this only took a little more than an hour to do.\nGetting the data is super simple, and it doesn’t need much cleaning, so I’ll do all the code in one chunk.\n\n\nShow code for getting data and making the plots\n# to know the table code \"tps00204\", I had to look up the dataset on-line and copy that value.\n# there is a function in the package to list all available data.\nbirths &lt;- eurostat::get_eurostat(\"tps00204\", type = \"label\", time_format = \"num\") %&gt;%\n    select(-freq) %&gt;%\n    rename(year = TIME_PERIOD)\n#&gt; \nindexed 0B in  0s, 0B/s\nindexed 1.00TB in  0s, 1.18PB/s\n                                                                              \n\n# make the plots\ntotal_births &lt;-\nbirths %&gt;%\n    filter(year == 2022) %&gt;%\n    filter(indic_de == \"Live births - number\") %&gt;%\n    filter(!grepl(\"Euro\",geo)) %&gt;%\n    arrange(desc(values), geo) %&gt;%\n    mutate(geo = forcats::fct_inorder(geo)) %&gt;%\n    {. -&gt;&gt; tmp} %&gt;%\n    ggplot(aes(values, fct_rev(geo))) +\n    geom_bar(stat = \"identity\", fill = \"#FFCC00\") +\n    scale_y_discrete(position = \"right\") +\n    # need to subset data to get highest label to align similar to the rest\n    geom_text(data = subset(tmp, values &lt;800000),\n        aes(label = format(values, big.mark=\",\")),\n                        color= \"#003399\", size = 4,\n                        hjust = -.25, nudge_x = .25) +\n    geom_text(data = subset(tmp, values &gt;800000),\n                        aes(label = format(values, big.mark = \",\")),\n                        color= \"#003399\", size = 4,\n                        hjust = 1, nudge_x = .5) +\n    labs(y = \"\", x = \"Total Live Births - 2022\") +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\nbirths_per1k &lt;-\nbirths %&gt;%\n    filter(year == 2022) %&gt;%\n    filter(indic_de == \"Crude birth rate - per thousand persons\") %&gt;%\n    filter(!grepl(\"Euro\",geo)) %&gt;%\n    arrange(desc(values), geo) %&gt;%\n    mutate(geo = forcats::fct_inorder(geo)) %&gt;%\n    ggplot(aes(values, fct_rev(geo))) +\n    geom_bar(stat = \"identity\", fill = \"#003399\") +\n    geom_text(aes(label = values),\n                        position = position_stack(vjust = 0.5),\n                        color= \"#FFCC00\", vjust = 0.5, size = 4) +\n    labs(y = \"\", x = \"Births per 1000 persons - 2022\") +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n# put them together with patchwork\nlibrary(patchwork)\ntotal_births + births_per1k + plot_annotation(\n    title = 'Turkey tops in both total births and births / 1000 people',\n    subtitle = 'France only other country top 10 in both measures. San Marino bottom in both',\n    caption = 'Data from EuroStats using eurostat package')\n\n\n\n\n\n\n\n\n\nThere you have it. Turkey is tops in both total number of births and births per 1000 people. France is the only other country in the top 10. Interestingly, Spain is near the top in total number (it’s a big country) but towards the bottom in births per 1000. San Marino is last in both total births and rate.\nWhat other interesting insights do you see here?\ncreated and posted April 22, 2024"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html#prompts-3-4---redo-waffle-prompt34",
    "href": "posts/30-day-chart-challenge-2024/index.html#prompts-3-4---redo-waffle-prompt34",
    "title": "30 Day Chart Challenge 2024",
    "section": "Prompts #3 & 4 - Redo & Waffle {#prompt3&4}",
    "text": "Prompts #3 & 4 - Redo & Waffle {#prompt3&4}\nFor this prompt I fulfilled prompt 3 “redo” by redoing the chart for prompt 1, and made it a waffle chart to fulfill prompt 4. I’d never done a waffle chart before, so new chart type…hooray! Cheers to Yann Holtz for his code-through on his “R Graph Gallery” website to help me better understand how to set-up the data and plot it.\nI’m once again using educational attainment data from the UK, via a Tidy Tuesday set from January 2024. We’ll get the data and make the chart in the same code chunk.\n\n\nShow code for getting data and making the plots\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(tidytuesdayR) #to get data from tidy tuesday repo\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n## ggplot helpers - load if necessary\nlibrary(waffle) # ggplot helper to make a waffle chart\nlibrary(ggtext) # helper functions for ggplot text\n\n# load tidy tuesday data\nukeduc1 &lt;- tt_load(\"2024-01-23\")\n#&gt; \n#&gt;  Downloading file 1 of 1: `english_education.csv`\n\n# get variable names\nukeduc_names &lt;- as_tibble(names(ukeduc1$english_education))\n\n# create tibble from csv, clean data\nukeduc &lt;- as_tibble(ukeduc1$english_education) %&gt;%\n    mutate(town11nm = ifelse(town11nm == \"Outer london BUAs\", \"Outer London BUAs\", town11nm)) %&gt;%\n    mutate(\n        ttwa_classification = ifelse(town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"),\n                                                                 \"Majority conurbation\", ttwa_classification)) %&gt;%\n    mutate(ttwa11nm = ifelse(town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"),\n                                                     \"London\", ttwa11nm)) %&gt;%\n    mutate(ttwa11cd = ifelse(town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"),\n                                                     \"E30000234\", ttwa11cd)) %&gt;%\n    mutate(across(26:29, ~ifelse(is.na(.),0,.))) %&gt;%\n    mutate(level_sum = rowSums(.[c(26:29)])) %&gt;%\n    mutate(highest_level_qualification_achieved_by_age_22_na = 100 - level_sum) %&gt;%\n    mutate(n_lesslev1_age22 =\n                    round(highest_level_qualification_achieved_by_age_22_less_than_level_1 * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev1to2_age22 =\n                    round(highest_level_qualification_achieved_by_age_22_level_1_to_level_2 * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev3to5_age22 =\n                    round(highest_level_qualification_achieved_by_age_22_level_3_to_level_5 * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev6plus_age22 =\n                    round(highest_level_qualification_achieved_by_age_22_level_6_or_above * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev_na_age22 =\n                    round(highest_level_qualification_achieved_by_age_22_na * (ks4_2012_2013_counts/100) ,0))\n\nglimpse(ukeduc)\n#&gt; Rows: 1,104\n#&gt; Columns: 38\n#&gt; $ town11cd                                                          &lt;chr&gt; \"E34…\n#&gt; $ town11nm                                                          &lt;chr&gt; \"Car…\n#&gt; $ population_2011                                                   &lt;dbl&gt; 5456…\n#&gt; $ size_flag                                                         &lt;chr&gt; \"Sma…\n#&gt; $ rgn11nm                                                           &lt;chr&gt; \"Eas…\n#&gt; $ coastal                                                           &lt;chr&gt; \"Non…\n#&gt; $ coastal_detailed                                                  &lt;chr&gt; \"Sma…\n#&gt; $ ttwa11cd                                                          &lt;chr&gt; \"E30…\n#&gt; $ ttwa11nm                                                          &lt;chr&gt; \"Wor…\n#&gt; $ ttwa_classification                                               &lt;chr&gt; \"Maj…\n#&gt; $ job_density_flag                                                  &lt;chr&gt; \"Res…\n#&gt; $ income_flag                                                       &lt;chr&gt; \"Hig…\n#&gt; $ university_flag                                                   &lt;chr&gt; \"No …\n#&gt; $ level4qual_residents35_64_2011                                    &lt;chr&gt; \"Low…\n#&gt; $ ks4_2012_2013_counts                                              &lt;dbl&gt; 65, …\n#&gt; $ key_stage_2_attainment_school_year_2007_to_2008                   &lt;dbl&gt; 65.0…\n#&gt; $ key_stage_4_attainment_school_year_2012_to_2013                   &lt;dbl&gt; 70.7…\n#&gt; $ level_2_at_age_18                                                 &lt;dbl&gt; 72.3…\n#&gt; $ level_3_at_age_18                                                 &lt;dbl&gt; 50.7…\n#&gt; $ activity_at_age_19_full_time_higher_education                     &lt;dbl&gt; 30.7…\n#&gt; $ activity_at_age_19_sustained_further_education                    &lt;dbl&gt; 21.5…\n#&gt; $ activity_at_age_19_appprenticeships                               &lt;dbl&gt; NA, …\n#&gt; $ activity_at_age_19_employment_with_earnings_above_0               &lt;dbl&gt; 52.3…\n#&gt; $ activity_at_age_19_employment_with_earnings_above_10_000          &lt;dbl&gt; 36.9…\n#&gt; $ activity_at_age_19_out_of_work                                    &lt;dbl&gt; NA, …\n#&gt; $ highest_level_qualification_achieved_by_age_22_less_than_level_1  &lt;dbl&gt; 0.0,…\n#&gt; $ highest_level_qualification_achieved_by_age_22_level_1_to_level_2 &lt;dbl&gt; 34.9…\n#&gt; $ highest_level_qualification_achieved_by_age_22_level_3_to_level_5 &lt;dbl&gt; 39.7…\n#&gt; $ highest_level_qualification_achieved_by_age_22_level_6_or_above   &lt;dbl&gt; 0.0,…\n#&gt; $ highest_level_qualification_achieved_b_age_22_average_score       &lt;dbl&gt; 3.32…\n#&gt; $ education_score                                                   &lt;dbl&gt; -0.5…\n#&gt; $ level_sum                                                         &lt;dbl&gt; 74.6…\n#&gt; $ highest_level_qualification_achieved_by_age_22_na                 &lt;dbl&gt; 25.4…\n#&gt; $ n_lesslev1_age22                                                  &lt;dbl&gt; 0, 0…\n#&gt; $ n_lev1to2_age22                                                   &lt;dbl&gt; 23, …\n#&gt; $ n_lev3to5_age22                                                   &lt;dbl&gt; 26, …\n#&gt; $ n_lev6plus_age22                                                  &lt;dbl&gt; 0, 8…\n#&gt; $ n_lev_na_age22                                                    &lt;dbl&gt; 17, …\n\n# cohort ed attainment by region\nukeduc2 &lt;-\n    ukeduc %&gt;%\n    rename(region =rgn11nm ) %&gt;%\n    group_by(region) %&gt;%\n    summarise(region_cohort = sum(ks4_2012_2013_counts),\n                        region_levless1_age22 = sum(n_lesslev1_age22),\n                        region_lev1to2_age22 = sum(n_lev1to2_age22),\n                        region_lev3to5_age22 = sum(n_lev3to5_age22),\n                        region_lev6plus_age22 = sum(n_lev6plus_age22)) %&gt;%\n    pivot_longer(cols = ends_with(\"age22\"), names_to = \"ed_attain_age22\", values_to = \"ed_attain_n\") %&gt;%\n    mutate(ed_attain_pct = ed_attain_n / region_cohort) %&gt;%\n    mutate(ed_attain_pct2 = round(ed_attain_pct*100)) %&gt;%\n    mutate(region = factor(region))%&gt;%\n    filter(!is.na(region)) %&gt;%\n    select(region, region_cohort, ed_attain_age22, ed_attain_pct2) %&gt;%\n    ## the data was a bit messy, so to get the values to sum to 100 to make the waffle\n    ## chart fill the plot area, I had to do a little reshaping after doing the percentages.\n    pivot_wider(names_from = ed_attain_age22, values_from = ed_attain_pct2) %&gt;%\n    mutate(region_sum = rowSums(.[c(3:6)])) %&gt;%\n    mutate(region_lev_na_age22 = ifelse(region_sum &lt; 100, 100 - region_sum, 0)) %&gt;%\n    select(-region_sum) %&gt;%\n    pivot_longer(cols = ends_with(\"age22\"), names_to = \"ed_attain_age22\", values_to = \"ed_attain_pct\") %&gt;%\n    mutate(ed_attain_age22 =\n                factor(ed_attain_age22,\n                             levels = c(\"region_lev_na_age22\", \"region_levless1_age22\", \"region_lev1to2_age22\",\n                                                 \"region_lev3to5_age22\", \"region_lev6plus_age22\"),\n                             labels = c(\"No data\", \"Level &lt;1\", \"Level = 1-2\",\n                                                 \"Level = 3-5\", \"Level = 6+\")))\nglimpse(ukeduc2)\n#&gt; Rows: 45\n#&gt; Columns: 4\n#&gt; $ region          &lt;fct&gt; East Midlands, East Midlands, East Midlands, East Midl…\n#&gt; $ region_cohort   &lt;dbl&gt; 39811, 39811, 39811, 39811, 39811, 48849, 48849, 48849…\n#&gt; $ ed_attain_age22 &lt;fct&gt; Level &lt;1, Level = 1-2, Level = 3-5, Level = 6+, No dat…\n#&gt; $ ed_attain_pct   &lt;dbl&gt; 2, 30, 41, 25, 2, 2, 27, 43, 27, 1, 2, 19, 43, 36, 0, …\n\n## chart\nukeduc2 %&gt;%\n    ggplot(aes(fill = ed_attain_age22, values = ed_attain_pct)) +\n    geom_waffle(na.rm=TRUE, n_rows=10, flip=TRUE, size = 0.33, colour = \"white\") +\n    facet_wrap(~region, nrow=1,strip.position = \"bottom\") +\n    scale_x_discrete() +\n    scale_y_continuous(labels = function(x) x * 10, # make this multiplyer the same as n_rows\n                                         expand = c(0,0)) +\n    scale_fill_brewer(palette = \"Set2\") +\n        labs(title = \"Students in London most likely to have at least 4-year degree by Age 22\",\n                 subtitle = \"Sixth Year Educational Outcomes for Level 4 2012-13 Cohort by UK Region&lt;br&gt;\n                 Each block = 1 %\",\n                 caption = \"*Tidy Tuesday data 01/23/2024, from UK Office of National Statistics*\",\n                 x = \"\", y = \"\") +\n    theme_minimal() +\n        theme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'),\n                    legend.key.width = unit(1.5, 'cm'), legend.margin=margin(-10, 0, 0, 0),\n                    plot.title = element_text(hjust = 0), plot.subtitle = element_markdown(),\n                    plot.caption = element_markdown(),\n                    panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    guides(fill = guide_legend(label.position = \"bottom\",\n                                                         title = \"Cohort at Age 22\", title.position = \"top\"))\n\n\n\n\n\n\n\n\n\nSo alright, a waffle chart. Similar insights from the horizontal bar, just visualized a bit differently.\ncreated and posted April 23, 2024"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html#prompts3and4",
    "href": "posts/30-day-chart-challenge-2024/index.html#prompts3and4",
    "title": "30 Day Chart Challenge 2024",
    "section": "Prompts #3 & 4 - Redo & Waffle",
    "text": "Prompts #3 & 4 - Redo & Waffle\nFor this prompt I fulfilled prompt 3 “redo” by redoing the chart for prompt 1, and made it a waffle chart to fulfill prompt 4. I’d never done a waffle chart before, so new chart type…hooray! Cheers to Yann Holtz for his code-through on his “R Graph Gallery” website to help me better understand how to set-up the data and plot it.\nI’m once again using educational attainment data from the UK, via a Tidy Tuesday set from January 2024. We’ll get the data and make the chart in the same code chunk.\n\n\nShow code for getting data and making the plots\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(tidytuesdayR) #to get data from tidy tuesday repo\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n## ggplot helpers - load if necessary\nlibrary(waffle) # ggplot helper to make a waffle chart\nlibrary(ggtext) # helper functions for ggplot text\n\n# load tidy tuesday data\nukeduc1 &lt;- tt_load(\"2024-01-23\")\n\n# get variable names\nukeduc_names &lt;- as_tibble(names(ukeduc1$english_education))\n\n# create tibble from csv, clean data\nukeduc &lt;- as_tibble(ukeduc1$english_education) %&gt;%\n    mutate(town11nm = ifelse(town11nm == \"Outer london BUAs\", \"Outer London BUAs\", town11nm)) %&gt;%\n    mutate(\n        ttwa_classification = ifelse(town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"),\n                                                                 \"Majority conurbation\", ttwa_classification)) %&gt;%\n    mutate(ttwa11nm = ifelse(town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"),\n                                                     \"London\", ttwa11nm)) %&gt;%\n    mutate(ttwa11cd = ifelse(town11nm %in% c(\"Inner London BUAs\", \"Outer London BUAs\"),\n                                                     \"E30000234\", ttwa11cd)) %&gt;%\n    mutate(across(26:29, ~ifelse(is.na(.),0,.))) %&gt;%\n    mutate(level_sum = rowSums(.[c(26:29)])) %&gt;%\n    mutate(highest_level_qualification_achieved_by_age_22_na = 100 - level_sum) %&gt;%\n    mutate(n_lesslev1_age22 =\n                    round(highest_level_qualification_achieved_by_age_22_less_than_level_1 * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev1to2_age22 =\n                    round(highest_level_qualification_achieved_by_age_22_level_1_to_level_2 * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev3to5_age22 =\n                    round(highest_level_qualification_achieved_by_age_22_level_3_to_level_5 * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev6plus_age22 =\n                    round(highest_level_qualification_achieved_by_age_22_level_6_or_above * (ks4_2012_2013_counts/100) ,0)) %&gt;%\n    mutate(n_lev_na_age22 =\n                    round(highest_level_qualification_achieved_by_age_22_na * (ks4_2012_2013_counts/100) ,0))\n\nglimpse(ukeduc)\n#&gt; Rows: 1,104\n#&gt; Columns: 38\n#&gt; $ town11cd                                                          &lt;chr&gt; \"E34…\n#&gt; $ town11nm                                                          &lt;chr&gt; \"Car…\n#&gt; $ population_2011                                                   &lt;dbl&gt; 5456…\n#&gt; $ size_flag                                                         &lt;chr&gt; \"Sma…\n#&gt; $ rgn11nm                                                           &lt;chr&gt; \"Eas…\n#&gt; $ coastal                                                           &lt;chr&gt; \"Non…\n#&gt; $ coastal_detailed                                                  &lt;chr&gt; \"Sma…\n#&gt; $ ttwa11cd                                                          &lt;chr&gt; \"E30…\n#&gt; $ ttwa11nm                                                          &lt;chr&gt; \"Wor…\n#&gt; $ ttwa_classification                                               &lt;chr&gt; \"Maj…\n#&gt; $ job_density_flag                                                  &lt;chr&gt; \"Res…\n#&gt; $ income_flag                                                       &lt;chr&gt; \"Hig…\n#&gt; $ university_flag                                                   &lt;chr&gt; \"No …\n#&gt; $ level4qual_residents35_64_2011                                    &lt;chr&gt; \"Low…\n#&gt; $ ks4_2012_2013_counts                                              &lt;dbl&gt; 65, …\n#&gt; $ key_stage_2_attainment_school_year_2007_to_2008                   &lt;dbl&gt; 65.0…\n#&gt; $ key_stage_4_attainment_school_year_2012_to_2013                   &lt;dbl&gt; 70.7…\n#&gt; $ level_2_at_age_18                                                 &lt;dbl&gt; 72.3…\n#&gt; $ level_3_at_age_18                                                 &lt;dbl&gt; 50.7…\n#&gt; $ activity_at_age_19_full_time_higher_education                     &lt;dbl&gt; 30.7…\n#&gt; $ activity_at_age_19_sustained_further_education                    &lt;dbl&gt; 21.5…\n#&gt; $ activity_at_age_19_appprenticeships                               &lt;dbl&gt; NA, …\n#&gt; $ activity_at_age_19_employment_with_earnings_above_0               &lt;dbl&gt; 52.3…\n#&gt; $ activity_at_age_19_employment_with_earnings_above_10_000          &lt;dbl&gt; 36.9…\n#&gt; $ activity_at_age_19_out_of_work                                    &lt;dbl&gt; NA, …\n#&gt; $ highest_level_qualification_achieved_by_age_22_less_than_level_1  &lt;dbl&gt; 0.0,…\n#&gt; $ highest_level_qualification_achieved_by_age_22_level_1_to_level_2 &lt;dbl&gt; 34.9…\n#&gt; $ highest_level_qualification_achieved_by_age_22_level_3_to_level_5 &lt;dbl&gt; 39.7…\n#&gt; $ highest_level_qualification_achieved_by_age_22_level_6_or_above   &lt;dbl&gt; 0.0,…\n#&gt; $ highest_level_qualification_achieved_b_age_22_average_score       &lt;dbl&gt; 3.32…\n#&gt; $ education_score                                                   &lt;dbl&gt; -0.5…\n#&gt; $ level_sum                                                         &lt;dbl&gt; 74.6…\n#&gt; $ highest_level_qualification_achieved_by_age_22_na                 &lt;dbl&gt; 25.4…\n#&gt; $ n_lesslev1_age22                                                  &lt;dbl&gt; 0, 0…\n#&gt; $ n_lev1to2_age22                                                   &lt;dbl&gt; 23, …\n#&gt; $ n_lev3to5_age22                                                   &lt;dbl&gt; 26, …\n#&gt; $ n_lev6plus_age22                                                  &lt;dbl&gt; 0, 8…\n#&gt; $ n_lev_na_age22                                                    &lt;dbl&gt; 17, …\n\n# cohort ed attainment by region\nukeduc2 &lt;-\n    ukeduc %&gt;%\n    rename(region =rgn11nm ) %&gt;%\n    group_by(region) %&gt;%\n    summarise(region_cohort = sum(ks4_2012_2013_counts),\n                        region_levless1_age22 = sum(n_lesslev1_age22),\n                        region_lev1to2_age22 = sum(n_lev1to2_age22),\n                        region_lev3to5_age22 = sum(n_lev3to5_age22),\n                        region_lev6plus_age22 = sum(n_lev6plus_age22)) %&gt;%\n    pivot_longer(cols = ends_with(\"age22\"), names_to = \"ed_attain_age22\", values_to = \"ed_attain_n\") %&gt;%\n    mutate(ed_attain_pct = ed_attain_n / region_cohort) %&gt;%\n    mutate(ed_attain_pct2 = round(ed_attain_pct*100)) %&gt;%\n    mutate(region = factor(region))%&gt;%\n    filter(!is.na(region)) %&gt;%\n    select(region, region_cohort, ed_attain_age22, ed_attain_pct2) %&gt;%\n    ## the data was a bit messy, so to get the values to sum to 100 to make the waffle\n    ## chart fill the plot area, I had to do a little reshaping after doing the percentages.\n    pivot_wider(names_from = ed_attain_age22, values_from = ed_attain_pct2) %&gt;%\n    mutate(region_sum = rowSums(.[c(3:6)])) %&gt;%\n    mutate(region_lev_na_age22 = ifelse(region_sum &lt; 100, 100 - region_sum, 0)) %&gt;%\n    select(-region_sum) %&gt;%\n    pivot_longer(cols = ends_with(\"age22\"), names_to = \"ed_attain_age22\", values_to = \"ed_attain_pct\") %&gt;%\n    mutate(ed_attain_age22 =\n                factor(ed_attain_age22,\n                             levels = c(\"region_lev_na_age22\", \"region_levless1_age22\", \"region_lev1to2_age22\",\n                                                 \"region_lev3to5_age22\", \"region_lev6plus_age22\"),\n                             labels = c(\"No data\", \"Level &lt;1\", \"Level = 1-2\",\n                                                 \"Level = 3-5\", \"Level = 6+\")))\nglimpse(ukeduc2)\n#&gt; Rows: 45\n#&gt; Columns: 4\n#&gt; $ region          &lt;fct&gt; East Midlands, East Midlands, East Midlands, East Midl…\n#&gt; $ region_cohort   &lt;dbl&gt; 39811, 39811, 39811, 39811, 39811, 48849, 48849, 48849…\n#&gt; $ ed_attain_age22 &lt;fct&gt; Level &lt;1, Level = 1-2, Level = 3-5, Level = 6+, No dat…\n#&gt; $ ed_attain_pct   &lt;dbl&gt; 2, 30, 41, 25, 2, 2, 27, 43, 27, 1, 2, 19, 43, 36, 0, …\n\n## chart\nukeduc2 %&gt;%\n    ggplot(aes(fill = ed_attain_age22, values = ed_attain_pct)) +\n    geom_waffle(na.rm=TRUE, n_rows=10, flip=TRUE, size = 0.33, colour = \"white\") +\n    facet_wrap(~region, nrow=1,strip.position = \"bottom\") +\n    scale_x_discrete() +\n    scale_y_continuous(labels = function(x) x * 10, # make this multiplyer the same as n_rows\n                                         expand = c(0,0)) +\n    scale_fill_brewer(palette = \"Set2\") +\n        labs(title = \"Students in London most likely to have at least 4-year degree by Age 22\",\n                 subtitle = \"Sixth Year Educational Outcomes for Level 4 2012-13 Cohort by UK Region&lt;br&gt;\n                 Each block = 1 %\",\n                 caption = \"*Tidy Tuesday data 01/23/2024, from UK Office of National Statistics*\",\n                 x = \"\", y = \"\") +\n    theme_minimal() +\n        theme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'),\n                    legend.key.width = unit(1.5, 'cm'), legend.margin=margin(-10, 0, 0, 0),\n                    plot.title = element_text(hjust = 0), plot.subtitle = element_markdown(),\n                    plot.caption = element_markdown(),\n                    panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    guides(fill = guide_legend(label.position = \"bottom\", \n                                            title = \"Cohort at Age 22\", \n                                            title.position = \"top\"))\n\n\n\n\n\n\n\n\n\nSo alright, a waffle chart. Similar insights from the horizontal bar, just visualized a bit differently.\ncreated and posted April 23, 2024"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html#prompt5",
    "href": "posts/30-day-chart-challenge-2024/index.html#prompt5",
    "title": "30 Day Chart Challenge 2024",
    "section": "Prompt #5 - Diverging",
    "text": "Prompt #5 - Diverging\nStaying with the theme of educational attainment, but switching countries to Denmark (where I was born and now live again), let’s look at differences in educational attainment for people in Denmark aged 25-69. The data come from Danmarks Statistik (Statistics Danmark, the national statistics agency) via the danstat package. I’ve been wanting to use the package, this was a great excuse.\nI won’t do too much explaining about education levels in Denmark, you can read up on them on the ministry’s page.\nThe danstat package is fairly easy to use once you get the hang of sorting out the table name and number, and variable names and values you need to filter on. It’s a good idea to start at Danmarks Statistik’s StatBank page, search the data you want, and when you find the table, you’ll see the code, in this case HFUDD11, and use that in the package calls. To see what I mean, let’s start with getting the data.\n\n\nShow code for getting data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(danstat) # package to get Danish statistics via api\nlibrary(ggtext) # enhancements for text in ggplot\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n\n# metadata for table variables, click thru nested tables to find variables and ids for filters\ntable_meta &lt;- danstat::get_table_metadata(table_id = \"hfudd11\", variables_only = TRUE)\n\n# create variable list using the ID value in the variable\nvariables_ed &lt;- list(\n    list(code = \"bopomr\", values = \"000\"),\n    list(code = \"hfudd\", values = c(\"H10\", \"H20\", \"H30\", \"H35\",\n                                        \"H40\", \"H50\", \"H60\", \"H70\", \"H80\", \"H90\")),\n    list(code = \"køn\", values = c(\"M\",\"K\")),\n    list(code = \"alder\", values = c(\"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                                                \"50-54\", \"55-59\", \"60-64\", \"65-69\")),\n    list(code = \"tid\", values = 2023))\n\n# past variable list along with table name. \n# note that in package, table name is lower case, though upper case on statbank page.\nedattain &lt;- get_data(\"hfudd11\", variables_ed, language = \"da\") %&gt;%\n    as_tibble() %&gt;%\n    select(sex = KØN, age = ALDER, edlevel = HFUDD, n = INDHOLD)\n\n\nThe data I pulled is by sex, age, and education level. I needed to age variable to filter for age 25+, as the set starts at age 15. At some point for another post I plan to look a bit more deeply at educational attainment here, including region and age and other fields. So let’s clean the data and get it ready to plot a diverging bar chart.\nEven though I’ve calculated percentages for sex by age by education level, I won’t be doing any breakdowns by age here, again, that’s a later independent post. But at least I have the code now.\n\n\nShow code for cleaning data\nedattain1 &lt;- edattain %&gt;%\n    ## create factors from the education levels\n    mutate(edlevel =\n        factor(edlevel,\n            levels = c(\"H10 Grundskole\", \"H20 Gymnasiale uddannelser\",\n            \"H30 Erhvervsfaglige uddannelser\", \"H35 Adgangsgivende uddannelsesforløb\",\n            \"H40 Korte videregående uddannelser, KVU\", \"H50 Mellemlange videregående uddannelser, MVU\",\n            \"H60 Bacheloruddannelser, BACH\", \"H70 Lange videregående uddannelser, LVU\",\n            \"H80 Ph.d. og forskeruddannelser\", \"H90 Uoplyst mv.\"),\n            labels = c(\"Grundskole/Primary\", \"Gymnasium\",\n            \"Erhvervsfaglige/Vocational HS\", \"Adgangsgivende/Qualifying\",\n            \"KVU/2-year college\", \"MVU/Professional BA\",\n            \"Bachelor\", \"LVU/Masters\", \"PhD\", \"Not stated\" ))) %&gt;%\n    mutate(sex = ifelse(sex == \"Kvinder\", \"Kvinder/Women\", \"Mænd/Men\")) %&gt;%\n    # calculate total number by sex\n    arrange(sex, edlevel) %&gt;%\n    group_by(sex) %&gt;%\n    mutate(tot_sex = sum(n)) %&gt;%\n    ungroup() %&gt;%\n    # calculate total number by sex and ed level\n    group_by(sex, edlevel) %&gt;%\n    mutate(tot_sex_edlev = sum(n)) %&gt;%\n    ungroup() %&gt;%\n    # calculate total number by sex and age\n    group_by(sex, age) %&gt;%\n    mutate(tot_sex_age = sum(n)) %&gt;%\n    ungroup() %&gt;%\n    # calculate percentages \n    mutate(level_pct = round(tot_sex_edlev / tot_sex, 3)) %&gt;%\n    mutate(level_pct = ifelse(sex == \"Mænd/Men\", level_pct *-1, level_pct)) %&gt;%\n    mutate(level_pct2 = round(level_pct * 100, 1)) %&gt;%\n    mutate(age_level_pct = round(n / tot_sex_age, 3)) %&gt;%\n    mutate(age_level_pct = ifelse(sex == \"Mænd/Men\", age_level_pct *-1, age_level_pct)) %&gt;%\n    mutate(age_level_pct2 = round(age_level_pct * 100, 1))\n\n\nNow let’s make the plot.\n\n\nShow code for making the plot\nvlines_df &lt;- data.frame(xintercept = seq(-100, 100, 20))\n\nedattain1 %&gt;%\n    filter(!edlevel == \"Not stated\") %&gt;%\n    distinct(sex, edlevel, .keep_all = TRUE) %&gt;%\n    select(sex, edlevel:tot_sex, level_pct, level_pct2 ) %&gt;%\n    # pass this temporary set thru to the ggplot call\n    {. -&gt;&gt; tmp} %&gt;%\n    ggplot() +\n    geom_col(aes(x = -50, y = edlevel), width = 0.75, fill = \"#e0e0e0\") +\n    geom_col(aes(x = 50, y = edlevel), width = 0.75, fill = \"#e0e0e0\") +\n    geom_col(aes(x = level_pct2, y = edlevel, fill = sex, color = sex), width = 0.75) +\n    scale_x_continuous(labels = function(x) abs(x), breaks = seq(-100, 100, 20)) +\n    geom_vline(data = vlines_df, aes(xintercept = xintercept), color = \"#FFFFFF\", size = 0.1, alpha = 0.5) +\n    coord_cartesian(clip = \"off\") +\n    scale_fill_manual(values = c(\"#C8102E\", \"#FFFFFF\")) +\n    scale_color_manual(values = c(\"#C8102E\", \"#C8102E\")) +\n    geom_text(data = subset(tmp, sex == \"Mænd/Men\"),\n                aes(x = level_pct2, y = edlevel, label = paste0(abs(level_pct2), \"%\")),\n                size = 5, color = \"#C8102E\",\n                hjust = 1, nudge_x = -.5) +\n    geom_text(data = subset(tmp, sex == \"Kvinder/Women\"),\n                aes(x = level_pct2, y = edlevel, label = paste0(abs(level_pct2), \"%\")),\n                size = 5, color = \"#C8102E\",\n                hjust = -.25) +\n    labs(x = \"\", y = \"\",\n             title = \"In Denmark, &lt;span style = 'color: #C8102E;'&gt;women&lt;/span&gt; more likely than men for highest education\n             level to be &lt;br&gt;Professional BA (MVU) or Masters.&lt;br&gt;&lt;br&gt;Men more likely to stop at primary level or vocational secondary diploma.\",\n             subtitle = \"&lt;br&gt;*Highest level of education attained by all people in Denmark aged 25-69 as of Sept 30 2023.\n             &lt;br&gt;&lt;span style = 'color: #C8102E;'&gt;Women are red bars&lt;/span&gt;, men are white.*\",\n             caption = \"*Data from Danmarks Statistik via danstat package*\") +\n    theme_minimal() +\n    theme(panel.grid = element_blank(), plot.title = element_markdown(),\n                plot.subtitle = element_markdown(), plot.caption = element_markdown(),\n                legend.position = \"none\",\n                axis.text.y = element_text(size = 10))\n\n\n\n\n\n\n\n\n\nThere it is, a simple, clean chart showing how highest educational attainment differs for women and men in Denmark. I’m definitely interested in exploring differences by age group and by region.\ncreated and posted April 24, 2024"
  },
  {
    "objectID": "posts/my-year-of-riding-danishly-pt2/index.html",
    "href": "posts/my-year-of-riding-danishly-pt2/index.html",
    "title": "My Year of Riding Danishly pt 2",
    "section": "",
    "text": "My commute bike\n\n\nI was pleasantly surprised by the response to the “My Year of Riding Danishly” post. It’s by far the most viewed post, owing in part to being mentioned on the R Weekly podcast. It’s also resulted in some follow-on work. Not bad for being the product of being forced to stay home while recuperating from my bike accident and needing a big project to dive into.\nIn addition to the podcast mention, I presented the work at the April 23rd Copenhagen R meetup. I put together the slides in Quarto, my first time using it for presentation slides. It took a bit of time figuring out some custom CSS and in-line tags, but I was happy with the result. You can see the html slides and the raw .qmd file on my github repo.\nIn the process of putting the slides together I decided I wanted to change how I presented the residuals and also to count and plot the number of unique days I rode in 2023. So let’s do that.\nFirst, the new residuals. Originally I plotted actuals vs predicted, so not really showing the residuals. My approach here is to plot the residuals (actual minus predicted) against the actuals to check for heteroscedasticity (variance of errors not constant). The code and results for the regression models are in the original post, so no need to repeat that code here. But I do show the code for the scatterplots and gt tables below\nThe code for the scatterplots is below, and gt tables to help explain the time and kilojoules residual plots will sit next to the tables. The code for the gt tables is after the graphics.\nWhen you look at the results, remember that a residual is the actual observation minus the predicted observation. Thus a positive residual means the model under-predicted the outcome, and a negative residual means the model over-predicted the outcome.\nAlso, if you want to see the plots in a bigger image, click on the plot…I have lightbox enabled on the site for easier graphic viewing if desired.\n\n\nShow code for scatterplot\n## time model\nride_models_time_df %&gt;%\n    ggplot(aes(x=moving_time_hms_dtm, y=moving_time_resid_hms)) +\n    geom_point() +\n    geom_smooth() +\n    scale_x_datetime(breaks = scales::breaks_width(\"15 min\"),labels=scales::date_format(\"%H:%M\")) +\n    labs(title = \"Slight under-prediction of ride-time for rides of fewer than 30 minutes\",\n             subtitle = \"*Residual = Actual - Predicted; &gt;0 = under-prediction, &lt;0 = over-prediction*\",\n        x = \"Moving time - actual (H/M/S)\", y = \"Moving time - residual (H/M/S)\") +\n    theme_minimal() +\n    theme(panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0.5, size = 12),\n                plot.subtitle = element_markdown(),\n                axis.text.x = element_markdown()) \n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat struck me when looking at the scatterplot was the hitch in the smoothing line at about 10-minute ride length. The model went from over-predicting to almost being eactly equal to the actual. To get a better sense of things I grouped the rides into four buckets and looked at the average residual. For rides under 15 minutes the model was over-predicting by seven seconds, noise essentially. For rides between 15-30 minutes the model was off by 32 seconds. The largest deviance was under-predicting the longer rides, those of 1+ hours.\n\nShow code for scatterplot\n## kilojoules model\nride_models_kjoules_df %&gt;%\n    ggplot(aes(x=kilojoules, y=kjoules_resid)) +\n    geom_point() +\n    geom_smooth() +\n    labs(title = \"Model starts to under-predict at ~125; slight over-prediction until then\",\n             subtitle = \"*Residual = Actual - Predicted; &gt;0 = under-prediction, &lt;0 = over-prediction*\",\n             x = \"Kilojoules - actual\", y = \"Kilojoules - residual\") +\n    theme_minimal() +\n    theme(panel.grid.minor = element_blank(),\n                plot.title = element_text(hjust = 0.5, size = 12),\n                plot.subtitle = element_markdown(),\n                axis.text.x = element_markdown())\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the kilojoules model there was also a hitch in the smoothing line at around 125 actual kilojoues. So again I put the rides into groups to see where the model was most sensitive. We can see that when I expended under 125 kilojoues the model was minimally over-prediccting. But at 126 to 500 the model under-predicted how much energy I would burn.\n\nShow code for scatterplot\n\n## watts model\nride_models_watts_df %&gt;%\n    ggplot(aes(x=average_watts, y=watts_resid)) +\n    geom_point() +\n    geom_smooth() +\n    labs(title = \"Model over-predicts for output to ~140, then starts to under-predict\",\n             subtitle = \"*Residual = Actual - Predicted; &gt;0 = under-prediction, &lt;0 = over-prediction*\",\n             x = \"Watts - actual\", y = \"Watts - residual\") +\n    theme_minimal() +\n    theme(panel.grid.minor = element_blank(),\n                plot.title = element_text(hjust = 0.5, size = 12),\n                plot.subtitle = element_markdown(),\n                axis.text.x = element_markdown())\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the watts model, the smoothing line was fairly, uh, smooth. So no need to dig too deeply. The model seemed to do alright predicting how much power I could generate.\n\n\n\nCode for the gt tables is here if you want to see it.\n\n\nShow code for gt tables\nride_models_time_df %&gt;%\n    group_by(ride_time_grp) %&gt;%\n    summarise(rides_n = n(),\n        residual_mean = mean(moving_time_resid)) %&gt;%\n    mutate(residual_mean = hms::hms(residual_mean)) %&gt;%\n    mutate(residual_mean = hms::round_hms(residual_mean, digits = 0)) %&gt;%\n    gt() %&gt;%\n    cols_label(ride_time_grp = \"Ride-Time Group\", rides_n = \"# of Rides\",\n                         residual_mean = md(\"Avg Residual&lt;br&gt;*(H/M/S)*\")) %&gt;%\n    cols_align(align = \"left\", columns = ride_time_grp) %&gt;%\n    cols_align(align = \"right\", columns = residual_mean) %&gt;%\n    tab_style(style = cell_fill(color = \"lightgrey\"), locations = cells_body(rows = c(2, 4))) %&gt;%\n    tab_header(title = md(\"*Average Residual Ride Time by Ride Time Group*\"),\n                         subtitle = md(\"*Residual = Actual - Predicted; &gt;0 = under-prediction, &lt;0 = over-prediction*\")) %&gt;% \n    gtsave(\"images/resid_time_table.png\")\n\nride_models_kjoules_df %&gt;%\n    group_by(kjoule_grp) %&gt;%\n    summarise(rides_n = n(),\n                        residual_mean = mean(kjoules_resid)) %&gt;%\n    gt() %&gt;%\n    cols_label(kjoule_grp = \"Kilojoule Group\", rides_n = \"# of Rides\",\n                         residual_mean = md(\"Avg Residual\")) %&gt;%\n    cols_align(align = \"left\", columns = kjoule_grp) %&gt;%\n    cols_align(align = \"right\", columns = residual_mean) %&gt;%\n    tab_style(style = cell_fill(color = \"lightgrey\"), locations = cells_body(rows = c(2, 4))) %&gt;%\n    tab_header(title = md(\"*Average Residual Kilojoules by Kilojoule Group*\"),\n                         subtitle = md(\"*Residual = Actual - Predicted; &gt;0 = under-prediction, &lt;0 = over-prediction*\")) %&gt;% \n    gtsave(\"images/resid_kjoule_table.png\")\n\n\nNow let’s look at the number of unique days ridden. I took some inspiration from Ryan Hart’s post that initially clued me into the rstrava app. He did a take on the github profile plot that shows how many days in a 12-month period you pushed commits, with darker coloring for more commits in a day.\nI used most of his code, streamlining a few things, like creating fewer dataframes and calling on the main dataframe to populate text and legends.\n\n\nShow code for setting up the data\n# subset the main dataset\nridedates1 &lt;- strava_data %&gt;%\n    filter(activity_year == 2023) %&gt;%\n    group_by(activity_date_p) %&gt;%\n    mutate(rides_day = n()) %&gt;%\n    ungroup() %&gt;%\n    select(distance_km, year = activity_year, month = activity_month_t, \n        day = activity_wday, date = activity_date_p, rides_day)\n\n# create 365 day date scaffold \nscaffold_df &lt;- \n    data.frame(date = date(seq(from = as.Date(\"2023-01-01\"), \n                        to = as.Date(\"2023-12-31\"), by = 1)))\n\n# join rides with date scaffold to show all days of the year and \n# add colors for fill based on km / day value\nridedates &lt;- full_join(ridedates1, scaffold_df) %&gt;%\n    mutate(distance_km = ifelse(is.na(distance_km), 0, distance_km)) %&gt;%\n    mutate(rides_day = ifelse(is.na(rides_day), 0, rides_day)) %&gt;%\n    group_by(date) %&gt;%\n    mutate(distance_day = sum(distance_km)) %&gt;%\n    distinct(date, .keep_all = TRUE) %&gt;%\n    ungroup() %&gt;%\n    mutate(color = case_when(distance_day == 0 ~ \"#171c22\",\n                    distance_day &gt; 0 & distance_day &lt;= 4.5 ~ \"#0E4429\",\n                    distance_day &gt; 4.5 & distance_day &lt;= 10 ~ \"#006D32\",\n                    distance_day &gt; 10 & distance_day &lt;= 20 ~ \"#26A642\",\n                    distance_day &gt; 20 ~ \"#39D354\")) %&gt;%\n    select(date, distance_day, rides_day, color)\n\n# for grid\nstart_day &lt;- as.Date(\"2023-01-01\")\nend_day &lt;- as.Date(\"2023-12-31\")\n\n# create main set for plotting the grid\ndf_grid &lt;- tibble(date = seq(start_day, end_day, by = \"1 day\")) %&gt;%\n    mutate(year = year(date),\n            month_abb = month(date, label = TRUE, abbr = TRUE),\n            day = wday(date, label = TRUE),\n            first_day_of_year = floor_date(date, \"year\"),\n            week_of_year = as.integer((date - first_day_of_year + wday(first_day_of_year) - 1) / 7) + 1) %&gt;%\n    left_join(ridedates) %&gt;%\n    arrange(date) %&gt;%\n    mutate(num = row_number()) %&gt;%\n    mutate(day = as.character(day)) %&gt;%\n    mutate(day = factor(day, levels = c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"))) %&gt;%\n    mutate(distance_year = round(sum(distance_day), 0)) %&gt;%\n    mutate(rides_year = sum(rides_day)) %&gt;%\n    mutate(ride_day = ifelse(rides_day &gt; 0, 1, 0)) %&gt;%\n    mutate(ride_days_unique = sum(ride_day)) %&gt;%\n    mutate(pct_days_ridden = round((ride_days_unique / 365) * 100, 1)) %&gt;%\n    select(-ride_day)\n\ndf_labels &lt;- df_grid %&gt;%\n    group_by(month_abb) %&gt;%\n    arrange(date) %&gt;%\n    filter(week_of_year == 1 | day == \"Sun\") %&gt;%\n    slice(1)\n\n# legend colors and text objects \ndf_legend &lt;- data.frame(y = c(-1, -1, -1, -1, -1),\n            x = c(44, 45, 46, 47, 48),\n            color = c(\"#171c22\", \"#0E4429\", \"#006D32\", \"#26A642\", \"#39D354\"))\n\ndf_legend_labels &lt;- data.frame(y = c(-1, -1),\n            x = c(43, 49),\n            label = c(\"Fewer km (black = 0)\", \"More km\"),\n            hjust = c(1, 0))\n\ndf_legend &lt;- data.frame(y = c(-1, -1, -1, -1, -1),\n            x = c(44, 45, 46, 47, 48),\n            color = c(\"#171c22\", \"#0E4429\", \"#006D32\", \"#26A642\", \"#39D354\"))\n\n\nNow that the data is sorted, let’s make a plot\n\n\nShow code for scatterplot\nggplot() +\n    statebins:::geom_rtile(data = df_grid,\n            aes(y = fct_rev(day), x = week_of_year, fill = color), radius = unit(1.75, \"pt\"),\n            color = \"white\", size = 1) +\n    statebins:::geom_rtile(data = df_legend,\n            aes(y = y, x = x, fill = color),\n            radius = unit(1.75, \"pt\"), color = \"#FFFFFF\", size = 1) +\n    geom_text(data = df_labels, aes(x = week_of_year, y = 8, label = month_abb),\n            hjust = 0.3, color = \"#848d97\", size = 3, check_overlap = TRUE) +\n    geom_text(data = df_grid, aes(x = -1.9, y = day, label = day),\n            color = \"#848d97\", size = 3, hjust = 0, check_overlap = TRUE) +\n    geom_text(data = df_legend_labels, aes(x, y, label = label, hjust = hjust),\n            color = \"#848d97\", size = 3) +\n    geom_text(data = df_grid,\n            aes(x = 0, y = -1, \n            label = paste0(\"Total kilometers ridden = \", scales::comma(distance_year))),\n            color = \"#848d97\", size = 4, hjust = 0) +\n    scale_y_discrete(breaks = c(\"Mon\", \"Wed\", \"Fri\")) +\n    expand_limits(y = c(-2, 9)) +\n    scale_x_continuous(expand = c(-2, NA)) +\n    scale_fill_identity() +\n    labs(\n        title = glue::glue(\"{df_grid$rides_year}\", \" rides this year on {df_grid$ride_days_unique} unique days, or {df_grid$pct_days_ridden} % of all days\"),\n        subtitle = \"Black square = no ride that day\",\n        caption = \"Strava data via rstrava app & Strava API\") +\n#   coord_equal() +\n    theme_void() +\n    theme(plot.title = element_text(size = 18, vjust = -6, color = \"#848d97\"),\n            plot.title.position = \"plot\",\n            plot.subtitle = element_text(size = 14, vjust = -7, \n                    color = \"#848d97\", margin = margin(t = 8, b = 10)),\n            plot.caption.position = \"plot\",\n            plot.caption = element_text(size = 9, color = \"#848d97\", vjust = 10,\n                    hjust = .9, margin = margin(t = 25)),\n            legend.position = \"none\",\n            plot.margin = unit(c(0.5, 1, 0.5, 1), \"cm\"),\n            plot.background = element_rect(color = NA, fill = \"#FFFFFF\"))\n\n\n\n\n\n\n\n\n\nThere you have it…206 unique days of riding, which is 56% of the year. I won’t come close to matching that this year, but hopefully in 2025, when I’m fully healed.\nPossible next steps for the this data could be comparing Denmark and California rides, using the gpx files to create my own maps…who knows. Hope this and the original post were an inpsiration on how to leverage data you create to tell a story.\n\n\n\nMy commute bike"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html#prompt6",
    "href": "posts/30-day-chart-challenge-2024/index.html#prompt6",
    "title": "30 Day Chart Challenge 2024",
    "section": "Prompt #6 - OECD",
    "text": "Prompt #6 - OECD\nStaying with the emerging theme of educational attainment, this prompt was a good excuse to try out the oecd package. The package wraps functions to access OECD data through their API for their Data Explorer platform.\nIt’s fairly straight forward to use…look up the table you want, set the filters, copy the API script and parse out as the package documentation shows. In the example they do not show it, but make sure if you set time parameters that you put it in the start_date and end_date calls. You can of course name the dataset and filters anything you want. I chose my names so they wouldn’t conflict with function calls.\nFor this chart I looked at educational attainment in Nordic countries; Denmark, Finland, Iceland, Norway, & Sweden (i), by sex for all people aged 25 to 64 in 2022. For the sake of expediency, I’ll copy the overall format & theme from prompt 1.\ni) let’s not get into here the difference between Nordic & Scandinavian, what constitutes Scandinavian & why…\nThe code for getting the data and making the chart is below.\n\n\nShow code for prompt 6\n## code for 30 Day Chart Challenge 2024, day 6 OECD\n## educational attainment via OECD package\n## nordics, by sex 25 to 64, 2020-2022\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(OECD) # package to get OECD data via api\nlibrary(ggtext) # enhancements for text in ggplot\n\n# table from website\n# https://www.oecd-ilibrary.org/education/data/education-at-a-glance/educational-attainment-and-labour-force-status_889e8641-en\n\nedattdata &lt;- \"OECD.CFE.EDS,DSD_REG_EDU@DF_ATTAIN,1.0\"\nedattfilter &lt;- \"A.CTRY.SWE+NOR+ISL+FIN+DNK...Y25T64.F+M.ISCED11_0T2+ISCED11_5T8+ISCED11_3_4..\"\n\noecd1 &lt;- get_dataset(edattdata, edattfilter, start_time = 2020, end_time = 2022)\n\n# get metadata - helpful to create labels for factors.\n# see package documentation on github \n# commented out here to speed up rendering of post html file...this takes a while to download\n   # oecd_metadata &lt;- get_data_structure(edattdata)\n\n# gets labels for country codes...this step commented out, data loading from local file.\n# datalabs_ct &lt;- oecd_metadata$CL_REGIONAL %&gt;%\n#   rename(COUNTRY = id)\n\ndatalabs_ct &lt;- readRDS(\"~/Data/r/30 Day Chart Challenge/2024/data/oecd_datalabs_ct.rds\") \n\n# keep only necessary vars, change some to factors & add labels\noecd &lt;- oecd1 %&gt;%\n    mutate(EDUCATION_LEV =\n        factor(EDUCATION_LEV,\n            levels = c(\"ISCED11_0T2\", \"ISCED11_3_4\", \"ISCED11_5T8\"),\n            labels = c(\"Pre-primary thru lower secondary\",\n                        \"Upper secondary and non-degree tertiary\", \"Tertiary education\"))) %&gt;%\n    mutate(SEX = case_when(SEX == \"M\" ~ \"Male\", SEX == \"F\" ~ \"Female\")) %&gt;%\n    mutate(pct_attain2 = as.numeric(ObsValue)) %&gt;%\n    mutate(pct_attain = pct_attain2 / 100) %&gt;%\n    # join to have labels instead of country abbrvs\n    left_join(datalabs_ct) %&gt;%\n    select(country = label,  year = TIME_PERIOD, SEX, EDUCATION_LEV, pct_attain, pct_attain2) %&gt;%\n    clean_names()\n\n## chart...horizontal stacked bar\noecd %&gt;%\n    filter(year == \"2022\") %&gt;%\n    ggplot(aes(pct_attain, fct_rev(country), fill = fct_rev(education_lev))) +\n    geom_bar(stat = \"identity\") +\n    scale_x_continuous(expand = c(0,0),\n                breaks = c(.01, 0.25, 0.50, 0.75, .97),\n                labels = c(\"0\", \"25%\", \"50%\", \"75%\", \"100%\")) +\n    facet_wrap(~ sex, nrow = 1) +\n    geom_text(aes(label = scales::percent(round(pct_attain , 2))),\n                position = position_stack(vjust = 0.5),\n                color= \"white\", vjust = 0.5, size = 5) +\n    labs(title = \"In Nordic countries, women age 25-64 more likely than men to complete college.&lt;br&gt;&lt;br&gt;\n             Finns have lowest levels of attainment stopping at lower secondary\",\n             subtitle = \"&lt;br&gt;*Educational Attainment in Nordic Countries, by Sex, ages 25-64 combined, 2022*\",\n             caption = \"*Data from OECD, via oecd package for r*\",\n             x = \"\", y = \"\") +\n    scale_fill_brewer(palette = \"Set2\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'),\n                legend.key.width = unit(1.5, 'cm'), legend.margin=margin(-10, 0, 0, 0),\n                plot.title = element_markdown(), plot.subtitle = element_markdown(),\n                plot.caption = element_markdown(),\n                axis.text.y = element_text(size = 12),\n                panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    guides(fill = guide_legend(label.position = \"bottom\", reverse = TRUE,\n                title = \"Education Levels\", title.position = \"top\"))\n\n\n\n\n\n\n\n\n\nYou can see that there are some differences in levels of attainment between countries. I don’t know enough context to speculate as to why. Also, as you add more countries to the filter, the education levels and age groups are aggregated into bigger buckets, presumably to facilitate comparisons. Denmark has a good national database where you can drill down more by age and level, so if I wanted to look at differences by smaller age groups and over time, I’d have to dig into the individual countries national statistics databanks and would have to hope that the age groups and attainment levels are similar enough.\ncreated and posted April 26, 2024"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html#prompt7",
    "href": "posts/30-day-chart-challenge-2024/index.html#prompt7",
    "title": "30 Day Chart Challenge 2024",
    "section": "Prompt #7 - Hazards",
    "text": "Prompt #7 - Hazards\nHazards is vague enough to mean anything, and if you search the socials for maps submitted for this prompt you’ll see a variety of approaches. Right away I thought of crime stats, and also right away I knew I wanted to make a map of crime data in Denmark.\nSo that’s what I set out to do.\nBut very soon into building it I realized I wanted to visualise different types of crime, which meant multiple plots. So then I got it in my head that this should be an exercise in really getting comfortable with functional programming and iterating using map() or a similar approach.\nSo that’s what I did.\nFollow along now as we get data, build a plot function, map over it, and make some maps.\nFirst, the data. As with the diverging challenge I got the data from Danmarks Statistik (Statistics Danmark, the national statistics agency). Unlike the diverging challenge, I did not end up using the danstat package.\nWhy? Well I wanted to use the province aggregation, the middle level between towns and regions. I wanted to see a bit of nuance in the maps. The problem is, while province data can be pulled from the StatBank data portal the API only has cities, regions, and all of Denmark. So loading flat-files it is.\nBut In this first chunk we get the geographic boundary data. For this we’ll be using NUTS3 level. What are the NUTS levels? (settle down, Beavis) Well, per the European Commission and Eurostat they are:\n\n“The Nomenclature of territorial units for statistics, abbreviated NUTS (from the French version Nomenclature des Unités territoriales statistiques) is a geographical nomenclature subdividing the economic territory of the European Union (EU) into regions at three different levels (NUTS 1, 2 and 3 respectively, moving from larger to smaller territorial units).\n\nIn Denmark NUTS1 is the entire country, NUTS2 are the major regions, and NUTS3 are the provinces. The process for doing this comes from one of the excellent tutorials by Milos Popvic in his Milos Makes Maps series of video and code how-tos.\n\n\nShow code for getting map boundary data\n# load all the packages we'll need\nlibrary(tidyverse) # to do tidyverse things\nlibrary(sf) # to make our geo data mappable\nlibrary(giscoR) # gets the region level boundries\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(danstat) # package to get Danish statistics via api\nlibrary(ggtext) # enhancements for text in ggplot\nlibrary(patchwork) # puts plots together\n\nsource(\"~/Data/r/basic functions.R\")\n\n### Get mapping data\n# define longlat projection\ncrsLONGLAT &lt;- \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\n\n# get NUTS3 data for Denmark and use sf to make it plottable\nnuts3_dk &lt;- giscoR::gisco_get_nuts(\n    year = \"2021\",  resolution = \"3\",\n    nuts_level = \"3\", country = \"DK\") |&gt;\n    rename(province_name = NAME_LATN) |&gt;\n    sf::st_transform(crsLONGLAT)\n\n\nNow we add the crime and population data via spreadsheet import, and join it to the map data. The crime data needs a bit of tidying up, shortening the names, some other text edits. We’ll normalize crime data by incidents per 100,000 people. Because the crime data come by yearly quarters and I wanted one entire year (adding up the four quarters), I pulled population data for the start of the 3rd quarter of 2023. I figured that was a good representative time-point for the entire year.\n\n\nShow code for getting crime and population data\ncrime_2023_1 &lt;- readxl::read_excel(\"~/Data/r/30 Day Chart Challenge/2024/data/dk_crime_by_province_2023.xlsx\") %&gt;%\n    clean_names() %&gt;%\n    fill(offence_cat_code) %&gt;%\n    fill(offence_cat_name) %&gt;%\n    rename_with(~ sub(\"^x\", \"tot_\", .x), starts_with((\"x\"))) %&gt;%\n    mutate(tot_2023 = rowSums(.[c(5:8)])) %&gt;%\n    mutate(province_name = str_replace(province_name, \"Province \", \"\")) %&gt;%\n    mutate(offence_cat_name = str_replace(offence_cat_name, \", total\", \"\")) %&gt;%\n    mutate(offence_cat_name = str_replace(\n        offence_cat_name, \"Nature of the offence\", \"All offences\")) %&gt;%\n    mutate(offence_cat_name = str_replace(\n        offence_cat_name, \"Offences against property\", \"Property crime\")) %&gt;%\n    mutate(offence_cat_name = str_replace(\n        offence_cat_name, \"Crimes of violence\", \"Violent crime\")) %&gt;%\n    mutate(offence_cat_name =\n            factor(offence_cat_name,\n            levels = c(\"All offences\", \"Criminal code\", \"Sexual offenses\", \"Violent crime\", \"Property crime\",\n                        \"Other offences\", \"Special acts\"))) %&gt;%\n    select(province_name, offence_cat_name, tot_2023)\n\n## get population data to normalize \n# based on total at start of 2023 Q3\npop_2023 &lt;- readxl::read_excel(\"~/Data/r/30 Day Chart Challenge/2024/data/dk_pop_2023_q3.xlsx\") %&gt;%\n    clean_names() %&gt;%\n    mutate(province_name = str_replace(province_name, \"Province \", \"\"))\n\n# join crime & population\ncrime_2023_2 &lt;- crime_2023_1 %&gt;%\n    left_join(pop_2023) %&gt;%\n    mutate(crime_per1k = round(tot_2023 / tot_pop * 100000, 0))\n\n## left_join nuts sf object to crime data to get sf object for plot\ncrime_2023 &lt;- nuts3_dk %&gt;%\n    left_join(crime_2023_2, by = \"province_name\")\n\n\nNow that we have a dataset ready to be mapped, it’s time to build the plot function. Being fully honest, functional programming and iterating over objects have long been my r bugaboos. I have no idea why, but I just couldn’t internalize the logic. But putting together the bicycle rides post where I employed Cedric Scherer’s tutorial I started to get it.\nIt took much longer to trial-and-error a few different approaches and get the text and layout as I wanted than it would have to copy-paste-adjust the plot code seven times and stitch together with patchwork, but now I have a map function I can re-use. A small but helpful part of the function is the ggsflabel package to help offset labels for Copenhagen (Byen København & Københavns omgen).\n\n\nShow code for the map function\ndk_crime_map &lt;- function(offence, maptitle) {\n    g &lt;-\n        ggplot() +\n        geom_sf(data = (crime_2023 %&gt;% filter(offence_cat_name== offence)),\n                        aes(fill = crime_per1k), color = \"#FFFFFF\", size = 3) +\n        geom_sf_text(data = (crime_2023 %&gt;%\n                    filter(province_name %notin% c(\"Byen København\", \"Københavns omegn\")) %&gt;%\n                    filter(offence_cat_name == \"Special acts\")),\n                        aes(label = province_name), nudge_x = -.5, size = 2)    +\n        ggsflabel::geom_sf_label_repel(data = (crime_2023 %&gt;%\n                    filter(province_name %in% c(\"Byen København\", \"Københavns omegn\")) %&gt;%\n                    filter(offence_cat_name == offence)),\n                        aes(label = province_name), size = 1.5,\n                        force = 1, nudge_x = 4, nudge_y = .75) +\n        scale_fill_gradient(trans = \"reverse\") +\n        labs(x = \"\", y = \"\") +\n        theme_minimal() +\n        ggtitle(maptitle) +\n        theme(panel.grid = element_blank(),\n                plot.title = element_text(size = 11, hjust = .6, vjust = -7),\n                axis.line = element_blank(), axis.ticks = element_blank(),\n                axis.text.x = element_blank(), axis.text.y = element_blank(),\n                legend.position = c(.4, -.2), \n                legend.title = element_text(size = 7),\n                legend.text = element_text(size = 7)\n                    ) +\n        guides(fill = guide_legend(\n            title = \"Incidents per 100K people\",\n            direction = \"horizontal\",\n            keyheight = unit(1, units = \"mm\"),\n            keywidth = unit(10, units = \"mm\"),\n            title.position = \"top\",\n            title.hjust = .5,\n            label.hjust = .5,\n            nrow = 1,\n            byrow = T,\n            reverse = F,\n            label.position = \"bottom\"\n        ))\n\n    return(g)\n}\n\n\nThe function is ready, now we iterate over the seven crime categories and put the plots together in one image. To magnify it for better viewing, click on it to bring it up in a lightbox.\n\n\nShow code for generating the plots\n## map over all crime categories\n# create list of crime types\ncrimecats &lt;- unique(crime_2023_2$offence_cat_name)\n\n# create plots, stitch together with patchwork\nwrap_plots(\n    map(crimecats, ~dk_crime_map(offence = .x, maptitle = .x)),\n    widths = 5, heights = 5) +\n    plot_annotation(\n        title = \"Crimes by Type and Province in Denmark, 2023\",\n        subtitle = \"*Total Crimes per 100K people*\",\n        caption = \"*Data from Danmarks Statistik. Criminal code = sexual offences + violence + property + other*\",\n        theme = theme(plot.subtitle = element_markdown(),\n                    plot.caption = element_markdown()))\n\n\n\n\n\n\n\n\n\nIf this were for an official company or organization report or academic publication there are some tweaks I’d want to make…the color palette, maybe writing a function to make the breaks better, spacing of province labels, maybe making it interactive with tool tips that show province name along with other data. I might do an inset or separate plots for Copenhagen. But for the purpose of the chart challenge I’m happy with it because it meets my goal, which was to build a plot function and iterate it over a vector, rather than copy-paste and edit the plot code.\ncreated and posted April 29, 2024"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html#prompt8",
    "href": "posts/30-day-chart-challenge-2024/index.html#prompt8",
    "title": "30 Day Chart Challenge 2024",
    "section": "Prompt #8 - Circular",
    "text": "Prompt #8 - Circular\nBecause prompt 7 took so much time, I’m cheating a bit with this prompt, and reusing two plots I made for the bicycle rides post.\nI wanted to visualise which hours and minutes I started my rides. The coord_polar() function turns a bar chart into a circle. Adding the right breaks and limits, and voila, first the hours of the day…\n\n\nShow code for circular time plots - hour\nstrava_data %&gt;%\n    filter(activity_year == 2023) %&gt;%\n    count(ride_type, activity_hour) %&gt;%\n    {. -&gt;&gt; tmp} %&gt;%\n    ggplot(aes(activity_hour, y = n, fill = ride_type)) +\n    geom_bar(stat = \"identity\") +\n    scale_x_continuous(limits = c(0, 24), breaks = seq(0, 24)) +\n    geom_text(data = subset(tmp, ride_type == \"Commute/Studieskolen\" & n &gt; 20),\n        aes(label= n), color = \"white\", size = 4) +\n    coord_polar(start = 0) +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"#0072B2\", \"#E69F00\", \"#CC79A7\"),\n                                        labels = c(\"Commute/&lt;br&gt;Studieskolen\", \"Other\", \"Workout\")) +\n    labs(x = \"\", y = \"\",\n             title = \"Most Rides During Morning and Evening Commuting Hours\",\n             subtitle = \"*Numbers Correspond to Hour of Day on a 24 hr clock*\") +\n    theme(legend.text = element_markdown(),\n                axis.text.y = element_blank(),\n                legend.title = element_blank(),\n                plot.title = element_text(size = 10, hjust = 0.5),\n                plot.subtitle = element_markdown(hjust = 0.5, size = 9))\nrm(tmp)\n\n\n\n…and the minutes.\n\n\nShow code for circular time plots - minute\nactivty_ampm %&gt;%\n    ggplot(aes(activity_min, y = activity_min_n, fill = ampm)) +\n    geom_col(position = position_stack(reverse = TRUE)) +\n    scale_x_continuous(limits = c(-1, 60), breaks = seq(0, 59), labels = seq(0, 59)) +\n    geom_text(data = subset(activty_ampm, activity_min_n &gt; 5),\n                        aes(label= activity_min_n), color = \"white\", size = 4, position = position_nudge(y = -1)) +\n    coord_polar(start = 0) +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"#E57A77\", \"#7CA1CC\"),\n                                        labels = c(\"AM\", \"PM\")) +\n    labs(x = \"\", y = \"\",\n             title = \"Most Morning Rides Started Between 12 & 30 Past the Hour &lt;br&gt;\n             Evening Rides More Evenly Spaced Through the Hour\",\n             subtitle = \"*Numbers Correspond to  Minutes of the Hour*\") +\n    theme(legend.text = element_markdown(),\n                axis.text.y = element_blank(),\n                legend.title = element_blank(),\n                plot.title = element_markdown(size = 10, hjust = 0.5),\n                plot.subtitle = element_markdown(hjust = 0.5, size = 9))\n\n\n\nThey aren’t perfect but they’re close enough and visualised what I wanted them to show…what time of the day did I start my bike rides in 2023. For more detail and context, read the bike rides post.\ncreated and posted April 29, 2024"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2024/index.html#prompt9",
    "href": "posts/30-day-chart-challenge-2024/index.html#prompt9",
    "title": "30 Day Chart Challenge 2024",
    "section": "Prompt #9 - Major/Minor",
    "text": "Prompt #9 - Major/Minor\nPlease note that in November 2024 Spotify deactivated many API end points, including the ones I demo in this post and in the Spotify post from February 2021. They are still dead as of late March 2025, and I have no idea when or if they will reactivate the API end points.\nThe very first thing that came to mind for this plot prompt was to look at differences in music between major and minor keys. That provided me with the perfect excuse to use the spotifyr package again, as I did in my post analyzing the music from my band Gosta Berling.\nAs I already had the code I needed to authorize API access through the package, I figued it would be easy, and this would be at most a couple of hours of work.\nNot so much.\nSparing the detail, let’s leave it by noting in order to get access to the playlist endpoint I had to redo the access code and that took a while. Then I had to figure out a way to overcome the rate limit for getting audio features. That took some time. But it’s all done, so let’s get some data and build some charts.\nIn my other Spotify-based post I addressed the ethical issues with using the platform, so I won’t rehash here except to say it’s a great tool for listening to and discovering music, but please, actually buy music from the artists you like or discover on the platform, especially independent artists.\nAnyway, Spotify grants access to the API and gives you an id and secret code. The code below shows how to store and access the credentials.\n\n\nShow code for getting playlist data - pt1\nlibrary(tidyverse)\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(spotifyr) # functions to access spotify API\n\n\n# if you need to add spotify API keys\n# usethis::edit_r_environ()\n# as SPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx'\n# as SPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx'\n\n# identify the scopes, or end-points, that you want.\n# read the spotify api documentation to decide on scopes\n# for playlists we need these\nscopes = c(\n    \"user-library-read\",\n    \"user-read-recently-played\",\n    \"playlist-read-private\",\n    \"playlist-read-collaborative\",\n    \"user-read-private\")\n\n# uses the scopes and credentials to authorise the session.\nauth &lt;- spotifyr::get_spotify_authorization_code(\n    Sys.getenv(\"SPOTIFY_CLIENT_ID\"),\n    Sys.getenv(\"SPOTIFY_CLIENT_SECRET\"),\n    scopes)\n\n# pulls a list of your playlists..default is 20, you can get up to 50\nplaylists &lt;- spotifyr::get_user_playlists('dannebrog13', limit = 20,\n    offset = 0, authorization = auth)\n\n# alternately, go to spotify and get the link to the playlist (click on the ... and select share)\n# https://open.spotify.com/playlist/6mlrhYRlFv3Ji5xwzqFWwQ?si=966ddb877c414599\n\n# The playlist code you need is after \"playlist/\" and goes up to the ?\n\n# set the playlist code as an object\nlikeddid &lt;- \"6mlrhYRlFv3Ji5xwzqFWwQ\"\n\n\nOk, we have the info we need to get the list of songs in the playlist. To do that I amended code I found in this post, and changed the function options for the get_playlist_tracks call.\n\n\nShow code for getting playlist data - pt2\nall_liked_tracks &lt;-\n    ceiling(get_playlist_tracks(\"6mlrhYRlFv3Ji5xwzqFWwQ\", include_meta_info = TRUE)[['total']] / 50) %&gt;%\n    seq() %&gt;%\n    map(function(x) {\n        get_playlist_tracks(\"6mlrhYRlFv3Ji5xwzqFWwQ\",\n        fields = NULL,\n        limit = 50, offset = (x - 1) * 50,\n        market = NULL,\n        authorization = get_spotify_access_token(),\n        include_meta_info = FALSE)\n    }) %&gt;% reduce(rbind)\n\n\nI would recommend saving successful pulls as local rds files. You run the risk of hitting your rate limit if you call the API too many times.\nNow that we have a list of songs, we want to get the audio features. These are measures that Spotify has attached to tracks to indicate things like “danceability”, “energy”, “valence” (essentially happiness) & other items. Read the Spotify developers’ guide for more detail.\nGetting the audio features for a long list like this one, 1900+ entries, poses a challenge when the rate limit per call is 100 tracks. I was trying to figure out a way to loop over chunks of 100 with Sys.sleep() call but couldn’t get it to work. Luckily, someone figured out the solution to get around the rate limit that takes hardly any time. I won’t copy the code here, you can find it on the package’s repo on the issues tab.\nWhat I’ll show below is the code to merge the track details with the audio features into a nice tidy dataframe. Read the comments in the code to see the process.\n\n\nShow code for getting playlist data - pt3\n# initial version of df. contains a couple of nested lists that need unpacking. easier to do in stages.\nliked_songs1 &lt;- liked_track_features %&gt;%\n# assign key number values to character values.\n    mutate(key_name = case_when(key == 0 ~ \"C\", key == 1 ~ \"C#/Db\", key == 2 ~ \"D\",\n            key == 3 ~ \"D#/Eb\", key == 4 ~ \"E\", key == 5 ~ \"F\",\n            key == 6 ~ \"F#/Gb\", key == 7 ~ \"G\", key == 8 ~ \"G#/Ab\",\n            key == 9 ~ \"A\", key == 10 ~ \"A#/Bb\", key == 11 ~ \"B\")) %&gt;%\n    mutate(mode_name = case_when(mode == 0 ~ \"Minor\", mode == 1 ~ \"Major\")) %&gt;%\n    mutate(key_mode = paste(key_name, mode_name, sep = \" \")) %&gt;%\n    select(track.id = id, key_name:key_mode, time_signature, tempo, duration_ms,\n                 danceability, energy, loudness, speechiness:valence,\n                 key, mode, type, uri, track_href, analysis_url) %&gt;%\n    # join to playlist track information\n    left_join(all_liked_tracks) %&gt;%\n    #remove a few variables we won't need\n    select(-video_thumbnail.url, -track.episode, -added_by.href:-added_by.external_urls.spotify) \n\n# unnest some columns\nliked_songs &lt;- unnest_wider(liked_songs1, track.artists, names_sep = \"_\") %&gt;%\n    select(-track.artists_external_urls.spotify, -track.artists_uri, -track.artists_type, -track.artists_href) %&gt;%\n    mutate(track.artist_names = map_chr(track.artists_name, toString)) %&gt;%\n    separate(track.artist_names, paste0('track.artist', c(1:6)), sep = ',', remove = F) %&gt;%\n    mutate(across(56:60, str_trim)) %&gt;%\n    select(track.id:track.artists_id, track.artist_names:track.artist6, everything(), -track.artists_name)\n\n## At this point, save the file as an rds!!\n\n\nBTW, the playlist in question is my “liked songs”, those I starred or hearted or whatever. To make it a list I could access via API I had to copy from the static list Spotify makes (but isn’t in the API) into a user list. You can find it here. There is of course some selection bias in the list…is the track on Spotify, have I either come across it & liked it or remembered to because I wanted a list like this for comfort listening…etc. It’s not perfectly representative of the music I like the most, but close enough.\nMy most liked songs are from artists like R.E.M., The National, The Hold Steady, Superchunk, Replacements, Paul Kelly, The Feelies, Teenage Fanclub, Yo La Tengo, Robyn Hitchcock…so you get the idea of sounds for when we look at the plots of audio features.\nNow it’s time to make a chart. I decided to visualise the differences in audio feature values between major and minor key songs. So first I created a dataframe with all the summary stats.\n\n\nShow code for getting visualisation data ready\nliked_songs_summary &lt;-\nliked_songs %&gt;%\n    rename(duration.ms = duration_ms) %&gt;%\n    # change loudness to abs value for better scaling in plots\n    mutate(loudness = abs(loudness)) %&gt;%\n    group_by(mode_name) %&gt;%\n    summarise_at(vars(tempo:valence),\n        list(mean = mean,\n        q25 = ~quantile(., 0.25),\n        med = median,\n        q75 = ~quantile(., 0.75),\n        min = min, max = max)) %&gt;%\n    pivot_longer(-mode_name,\n        names_to = \"var_measure\",\n        values_to = \"value\") %&gt;%\n    separate_wider_delim(var_measure, \"_\", names = c(\"var\", \"measure\")) %&gt;%\n    #mutate(value = round(value, 2)) %&gt;%\n    # change duration to seconds for easier explanation\n    mutate(value = ifelse(var == \"duration.ms\", value / 1000, value)) %&gt;%\n    mutate(value = round(value, 2)) %&gt;%\n    mutate(var = ifelse(var == \"duration.ms\", \"duration_sec\", var))\n\n\nNow to make charts. I decided on lollipop charts to show the min and max values at either end, and the mean value along the lollipop stem. I thought I’d be able to write the basic chart as a function or put them all in one faceted cahart, but the audio features have different scales. Most are 0-1, but tempo (beats per minute) gets up to the 200s, loudness (in decibels) goes -60-0.\nThen I tried to make a function for the axis scale to respond to the audio feature, but it wasn’t working so in the end it was easier to copy and amend the code as needed for the features. First up, we do all the 0-1 scale features as a faceted plot.\nClick on the chart to enlarge it.\n\n\nShow code for audio features plot 1\nlibrary(ggtext) # enhancements for text in ggplot\n\nliked_songs_summary %&gt;%\n    filter(var %in% c(\"acousticness\", \"danceability\", \"energy\", \"instrumentalness\",\n                                     \"liveness\", \"speechiness\", \"valence\")) %&gt;%\n    pivot_wider(names_from = measure, values_from = value) %&gt;%\n    ggplot() +\n    geom_segment(aes(x= min, xend=max, y=fct_rev(mode_name), yend=fct_rev(mode_name)), color=\"grey\") +\n    geom_point( aes(x=min, y=mode_name), color=\"#4E79A7\", size=3 ) +\n    geom_point( aes(x=max, y=mode_name), color=\"#79A74E\", size=3 ) +\n    geom_point( aes(x=mean, y=mode_name), color=\"#A74E79\", size=3 ) +\n    scale_x_continuous(limits = c(0, 1), breaks = scales::pretty_breaks(4)) +\n    labs(x = \"\", y = \"\", title = \"Spotify defined audio feature values for Liked Songs playlist by major & minor key tracks\",\n             subtitle = \"&lt;span style='color: #4E79A7;'&gt;Min&lt;/span&gt; \\n&lt;span style='color: #A74E79;'&gt;Mean&lt;/span&gt; \\n&lt;span style='color: #79A74E;'&gt;Max&lt;/span&gt;\",\n             caption = \"*Data from Spotify API via spotifyr package*\") +\n    theme_minimal() +\n    theme(panel.grid = element_blank(),\n                plot.subtitle = element_markdown(size = 12),\n                plot.caption = element_markdown(),\n                strip.text.x = element_text(size = 10 )) +\n    facet_wrap(~ var, ncol = 2, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nThere’s not a ton of difference between major and minor key songs, only for liveness, which tries to measure the presence of an audience, and speechiness, or is the singing more talky or not. I was a bit surprised that valence (Spotify’s happiness measure) was the same for both.\nLet’s now do tempo (beats per minute), duration (in seconds), and loudness (in decibels, “the quality of a sound that is the primary psychological correlate of physical strength [amplitude]”).\n\n\nShow code for audio features plot2-4\n# var == \"tempo\" ~ c(60, 210),\nliked_songs_summary %&gt;%\n    filter(var == \"tempo\") %&gt;%\n    select(-var) %&gt;%\n    pivot_wider(names_from = measure, values_from = value) %&gt;%\n    ggplot() +\n    geom_segment(aes(x= min, xend=max, y=fct_rev(mode_name), yend=fct_rev(mode_name)), color=\"grey\") +\n    geom_point( aes(x=min, y=mode_name), color=\"#4E79A7\", size=3 ) +\n    geom_point( aes(x=max, y=mode_name), color=\"#79A74E\", size=3 ) +\n    geom_point( aes(x=mean, y=mode_name), color=\"#A74E79\", size=3 ) +\n    scale_x_continuous(limits = c(0, 220), breaks = scales::pretty_breaks(4)) +\n    labs(x = \"\", y = \"\", title = \"Minimal difference in tempo values between major & minor key songs\",\n             subtitle = \"*Audio feature: tempo (beats per minute) &lt;br&gt;\n             x axis scale reflects min & max values for playlist.*\",\n             caption = \"*Data from Spotify API via spotifyr package*\") +\n    annotate(geom = \"richtext\",\n                     label = \"&lt;span style='color: #4E79A7;'&gt;Min&lt;/span&gt; \\n&lt;span style='color: #A74E79;'&gt;Mean&lt;/span&gt; \\n&lt;span style='color: #79A74E;'&gt;Max&lt;/span&gt;\",\n                     x = 150, y = 1.5) +\n    theme_minimal() +\n    theme(panel.grid = element_blank(),\n                plot.subtitle = element_markdown(size = 12),\n                plot.caption = element_markdown(),\n                axis.text.y = element_text(size = 12))\n\n\n\n\n\n\n\n\n\nShow code for audio features plot2-4\n\n# var == \"duration\" ~ c(60, 1100))\n#duration &lt;-\nliked_songs_summary %&gt;%\n    filter(var == \"duration_sec\") %&gt;%\n    select(-var) %&gt;%\n    pivot_wider(names_from = measure, values_from = value) %&gt;%\n    ggplot() +\n    geom_segment(aes(x= min, xend=max, y=fct_rev(mode_name), yend=fct_rev(mode_name)), color=\"grey\") +\n    geom_point( aes(x=min, y=mode_name), color=\"#4E79A7\", size=3 ) +\n    geom_point( aes(x=max, y=mode_name), color=\"#79A74E\", size=3 ) +\n    geom_point( aes(x=mean, y=mode_name), color=\"#A74E79\", size=3 ) +\n    scale_x_continuous(limits = c(0, 1100), breaks = scales::pretty_breaks(5)) +\n    labs(x = \"\", y = \"\", title = \"Minimal difference in average length, though longest song is major key.\",\n             subtitle = \"*Audio feature: duration (in seconds)*\",\n             caption = \"*Data from Spotify API via spotifyr package*\") +\n    annotate(geom = \"richtext\",\n                     label = \"&lt;span style='color: #4E79A7;'&gt;Min&lt;/span&gt; \\n&lt;span style='color: #A74E79;'&gt;Mean&lt;/span&gt; \\n&lt;span style='color: #79A74E;'&gt;Max&lt;/span&gt;\",\n                     x = 700, y = 1.5) +\n    theme_minimal() +\n    theme(panel.grid = element_blank(),\n                plot.subtitle = element_markdown(size = 12),\n                plot.caption = element_markdown(),\n                axis.text.y = element_text(size = 12))\n\n\n\n\n\n\n\n\n\nShow code for audio features plot2-4\n\n# var == \"loudness\" ~ c(0, 30),\n#loud &lt;-\nliked_songs_summary %&gt;%\n    filter(var == \"loudness\") %&gt;%\n    select(-var) %&gt;%\n    pivot_wider(names_from = measure, values_from = value) %&gt;%\n    ggplot() +\n    geom_segment(aes(x= min, xend=max, y=fct_rev(mode_name), yend=fct_rev(mode_name)), color=\"grey\") +\n    geom_point( aes(x=min, y=mode_name), color=\"#4E79A7\", size=3 ) +\n    geom_point( aes(x=max, y=mode_name), color=\"#79A74E\", size=3 ) +\n    geom_point( aes(x=mean, y=mode_name), color=\"#A74E79\", size=3 ) +\n    scale_x_continuous(limits = c(0, 60)) +\n    labs(x = \"\", y = \"\", title = \"Minimal difference in loudness profile between major & minor key songs\",\n             subtitle = \"*Audio feature: loudness. &lt;br&gt;Original values are -60db-0db (decibels). Absolute value use for scaling.*\",\n             caption = \"*Data from Spotify API via spotifyr package*\") +\n    annotate(geom = \"richtext\",\n                     label = \"&lt;span style='color: #4E79A7;'&gt;Min&lt;/span&gt; \\n&lt;span style='color: #A74E79;'&gt;Mean&lt;/span&gt; \\n&lt;span style='color: #79A74E;'&gt;Max&lt;/span&gt;\",\n                     x = 30, y = 1.5) +\n    theme_minimal() +\n    theme(panel.grid = element_blank(),\n                plot.subtitle = element_markdown(size = 12),\n                plot.caption = element_markdown(),\n                axis.text.y = element_text(size = 12))\n\n\n\n\n\n\n\n\n\nSo there you have it. An approach to visualising audio features. There’s more to do here, but that will have to wait for an even more in-depth post. This turned out to be longer than I planned.\np.s. - yes, I know I’m now posting these after the chart challenge is officially over. but they are good exercises for me to keep learning and creating. and i’m enjoying it, so…\ncreated and posted May 2, 2024"
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html",
    "href": "posts/if-you-sell-it-will-they-come/index.html",
    "title": "If You Sell It, Will They Come?",
    "section": "",
    "text": "An almost sold-out Groupama Stadium, Lyon v Nice, November 2022. Photo by author."
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html#intro",
    "href": "posts/if-you-sell-it-will-they-come/index.html#intro",
    "title": "If You Sell It, Will They Come?",
    "section": "Introduction",
    "text": "Introduction\nReading a post on a football blog comparing average attendance rates across leagues in Europe with Major League Soccer (MLS) in the US, I thought that the analysis needed a bit more than averages. Looking at stadium capacity raised more interesting questions, including:\n\nWhat percentage of available tickets were accounted for, or to flip the question, how much of the stadium was full?\nAre there differences between leagues? By clubs within each league?\n\nWhy does capacity matter? To start with it’s a measure of demand based on supply. How many of your supply of seats do people want to purchase?\nAlso, as anyone who’s performed on stage can tell you, it’s better to play a room that’s too small and have a happening sold-out vibe than to play a room that’s too large where the impression is left that you’re not a strong draw. When MLS started in the mid-1990s, almost every team was playing in their city’s American football stadium. These stadiums could hold upwards of 50,000 people and MLS clubs were maybe drawing 10,000. A dead vibe that looked bad on television and felt inert if you were at a match.\nWatch a European match at a sold-out ground, or an MLS match now from Portland, Philadelphia, Columbus, or any other smaller soccer-specific ground. Electric vibes. You want to be there and be part of a happening."
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html#the-data",
    "href": "posts/if-you-sell-it-will-they-come/index.html#the-data",
    "title": "If You Sell It, Will They Come?",
    "section": "The Data",
    "text": "The Data\nTo do the analysis I needed match attendance and stadium capacity. Match attendance comes from FBRef via the worldfootballR package. Stadium data was scraped from from Wikipedia pages. I’ve pulled data for MLS and eleven 1st division leagues in Europe: the 5 major top flights in England, France, Germany, Italy & Spain; along with Belgium, Denmark (of course), Netherlands, Portugal, Scotland, Sweden, and Switzerland. Attendance figures are only for league matches. Champions League & other UEFA tournaments, and league cup matches (e.g. FA Cup, Coppa Italia, etc) are not included. Promotion and relegation playoff matches are included if the home team is in the top-flight league.\nBecause Sweden, like MLS, runs a spring-fall schedule I’m using the 2022-23 season figures for better comparison, even though the 2023-24 season just ended in all other Euro leagues. I’d hoped to add football-mad Turkey but 90% of the matches did not have any attendance numbers listed on FBRef.\nIt’s worth noting a big caveat about attendance figures; in the US it’s generally reported as tickets accounted for (sold, given away, etc), not the turnstile count. Most Premier League clubs also use that method. I’m not sure how attendance is counted in other leagues. So if you wonder why your favorite team shows a higher average capacity than the eyeball test of vacant seats, that’s why.\nIt’s also worth noting that the FBRef data may not be 100% accurate. In putting together the France data for instance, I noticed that some stadiums listed as venues were incorrect. For Denmark they transposed the 2nd stage groups, championship and relegation. In each case I let them know and they say they are correcting the problem matches. I only spotted those errors, though, because I know the leagues well enough having lived in France for a bit and living now in Denmark.\nThe stadium data from Wikipedia also needed edits, as stadium capacity can fluctuate, especially for stadiums undergoing renovation. I knew of a few reduced capacities (e.g. at Real Madrid in 2022-23) but I’m sure I missed some in other leagues. If you see any issues there, let me know.\nAnother data challenge was inconsistent naming of stadiums between FBRef and Wikipedia. I could only fix the names after doing the initial join, seeing what didn’t match, and correcting from there. Thanks to naming rights deals, stadium names can change. In general I defaulted to the traditional name of the stadium rather than the current sponsored name, for example the Borussia Dortmund stadium is currently Signal Iduna Park but I’ve standardized the name to the original Westfalenstadion."
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html#plan",
    "href": "posts/if-you-sell-it-will-they-come/index.html#plan",
    "title": "If You Sell It, Will They Come?",
    "section": "Outline",
    "text": "Outline\nThe plan for this post is to:\n\nShow how I got the stadium information and match data, cleaned & joined the sets, and created functions to prep the data for visualization and for the plots. It’s always better to write a function if you know you’ll be repeating a process.\nDisplay the top-line figures for all leagues - average attendance, stadium capacity, and average percent capacity for the season.\nAfter that we’ll look at each league individually by way of visualizations displaying by-team numbers compared to the league average.\nFor the individual league snapshots there will be two sets of tabsets, the top 5 Euro leagues and MLS in one tabset, and the smaller Euro leagues in the other.\n\nI’ll show code along the way where it makes sense. If you’re more interested in the results than the process, use the table of contents to the right to skip ahead to the charts. Code is mostly folded up, to see it click where it reads “Show code for (process)”. If you want to see all of the code, head over to the github repo."
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html#summary",
    "href": "posts/if-you-sell-it-will-they-come/index.html#summary",
    "title": "If You Sell It, Will They Come?",
    "section": "Summary",
    "text": "Summary\nYes, it’s a bit of a long post, so here are the highlights:\n\nThe Premier League & Bundesliga have the strongest overall demand, with most clubs exceeding 90% capacity.\nMost MLS clubs are around around 85%, with many clubs above that level.\nThe top clubs in France, Italy, & Spain are around 80%-90% capacity but many clubs struggle to reach even 70% capacity.\nIn the smaller leagues, there is generally lower average capacity, but much more variance between clubs. Some clubs are above 80%, some below 50%.\nOverall then, demand for MLS tickets compares favorably with European leagues. It’s not as strong league-wide as the Premier League or Bundesliga, but it’s close. And MLS does better on average than the other major European leagues."
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html#stadium",
    "href": "posts/if-you-sell-it-will-they-come/index.html#stadium",
    "title": "If You Sell It, Will They Come?",
    "section": "Getting the stadium data",
    "text": "Getting the stadium data\nOne good outcome of this project is that I now have my wikipedia table scraping process sorted. It’s fairly easy, using the httr, rvest, and `polite packages. I found stadium info for each country by searching “list of football stadiums in {country name}”. The data wasn’t consistent across country pages, and some pages had separate tables for larger and smaller stadiums. I’ll show the code for Germany, which was two tables. I used this post from Isabella Velásquez as a guide.\n\n\nShow code for wikipedia scraping\n# load the packages\nlibrary(rvest)\nlibrary(httr)\nlibrary(polite)\nlibrary(tidyverse)\nlibrary(janitor) # esp for clean_names() function\n\n# set the url object\nger_url &lt;- \"https://en.wikipedia.org/wiki/List_of_football_stadiums_in_Germany\"\n\n# politely say hello, make sure we adhere to polite scraping rules\nger_url_bow &lt;- polite::bow(ger_url)\nger_url_bow\n\n# get the data into a list, and pull out the html nodes \n# (see Isabella's post for the how-to)\nger_stad_html &lt;-\n    polite::scrape(ger_url_bow) %&gt;%  # scrape web page\n    rvest::html_nodes(\"table.wikitable.sortable\") %&gt;% # pull out specific table\n    rvest::html_table(fill = TRUE)\n\n# pull the 1st table we want, change capacity to numeric, save the columns we'll need\nger_stad_df1 &lt;-\n    ger_stad_html[[1]] %&gt;%\n    clean_names() %&gt;%\n    mutate(capacity = str_split(capacity, \"\\\\[\", simplify=T)[,1]) %&gt;%\n    mutate(capacity = as.numeric(gsub(\",\", \"\", as.character(capacity)))) %&gt;%\n    select(stadium, city, state, capacity, team = tenants, opened)\n\n# pull the second table, same cleaning functions. \nger_stad_df2 &lt;-\n    ger_stad_html[[2]] %&gt;%\n    clean_names() %&gt;%\n    mutate(capacity = str_split(capacity, \"\\\\[\", simplify=T)[,1]) %&gt;%\n    mutate(capacity = as.numeric(gsub(\",\", \"\", as.character(capacity)))) %&gt;%\n    add_column(state = \"\") %&gt;%\n    add_column(opened = 0) %&gt;%\n    mutate(opened = as.integer(opened)) %&gt;%\n    select(stadium, city = location, state, capacity, team = tenants, opened)\n\n# join the tables, fix some names. \nger_stad_df &lt;- ger_stad_df1 %&gt;%\n    rbind(ger_stad_df2) %&gt;%\n    mutate(stadium = ifelse(\n        stadium == \"Deutsche Bank Park  (Waldstadion)\", \"Waldstadion\", stadium)) %&gt;%\n    mutate(stadium = ifelse(\n        stadium == \"Borussia-Park\", \"Stadion im Borussia-Park\", stadium)) %&gt;%\n    mutate(stadium = ifelse(\n        stadium == \"Signal Iduna Park  (Westfalenstadion)\", \"Westfalenstadion\", stadium)) %&gt;%\n    mutate(stadium = ifelse(\n        stadium == \"MHPArena  (Neckarstadion)\", \"Neckarstadion\", stadium)) %&gt;%\n    mutate(stadium = ifelse(\n        stadium == \"RheinEnergieStadion  (Müngersdorfer Stadion)\", \"Müngersdorfer Stadion\", stadium)) %&gt;%\n    mutate(stadium = ifelse(\n        stadium == \"Veltins-Arena  (Arena AufSchalke)\", \"Arena AufSchalke\", stadium)) %&gt;%\n    mutate(stadium = ifelse(\n        stadium == \"PreZero Arena  (Rhein-Neckar-Arena)\", \"Rhein-Neckar-Arena\", stadium))\n\n## repeat for all other countries"
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html#attendance",
    "href": "posts/if-you-sell-it-will-they-come/index.html#attendance",
    "title": "If You Sell It, Will They Come?",
    "section": "Getting the attendance figures",
    "text": "Getting the attendance figures\nNow that we have the stadium data we’ll get the FBRef data via worldfootballR and join the datasets. It’s just one line of code to get the data:\nger_match_2023 &lt;- fb_match_results(country = \"GER\", gender = \"M\", season_end_year = 2023, tier = \"1st\")\nRepeat for all other countries.\nAs I mentioned, the join step necessitated lots of data cleaning on stadium names, which took some detective work. I needed to track down stadium names free of current naming rights holders, and change FBRef’s use of shorthand team names. We’ll look at Germany for an example. If you want to see how I handled other leagues, look at the individual scripts in the Github repo.\n\n\nShow code for data join\nger_match_2023  %&gt;%\n    # remove hamburg's relegation playoff match\n    filter(!Home == \"Hamburger SV\") %&gt;%\n    mutate(wk_n = as.numeric(Wk)) %&gt;%\n    mutate(match_week = ifelse(between(wk_n, 1, 9), paste0(\"0\", Wk), Wk)) %&gt;%\n    select(Competition_Name:Round, match_week, Wk, Day:Referee) %&gt;%\n    # cleaning of team and stadium names after initial join, seeing what didn't match\n    mutate(Home = ifelse(Home == \"Eint Frankfurt\", \"Eintracht Frankfurt\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"Eint Frankfurt\", \"Eintracht Frankfurt\", Away)) %&gt;%\n    mutate(Home = ifelse(Home == \"M'Gladbach\", \"Borussia Mönchengladbach\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"M'Gladbach\", \"Borussia Mönchengladbach\", Away)) %&gt;%\n    mutate(Home = ifelse(Home == \"Dortmund\", \"Borussia Dortmund\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"Dortmund\", \"Borussia Dortmund\", Away)) %&gt;%\n    mutate(Home = ifelse(Home == \"Stuttgart\", \"VfB Stuttgart\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"Stuttgart\", \"VfB Stuttgart\", Away)) %&gt;%\n    mutate(Home = ifelse(Home == \"Köln\", \"FC Köln\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"Köln\", \"FC Köln\", Away)) %&gt;%\n    mutate(Home = ifelse(Home == \"Hoffenheim\", \"TSG 1899 Hoffenheim\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"Hoffenheim\", \"TSG 1899 Hoffenheim\", Away)) %&gt;%\n    mutate(Home = ifelse(Home == \"Leverkusen\", \"Bayer 04 Leverkusen\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"Leverkusen\", \"Bayer 04 Leverkusen\", Away)) %&gt;%\n    mutate(Home = ifelse(Home == \"Wolfsburg\", \"VfL Wolfsburg\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"Wolfsburg\", \"VfL Wolfsburg\", Away)) %&gt;%\n    mutate(Home = ifelse(Home == \"Bochum\", \"VfL Bochum\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"Bochum\", \"VfL Bochum\", Away)) %&gt;%\n    mutate(Home = ifelse(Home == \"Mainz 05\", \"FSV Mainz 05\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"Mainz 05\", \"FSV Mainz 05\", Away)) %&gt;%\n    mutate(Home = ifelse(Home == \"Freiburg\", \"SC Freiburg\", Home)) %&gt;%\n    mutate(Away = ifelse(Away == \"Freiburg\", \"SC Freiburg\", Away)) %&gt;%\n\n    mutate(Venue = ifelse(Venue == \"Deutsche Bank Park\", \"Waldstadion\", Venue)) %&gt;%\n    mutate(Venue = ifelse(Venue == \"Signal Iduna Park\", \"Westfalenstadion\", Venue)) %&gt;%\n    mutate(Venue = ifelse(Venue == \"Mercedes-Benz Arena\", \"Neckarstadion\", Venue)) %&gt;%\n    mutate(Venue = ifelse(Venue == \"RheinEnergieSTADION\", \"Müngersdorfer Stadion\", Venue)) %&gt;%\n    mutate(Venue = ifelse(Venue == \"Veltins-Arena\", \"Arena AufSchalke\", Venue)) %&gt;%\n    mutate(Venue = ifelse(Venue == \"PreZero Arena\", \"Rhein-Neckar-Arena\", Venue)) %&gt;%\n # order and name the data columns for next steps\n    select(league = Competition_Name, season = Season_End_Year, round = Round,\n                 match_date = Date, match_day = Day, match_time = Time,\n                 match_home = Home, match_away = Away,\n                 match_stadium = Venue, match_attendance = Attendance,\n                 HomeGoals, Home_xG, AwayGoals, Away_xG, Referee) %&gt;%\n    # join on stadium data\n    left_join(ger_stad_df, by = c(\"match_stadium\" = \"stadium\")) %&gt;%\n    # fix some other issues that came up after 1st pass at visualization\n    mutate(capacity = ifelse(match_stadium == \"Neckarstadion\", 50000, capacity)) %&gt;%\n    mutate(capacity = ifelse(match_stadium == \"Waldstadion\", 51500, capacity)) %&gt;%\n    mutate(capacity = ifelse(match_stadium == \"Mewa Arena\", 33305, capacity)) %&gt;%\n    # create the capacity percent for each match\n    mutate(match_pct_cap = match_attendance / capacity)\n\n\nOk, now that we have some clean data to work with, it’s time to make a chart. The first step is to create a tidy summary dataset. Since I would be doing the same thing multiple times, I created a function. The code below includes some commented-out fields that I didn’t end up needing for this analysis, but I left the code in there for future use.\n\n\nShow code for summary function\nattend_sum &lt;- function(input_df, dfname = \"NA\") {\n    dfout &lt;- input_df %&gt;%\n        group_by(match_home, match_stadium) %&gt;%\n        mutate(attend_avg_team = round(mean(match_attendance), 0)) %&gt;%\n        mutate(attend_min_team = min(match_attendance)) %&gt;%\n        mutate(attend_max_team = max(match_attendance)) %&gt;%\n        mutate(attend_tot_team = sum(match_attendance)) %&gt;%\n        mutate(capacity_tot_team = sum(capacity)) %&gt;%\n        mutate(capacity_pct_team = attend_tot_team / capacity_tot_team) %&gt;%\n        ungroup() %&gt;%\n        # league figures\n        add_row(tibble_row(match_home = \"League Average\")) %&gt;%\n        mutate(attend_tot_league = sum(match_attendance, na.rm = TRUE)) %&gt;%\n        mutate(capacity_tot_league = sum(capacity, na.rm = TRUE)) %&gt;%\n        mutate(capacity_pct_league = attend_tot_league / capacity_tot_league) %&gt;%\n        mutate(attend_avg_league = round(mean(match_attendance, na.rm = TRUE), 0)) %&gt;%\n        mutate(capacity_avg_league = round(mean(capacity, na.rm = TRUE), 0)) %&gt;%\n        mutate(attend_avg_team = ifelse(match_home == \"League Average\", attend_avg_league, attend_avg_team)) %&gt;%\n        mutate(capacity_pct_team = ifelse(match_home == \"League Average\", capacity_pct_league, capacity_pct_team)) %&gt;%\n        mutate(capacity = ifelse(match_home == \"League Average\", capacity_avg_league, capacity)) %&gt;%\n        # mutate(attend_med_league = round(median(match_attendance), 0)) %&gt;%\n        # mutate(attend_min_league = min(match_attendance)) %&gt;%\n        # mutate(attend_max_league = max(match_attendance)) %&gt;%\n        # mutate(capacity_med_league = round(median(capacity), 0)) %&gt;%\n        # mutate(capacity_min_league = min(capacity)) %&gt;%\n        mutate(capacity_max_league = max(capacity)) %&gt;%\n        # group_by(match_home) %&gt;%\n        # mutate(capacity_tot2 = sum(capacity)) %&gt;%\n        # mutate(attendance_tot2 = sum(match_attendance)) %&gt;%\n        # mutate(capacity_pct_team = attendance_tot2 / capacity_tot2) %&gt;%\n        # ungroup() %&gt;%\n        select(team_name = match_home, stadium_name = match_stadium, stadium_capacity = capacity,\n        attend_avg_team, attend_min_team, attend_max_team,\n        attend_tot_team, capacity_tot_team, capacity_pct_team,\n        attend_avg_league,\n        # attend_med_league, attend_min_league, attend_max_league,\n        capacity_avg_league,\n        # capacity_med_league, capacity_min_league,\n        capacity_max_league,\n        attend_tot_league,\n        capacity_tot_league, capacity_pct_league, league) %&gt;%\n        #, capacity_pct_team) %&gt;%\n        distinct(team_name, stadium_name, .keep_all = T)\n    assign(str_c(dfname, \"_sum\"), dfout, envir=.GlobalEnv)\n}\n\n\nCreating the summary dataframe was thus reduced to one line of code.\nattend_sum(bundes_att_23, \"bundes_att_23\")\nThe first part of the function call is the match data. The text in quotes will come before the “_sum” suiffix in the output dataframe, e.g. bundes_att_23_sum. This is accomplished via the assign() command in the function code. I did that to have easily identifiable dataframe objects to call, given I’d be working with multiple countries in the same session.\nRemember…\nGood data management is consistent naming for objects that do the same thing.\nNow that we have a nice tidy dataframe to plot, let’s make the plot. Again, since I’m making multiples of the same plot, I wrote a function.\n\n\nShow code for plot function\n## highlight function for plot labels\n# from https://stackoverflow.com/questions/61733297/apply-bold-font-on-specific-axis-ticks\nhighlight = function(x, pat, color=\"black\", family=\"\") {\n    ifelse(grepl(pat, x), glue::glue(\"&lt;b style='font-family:{family}; color:{color}'&gt;{x}&lt;/b&gt;\"), x)\n}\n\n## plotting function. run against plotting df, output as object, then add title\nattend_plot1 &lt;- function(plotdf) {\n    plotdf %&gt;%\n        ggplot(aes(stadium_capacity, reorder(team_name, stadium_capacity))) +\n        # points for avg attendace & capacity\n        geom_point(aes(x=stadium_capacity, y= reorder(team_name, stadium_capacity)),\n            color=\"#4E79A7\", size=10, alpha = .5 ) +\n        geom_point(aes(x=attend_avg_team, y= reorder(team_name, stadium_capacity)),\n            color=\"#A74E79\", size=10, alpha = .5 ) +\n        # data labels for points\n        geom_text(data = plotdf %&gt;% filter(capacity_pct_team &lt; .95),\n            aes(x = attend_avg_team,\n                label = format(round(attend_avg_team, digits = 0),big.mark=\",\",scientific=FALSE)),\n                color = \"black\", size = 2.5) +\n        geom_text(data = plotdf %&gt;% filter(capacity_pct_team &gt;= .95),\n            aes(x = attend_avg_team,\n                label = format(round(attend_avg_team, digits = 0),big.mark=\",\",scientific=FALSE)),\n                color = \"black\", size = 2.5, hjust = 1.5) +\n        geom_text(aes(x = stadium_capacity,\n                label = format(round(stadium_capacity, digits = 0),big.mark=\",\",scientific=FALSE)),\n                color = \"black\", size = 2.5) +\n        # line connecting the points.\n        geom_segment(aes(x=attend_avg_team + 900 , xend=stadium_capacity - 900,\n                y=team_name, yend=team_name), color=\"lightgrey\") +\n        # sets league average in bold\n        scale_y_discrete(labels= function(x) highlight(x, \"League Average\", \"black\")) +\n        # text for avg season capacity\n        geom_text(data = plotdf %&gt;% filter(stadium_capacity &lt; capacity_max_league & team_name != \"League Average\"),\n            aes(x = stadium_capacity + 1100, y = team_name,\n                label = paste0(\"Pct of capacity for season = \", round(capacity_pct_team * 100, 1), \"%\"),\n                hjust = -.02)) +\n        geom_text(data = plotdf %&gt;% filter(team_name == \"League Average\"),\n            aes(x = stadium_capacity + 1100, y = team_name,\n                label = paste0(\"Pct of capacity for season = \", round(capacity_pct_team * 100, 1), \"%\"),\n                hjust = -.02, fontface = \"bold\")) +\n        scale_x_continuous(limits = \n            c(min(plotdf$attend_avg_team), max(plotdf$stadium_capacity + 3000)),\n                breaks = scales::pretty_breaks(6),\n                labels = scales::comma_format(big.mark = ',')) +\n        labs(x = \"Stadium capacity\", y = \"\",\n                 subtitle = \"*The further the red dot is to the left of the blue dot, the more average attendance is less than stadium capacity. Teams sorted by stadium capacity.*\",\n                 caption = \"*Match attendance data from FBRef using worldfootballr package. Stadium capacity data from Wikipedia*\") +\n        theme_minimal() +\n        theme(panel.grid = element_blank(),\n            plot.title.position = \"plot\",\n            plot.title = ggtext::element_textbox_simple(\n            size = 12, fill = \"cornsilk\",\n            lineheight = 1.5,\n            padding = margin(5.5, 5.5, 5.5, 2),\n            margin = margin(0, 0, 5.5, 0)),\n            plot.subtitle = ggtext::element_markdown(size = 10),\n            plot.caption = ggtext::element_markdown(),\n            axis.text.x = ggtext::element_markdown(size = 10),\n            axis.text.y = ggtext::element_markdown(size = 11))\n}\n\n\nSo now we can create the base plot with just one line of code:\nbundes_attplot &lt;- attend_plot1(bundes_att_23_sum)\nThe title and some other elements will be added individually for each league plot. But but writing the function cut down the overall amount of code for each league script by a significant amount, not to mention eliminating a lot of copy-paste.\nI also did a scatterplot of stadium capacity on the x axis and average match capacity by team on the y axes. The code for the function is here:\n\n\nShow code for scatterplot function\n\nattend_scatter &lt;- function(plotdf) {\n  plotdf %&gt;%\n  ggplot(aes(x = stadium_capacity, y = capacity_pct_team)) +\n  geom_point() +\n  geom_smooth() +\n  geom_text_repel(aes(label = team_name)) +\n  scale_x_continuous(labels = scales::comma_format(big.mark = ',')) +\n  scale_y_continuous(limits = c(0,1), labels = scales::percent_format()) +\n  labs(x = \"Stadium Capacity\", y = \"Avg % of Capacity\") +\n  theme_minimal() +\n  theme_minimal()\n}\n\n\nGetting the scatterplot thus requires just one line of code:\nbundes_scatter &lt;- attend_scatter(bundes_att_23_sum)\nTo run the summary and plot functions I put them in a separate script in this project and used the source() command, in this case: source(\"attend_functions.R\"). Using source() keeps that job in a separate file, making it easier to edit and keeping the analytical files focused on their jobs.\nBefore we get to the individual league plots I want to run a few tables to show the top-line analysis across leagues. To do that we’ll need to join the summary tables together, create a table in gt and write a slightly different plot."
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html#average-match-capacity-by-league",
    "href": "posts/if-you-sell-it-will-they-come/index.html#average-match-capacity-by-league",
    "title": "If You Sell It, Will They Come?",
    "section": "Average match capacity by league",
    "text": "Average match capacity by league\nTo do the table below I combined the individual league attendance files into a single dataframe. I created a function to read them in, select only the necessary columns and bind them. I include the function below in case you encounter a similar need…to read in a bunch of files, clean, and join them. All you need to do is change the file type and the regex for the filename pattern.\nTo run the function, it’s one line of code:\nattdfs &lt;- combine_rds_files(\"file-path-here\")\n\n\nShow code for rds function\n## read in and rbind attendance files\ncombine_rds_files &lt;- function(directory) {\n  # List all RDS files in the specified directory\n  rds_files &lt;- list.files(directory, pattern = \"^att_2023.*\\\\.rds\", full.names = TRUE)\n\n  # Initialize an empty list to store dataframes\n  df_list &lt;- list()\n\n  # Loop through each RDS file, read it, select columns, and store in the list\n  for (file in rds_files) {\n    data &lt;- readRDS(file)\n    # Select the desired columns\n    selected_data &lt;- data %&gt;%\n      select(league, match_date, match_home, match_away, match_stadium, capacity, match_attendance)\n    df_list[[file]] &lt;- selected_data\n  }\n\n  # Combine all dataframes in the list into one dataframe\n  combined_df &lt;- bind_rows(df_list)\n\n  return(combined_df)\n}\n\n\nWhat does the table tell us?\nPremier League and Bundesliga are in very high demand, and Dutch Eredivisie demand is not much further behind. And MLS is right up there, 4th highest in comparison to the major European leagues.\nThe scatterplots below the table display the percentage capacity (y axis) by overall stadium capacity and then average attendace. They show that except for the English Premier League and the Bundesliga, a larger average stadium size or attendance doesn’t mean that a league’s clubs are doing better at filling their grounds. Scottish and Dutch clubs, along with MLS, have relatively smaller grounds on average, but they’re well-matched to demand so they are playing to crowds above 80% capacity.\nThe bottom line is, if you’re a football tourist and not fixated on a Premier League or Budnesliga match, tickets are easier to come by in Belgium, Portugal (Primeira Liga), Denmark (Superliga), and Sweden (Allsvenskan). The by-league charts below will show you which clubs in even the more in-demand leagues might offer you a better chance at getting a ticket.\n\ntable is sortable - click on column headers\n\n\n\nShow code for interactive gt table\natt23_all %&gt;%\n  arrange(desc(capacity_pct_league)) %&gt;%\n  select(league, capacity_avg_league, attend_avg_league, capacity_pct_league) %&gt;%\n  gt() %&gt;%\n    fmt_number(columns = c(attend_avg_league, capacity_avg_league), decimals = 0) %&gt;%\n  fmt_percent(columns = c(capacity_pct_league), decimals = 1) %&gt;%\n    cols_label(attend_avg_league = md(\"&lt;span style=color:white&gt;League Average&lt;br&gt;Attendance&lt;/span&gt;\"), \n               capacity_avg_league = md(\"&lt;span style=color:white&gt;League Average&lt;br&gt; Stadium Capacity&lt;/span&gt;\"),\n               capacity_pct_league = md(\"&lt;span style=color:white&gt;League Average&lt;br&gt; Match Capacity&lt;/span&gt;\"), \n               league = md(\"&lt;span style=color:white&gt;League&lt;/span&gt;\")) %&gt;%\n    cols_align(align = \"right\", columns = everything()) %&gt;%\n  cols_align(align = \"left\", columns = c(league)) %&gt;%\n  opt_stylize(style = 6) %&gt;%\n    tab_style(\n        style = cell_text(align = \"center\"),\n        locations = cells_column_labels(\n            columns = c(attend_avg_league, capacity_avg_league, capacity_pct_league))) %&gt;%\n    tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = c(league, attend_avg_league, capacity_avg_league, capacity_pct_league),\n      rows = league == \"Average all leagues\")) %&gt;%\n  # function to make table sortable\n  opt_interactive(use_sorting = TRUE,\n                  use_pagination = FALSE,\n                  use_compact_mode = TRUE)"
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html#club-attendance-figures-for-top-5-european-leagues-mls",
    "href": "posts/if-you-sell-it-will-they-come/index.html#club-attendance-figures-for-top-5-european-leagues-mls",
    "title": "If You Sell It, Will They Come?",
    "section": "Club attendance figures for Top 5 European leagues & MLS",
    "text": "Club attendance figures for Top 5 European leagues & MLS\nAmong the top 5 European leagues, the Premier League and Bundesliga had the highest demand, with almost every club in those two leagues well above 90% capacity. The main outlier was Hertha Berlin, at 72% capacity. They were relegated that season, after a few very bad years that saw already shaky fan support dwindle. They also play in Berlin’s Olympic Stadium, which is arguably too big for them regardless of how successul they are.\nCapacity in France, Italy, and Spain was much more varied, with some clubs over 90% and some clubs at less than half capacity. The bigger, more successful clubs had higher capacities relative to the rest of their leagues. Think Real Madrid, Barcelona, the Milan clubs, Juventus, Paris Saint Germain, Marseille. But even in Spain no club was above 90%, Real Madrid just at 90%. In France some less world-renowned clubs like Lens and Stade Rennais did very well, thanks to good seasons on the pitch and generally strong support.\nThe MLS situation was mostly strong, with most teams playing to stadiums at at least 85% capacity. But the MLS data is odd, with some teams showing capacity at more than 100%. I understand that to be selling standing-room tickets in excess of seated capacity. And remember that MLS definitely reports tickets sold/given away, not turnstile counts.\nBut still you can argue that MLS is a good draw compared to the major European leagues. The capacity figures for teams have improved as many have moved into smaller soccer-specific stadia, no longer playing in NFL stadiums that they would never hope to fill. Those still playing in NFL stadia often reduce capacity by not making all sections available for sale.\nAs you look at the attendance figures, what do you see?\nNotes:  Plot images expand “lightbox” style when clicked.  The France tab has a code example showing customization for Ligue 1.\n\nEnglandFranceGermanyItalySpainMLS\n\n\nYou might look at the bubble chart on the right and wonder why I kept it in given that you can’t distinguish the blue stadium capacity bubbles from the orange average attendance bubbles.\nWell, as the bar chart on the left shows, the demand and thus the overlap is the story. Demand for EPL tickets is so high, that there’s no daylight between how many people EPL stadiums can hold and the average number of people turning up on match day.\n\nAnd same with the scatterplot. Why not set the y-axis to like a 50%-100% range? Well again, the story is the high demand, meaning all clubs are in the narrow 90%+ range, many above 95%. I wanted to visually highlight that all EPL clubs are playing to mostly full houses.\n\n\n\nThe stadium capacity story in France is that a couple of the top clubs, PSG and Marseille, are doing very well, as are medium-sized clubs like Stade Rennais (Rennes) and Lens. France doesn’t have a uniformly strong football culture. In some areas rugby is much more popular and football is still viewed as the hooligan’s sport. And to be honest the behaviour of some club ultras doesn’t help that perception. The clubs struggling to attract crowds are mainly those at the bottom of the table or those like Monaco where the football culture just isn’t as strong. Lyon have been struggling for a few years and could do better than 3/4 capacity if they were a bit more successful on the pitch.\nAs for the process of making and annotating the plot…\nFirst I run the data thru the summary function to create the set used for plotting. It’s now this one line of code: attend_sum(att_2023_ligue1, \"fra_att_23\")\nAfter looking at the basic plot, I then decide on title text that describes the insight from the visualization and add it via patchwork’s plot_annotation() call.\n\n\nShow code for plotting\n# run the basic plot from the function:\nfra_attplot &lt;- attend_plot1(fra_att_23_sum)\n\n# final plot - adjust max geom_text (DATA SET), title (LEAGUE)\nfra_attplot +\n  plot_annotation(title = \"&lt;b&gt;Ligue 1\n  &lt;span style='color: #8DA0CB;'&gt;Average percent of capacity for season&lt;/span&gt;&lt;/b&gt;&lt;i&gt; (left bar chart)&lt;/i&gt;,\n  &lt;b&gt;&lt;span style='color: #FF7F00;'&gt;Average attendance&lt;/span&gt;&lt;/b&gt; and\n  &lt;b&gt;&lt;span style='color: #1F78B4;'&gt;Stadium capacity&lt;/span&gt;&lt;/b&gt; (right bubble chart), by club, 2022-23 season.&lt;br&gt;\n                There is a lot of variance in demand for tickets relative to stadium capacity in Ligue 1.\n            Some clubs are well above 90% capacity, some play to less-than half full houses.&lt;br&gt;\n      See scatterplot below for stadium capacity numbers obscured by overlapping bubbles.\",\n                  theme = theme(plot.title =\n                                  ggtext::element_textbox_simple(\n                                    size = 12, fill = \"cornsilk\",\n                                    lineheight = 1.5,\n                                    padding = margin(5.5, 5.5, 5.5, 2),\n                                    margin = margin(0, 0, 5.5, 0))))\n\n\n\n\n\n\nDemand for tickets in German is very strong. All but three teams are regularly playing to 90%+ full stadiums. The Bundesliga is a tough ticket. I mentioned in the intro the issues with Hertha, and why they are the outlier here.\n\n\n\n\nThe top clubs in Italy are playing to almost full stadiums, but overall from what I’ve read many stadiums in Italy are older and run-down, so not the most inviting experience for the casual match-goer. Spezia is an example of a team where the size of the stadium accurately reflects demand…in this season it was the smallest in Serie A but was regularly 90% full. And this in a season where they were relegated, finishing 18th.\n\n\n\n\nWhile I’m not surprised that Real Madrid and Rayo are playing at close to 90% capacity, I was surprised to see Athletic Club only at 80%. Seems whenever I watch them on television, San Mamés is full. Maybe there’s an issue with the attendance or capacity numbers? Not surprised as well that Getafe, Cadiz & Espanyol were as low as they were. In another post I do want to see if these clubs are negatively impacted by playing often on Friday or Monday nights.\n\n\n\n\nMLS was a tough one to do. Teams were flexible with stadium capacity or even venue depending on the opponent, for example if Miami & Messi came to town. The San Jose Earthquakes sometimes play in larger venues if one of the LA teams come up north. This required a bit of bespoke coding to get the capacity numbers correct and to visualize the effect. The bar & bubble chart shows alternate venues, even for one-off matches.\nThe good news is that in general an MLS ticket is in high demand at many clubs.\nChicago Fire stand out for their generally low capacity, but the club’s been a mess for a while now. NY Red Bulls have also been struggling with results in recent years, so attendance has suffered. New England and Vancouver are also in that 70% band.\nThe other low-capacity numbers are for one-off matches played at a different venue than the usual home field, like NYCFC needing to play across the river at Red Bull Arena in New Jersey, or Montreal having to play indoors in the Olympic Stadium.\n\nBecause MLS as a) too many clubs and b) lots of clubs with similar sized soccer-specific grounds (a good thing!) the stadium capacity by average percent capacity scatterplot is a bit cluttered at the lower end. Plus the one-off matches and two significantly larger venues distort the smoothing line.\n\nSo that’s why this 2nd scatterplot, with one-offs and the two largest stadiums removed, shows slightly more clear look at the relationship between stadium capacity and ticket demand. And what we see are a couple of clubs, NYCFC in particular, who might be playing in stadiums that are too large, or at least not reducing capacity enough to avoid killing the vibe with empty seats. NYCFC at least is taking steps to get their own stadium with a capacity more appropriate to demand."
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html#club-attendance-figures-for-other-european-leagues",
    "href": "posts/if-you-sell-it-will-they-come/index.html#club-attendance-figures-for-other-european-leagues",
    "title": "If You Sell It, Will They Come?",
    "section": "Club attendance figures for other European leagues",
    "text": "Club attendance figures for other European leagues\nWhat about the other European leagues, those in Belgium, Denmark, the Netherlands, Portugal, Scotland, Sweden, and Switzerland? It’s a mixed bag, with the Dutch Eredivisie and Scottish Premiership clubs playing to 80%+ full arenas. Meanwhile the Swiss, Belgian, and Portuguese clubs on average are under 60% full. As with the larger leagues, the capacity picture varies by club. \n\nBelgiumDenmarkNetherlandsPortugalScotlandSwedenSwitzerland\n\n\nClub Brugge may have the highest average attendance, but Antwerp have the highest percent of capacity number, at near 90%. Cercle Brugge are not even at 20% capacity, playing in the same arena as Club Brugge. Though to be honest, something about a number that low makes me wonder if there’s a data quality issue. But I can only display what’s being reported.\n\n\n\n\nFC Copenhagen (FCK) are by far the biggest team in Denmark. They play in Parken, the national team stadium, and regularly draw almost 29,000 people per match. Danes do love football, but winters can be cold and damp, and in Jutland handball is also a big winter sport, so demand isn’t strong enough to fill arenas. But the vibes are still great, and the quality of play is good. If you want a fun football experience, come to Denmark, and don’t just go to FCK. Visit some of the smaller grounds.\n\n\nBecause Denmark splits the club season into two parts, a regular league-wide round-robin home-away round and a round where the top and bottom halves compete separately for separate honors or relegation ignominy, it’s worth looking quickly to see if attendances increase or decrease depending on if a team is in the championship or relegation group.\nThe plot below shows that teams with something real on the line, either the league title or avoiding relegation, tend to see an increase in the 2nd stage of competition relative to their attendance from the regualr season. So FC Copenhagen, going for the title, saw bigger crowds in the spring. So did Aalborg and Lyngby, battling hard to avoid relegation. Aalborg went down, Lyngby stayed up. The outlier in this respect is Horsens, who were in a tight relegation battle but saw a decrease in attendance for the 2nd stage.\nOf course one factor in 2nd stage attendance bumps could be nice spring weather after the cold, wet, dark Danish winter and handball season ending, meaning less competition for the sporting entertainment kroner.\n\n\n\nThe Dutch also love their football, and Eredivisie tickets are generally in high demand. Passions run so deep that there’s a slight ultras problem with violence, flares, pitch invaders, and objects being thrown onto the pitch. So some matches have been played at reduced capacity or behind closed doors, dperessing the overall capacity percentages a bit.\n\n\n\n\nLike Ligue 1 and the Eredivisie, the Portuguese league features a steady pipeline of players who go on to feature at clubs who play in the Champions League. Does the transient nature of club stars play a part in depressed demand in Portugal? Is it economic? Some clubs are barely filling 30% of their seats. I don’t know enough about how the economy or match-going culture affects attenance in Portugal.\n\n\n\n\n“Fitba” is big in Scotland, especially at the Old Firm (Celtic & Rangers) and the big Edinburgh clubs (Hibs & Hearts). Those are hard tickets to come by. Clubs lower down the league ladder are having trouble filling seats. That might have to do with the fact that it’s not a very competitive league. Either Celtic or Rangers will almost always win the title.\n\n\n\n\nThe big clubs in Sweden do well in terms of filling seats. AIK Stockholm’s capacity number is perhaps hindered by them playing in the national team’s stadium. While football is somewhat popular in Sweden, the league does run in the summer, and Swedes take their summer vacation seriously. Does that mean fewer people go during July and August? Something I might check out in a deeper dive post.\n\n\n\n\nI honestly don’t know enough about football culture and demand in Switzerland to offer much insight. We do see from the capacity percentage number that FC Winterthur play in an appropriately-sized arena. Almost every match is sold out. Young Boys are the most perennially successful team, so it makes sense there’s demand for their matches. But why are Grasshopper and Servette not even at 30% capacity?"
  },
  {
    "objectID": "posts/if-you-sell-it-will-they-come/index.html#what-does-it-all-mean",
    "href": "posts/if-you-sell-it-will-they-come/index.html#what-does-it-all-mean",
    "title": "If You Sell It, Will They Come?",
    "section": "What does it all mean?",
    "text": "What does it all mean?\nComing back to the origin point for the post, MLS has relatively good demand for live matches relative to the top European leagues. League-wide more than 85% of tickets are taken. Whatever one thinks of the quality of play, that too many teams make the playoffs, that perhaps there’s too many teams in the league, that there should be promotion-relegation, and other issues, enough people think an MLS match is good entertainment value for the money.\nIn terms of average attendances across all leagues, it’s worth keeping in mind that the 2022-23 season was the first since 2018-19 to not be interrupted and/or disrupted by COVID. Stadiums were open to full crowds, and teams and leagues were reporting higher demand than the pre-COVID seasons, especially the 2nd through 4th tier EFL leagues in England, and even non-league football there. People were happy to be able to go out and do things again after partial or full lockdowns. There was pent up demand for entertainment.\nI do want to see which leagues sustained their momentum. It’s too much work, but it would also be interesting to pull data from 2018-19 to compare to 2022-23 and see what the COVID lockdown effect might have been, if what some teams were reporting held up across leagues.\nWhat else to explore? In addition to tracking the numbers season-to-season, I would like to look a bit more closely at league position and ticket demand; what teams bring crowds in spite of league position, what teams are most affected? I would love to look at the relationship between ticket prices and demand, but price data is not easy to find, especially across all these leagues. Even just looking at 2 or 3 leagues would entail a lot of work.\nI have started another post that will look at MLS a bit more, specifically if there was a “Messi effect” on attendance, and if there’s a lull in mid-season attendance. A criticism of the MLS structure is that there are too many teams in general, and too many teams make the playoffs thus reducing the impact of individual matches. I also want to compare the 2022-23 season with the next two seasons to see if the move, starting in 2023-24, to a standard Saturday 7:30pm (local time) match time had an effect.\nI’ll also look a bit deeper into Spain, and see if Friday & Monday matches hurt attendance. I might also see if the summer vacation in Sweden has an effect on attendance in June through August.\nBut that’s enough for now. If you’ve made it this far, I’d love to know what you think."
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "gregers kjerulf dubrow",
    "section": "",
    "text": "I provide expertise in helping to maximize the impact of your data by providing a range of services including:\n\nData insights - Auditing, cleaning, and exploratory data analysis to help you understand your data and which elements can help to provide meaningful insights and impact.\nData visualization - Building dashboards and reports that will deliver insights, tell a story, and help with strategic decision-making.\nProgram evaluation and outcomes studies - Design descriptive and quasi-experimental evaluations of academic and co-curricular programs.\nSurvey design and analysis - Instruments to help you measure the impact of academic and co-curricular programs and events.\nData quality - Documentation and codebooks to help you keep track of updates and changes.\nData management - Working with your IT team to help set up a functional and scalable data pipeline.\nWorkshops and training - Help your organization foster a data-driven culture with technical and non-technical workshops in data literacy, data visualization, and coding in r.\n\nI have more than 15 years of experience in public higher education in California and Florida, as well as experience as an academic researcher and teacher. I am particularly attuned to the needs of educational institutions and non-profits.\nLearn more about my skills and experience on LinkedIn\nYou can see examples of my personal data projects and approach to visualization in the blog section of the website\nTo inquire about any of my services, send an email to gkdubrow.data@gmail.com"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2025/index.html#plan",
    "href": "posts/30-day-chart-challenge-2025/index.html#plan",
    "title": "30 Day Chart Challenge 2025",
    "section": "The Plan for the 2025 Challenge",
    "text": "The Plan for the 2025 Challenge\nQuick note - if you’re reading this before April 1 it’s because due to something I need to do on April 2 that might keep me away from the computer for a day or two, I’m getting ahead of the challenge by doing some charts in late March. I’ll have at least 3 done, if not more. To at least play within the spirit of the challenge I won’t post them to social media until April 1 and I’m putting the first publish date on blog posts as April 1.\nIt’s time again for the 30 Day Chart Challenge, a social-media-driven data viz challenge to create and share a data visualization on any topic, but somehow corresponding to the daily prompt as suggested by the organizers.\nI did participate last year, though started late and did not get to every prompt. Like last year I am going to keep all the prompt posts in one mega-post, adding to it regularly. Perhaps not each day, but I will try to get to every prompt. Like last year:\n\nI will post contributions to social media via my Bluesky, and LinkedIn accounts.\nI will add contributions to this post, meaning the posting date will refresh to the date I’ve added the chart. So If I get to a lot or even all of them, this will be a long post.\nUse the table of contents to the right to navigate to a specific chart.\nImages are “lightboxed”, so click on them to enlarge. Click esc or outside the image to restore the page.\n\nOne major change is that this year I am going to only pull data from Danmarks Statistik (Statistics Danmark, the national statistics agency) via the danstat package for r. To the extent possible I’ll be focusing on education data. These constraints will help me by not having to figure out which data source to use each day, plus I’ll hopefully be telling a story of the coming weeks.\nIt may not be a straight linear narrative and not every post will necessarily connect to the whole. I may not even know exactly what I have until I’m done. I have sketched out ideas for many of the prompts, but things may change as I go along. If the daily prompt just doesn’t lend itself to Danish education data, or Danish data at all, the subject for that chart will be something else.\nAs with all of my posts, I’m offering a tutorial/how-to as much as I am trying to convey insights from the data. If a topic or approach interests you, hopefully there’s enough explanation in the writing or the code so that you can do something similar or use it to hone your r skills.\nLet’s get to the charts."
  },
  {
    "objectID": "posts/30-day-chart-challenge-2025/index.html#prompt1",
    "href": "posts/30-day-chart-challenge-2025/index.html#prompt1",
    "title": "30 Day Chart Challenge 2025",
    "section": "Prompt #1 - Fractions",
    "text": "Prompt #1 - Fractions\nThe first week of prompts is devoted to comparisons, and the first day is fractions. I cheated a bit and replicated last year’s 1st prompt “Part-to-Whole”. But where last year I looked at educational attainment in the UK, this year it’s Denmark.\nI won’t do too much explaining about education levels in Denmark, you can read up on them on the ministry’s page.\nWe’ll do a horizontal stacked bar chart displaying the highest education attained by Danes aged 25-69 as of 2023, with this population divided into four age groups.\nTo pull data from danstat, which accesses the Danmarks Statistik API, you have to know at least the table name and number. So it’s necessary to poke around Danmarks Statistik’s StatBank page and figure out which table has the data you need. The value added of the API and package is not having to download and keep flat files.\nThe table_meta command produces a nested tibble with values for each field in the table. To get the data you first create a list of fields and values (see the varaibles_ed object) then use the get_data command to actually fetch the data.\n\n\nShow code for getting and cleaning data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(danstat) # package to get Danish statistics via api\nlibrary(ggtext) # enhancements for text in ggplot\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n\n# metadata for table variables, click thru nested tables to find variables and ids for filters\ntable_meta &lt;- danstat::get_table_metadata(table_id = \"hfudd11\", variables_only = TRUE)\n\n# create variable list using the ID value in the variable\nvariables_ed &lt;- list(\n    list(code = \"bopomr\", values = c(\"000\", \"081\", \"082\", \"083\", \"084\", \"085\")),\n    list(code = \"hfudd\", values = c(\"H10\", \"H20\", \"H30\", \"H35\",\n                                        \"H40\", \"H50\", \"H60\", \"H70\", \"H80\", \"H90\")),\n    list(code = \"køn\", values = c(\"TOT\")),\n    list(code = \"alder\", values = c(\"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                                            \"50-54\", \"55-59\", \"60-64\", \"65-69\")),\n    list(code = \"tid\", values = 2023))\n\n# past variable list along with table name.\n# note that in package, table name is lower case, though upper case on statbank page.\nedattain1 &lt;- get_data(\"hfudd11\", variables_ed, language = \"da\") %&gt;%\n    as_tibble() %&gt;%\n    select(region = BOPOMR, age = ALDER, edlevel = HFUDD, n = INDHOLD)\n\n# arrange data for chart\n# collapse age and education levels into groups\nedattain &lt;- edattain1 %&gt;%\n    mutate(age_group = case_when(age %in% c(\"25-29 år\", \"30-34 år\", \"35-39 år\") ~ \"25-39\",\n                age %in% c(\"40-44 år\",\"45-49 år\") ~ \"40-49\",\n                age %in% c(\"50-54 år\",\"55-59 år\") ~ \"50-59\",\n                age %in% c(\"60-64 år\",\"65-69 år\") ~ \"60-69\")) %&gt;%\n    mutate(ed_group = case_when(edlevel == \"H10 Grundskole\" ~ \"Grundskole/Primary\",\n                edlevel %in% c(\"H20 Gymnasiale uddannelser\",\n                \"H30 Erhvervsfaglige uddannelser\",\n                \"H35 Adgangsgivende uddannelsesforløb\") ~ \"Secondary\",\n                edlevel == \"H40 Korte videregående uddannelser, KVU\" ~ \"Tertiary - 2yr\",\n                edlevel %in% c(\"H50 Mellemlange videregående uddannelser, MVU\",\n                \"H60 Bacheloruddannelser, BACH\") ~ \"Tertiary - Bachelor\",\n                edlevel == \"H70 Lange videregående uddannelser, LVU\" ~ \"Tertiary - Masters\",\n                edlevel == \"H80 Ph.d. og forskeruddannelser\" ~ \"Tertiary - PhD\",\n                edlevel == \"H90 Uoplyst mv.\" ~ \"Not stated\")) %&gt;%\n    group_by(region, age_group, ed_group) %&gt;%\n    mutate(n2 = sum(n)) %&gt;%\n    ungroup() %&gt;%\n    select(-n, -age) %&gt;%\n    distinct(region, age_group, ed_group, .keep_all = T) %&gt;%\n    rename(n = n2) %&gt;%\n    mutate(ed_group =\n                factor(ed_group,\n                levels = c(\"Grundskole/Primary\", \"Secondary\", \"Tertiary - 2yr\",\n                \"Tertiary - Bachelor\", \"Tertiary - Masters\", \n                \"Tertiary - PhD\", \"Not stated\")))\n\n\nNow that we have a nice tidy tibble, let’s make a chart. The approach is fairly straight-forward ggplot. I used tricks like fct_rev to order the age groups on the chart, and the scales and ggtext packages to make things look nicer.\n\n\nShow code for making the chart\n\n## chart - all DK, horizontal bar age groups, percent stacks percent by deg level\nedattain %&gt;%\n    filter(region == \"Hele landet\") %&gt;%\n    group_by(age_group) %&gt;%\n    mutate(age_total = sum(n)) %&gt;%\n    mutate(age_pct = round(n/age_total, 2)) %&gt;%\n    select(age_group, ed_group, age_pct) %&gt;%\n    # the line below passes temporary changes to the data object through to the chart commands\n    {. -&gt;&gt; tmp} %&gt;%\n    ggplot(aes(age_pct, fct_rev(age_group), fill = fct_rev(ed_group))) +\n    geom_bar(stat = \"identity\") +\n    scale_x_continuous(expand = c(0,0),\n                breaks = c(0, 0.25, 0.50, 0.75, 1),\n                labels = c(\"0\", \"25%\", \"50%\", \"75%\", \"100%\")) +\n    geom_text(data = subset(tmp, age_pct &gt;0.025),\n                aes(label = scales::percent(round(age_pct , 2))),\n                position = position_stack(vjust = 0.5),\n                color= \"white\", vjust = 0.5, size = 12) +\n    scale_fill_brewer(palette = \"Set3\") +\n    labs(title = \"Danes under 40 have a higher rate of post-Bachelor educational attainment than other age groups\",\n             subtitle = \"Highest education level attained by age groups\",\n             caption = \"*Data from Danmarks Statistik via danstat package*\",\n             x = \"\", y = \"Age Group\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'),\n                legend.key.width = unit(4, 'cm'), legend.margin=margin(-10, 0, 0, 0),\n                legend.text = element_text(size = 12), legend.title = element_text(size = 16),\n                plot.title = element_text(hjust = .5, size = 20),\n                plot.subtitle = element_text(size = 16),\n                plot.caption = element_markdown(size = 12, face = \"italic\"),\n                axis.text.x = element_text(size = 14),\n                axis.text.y = element_text(size = 14),\n                panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\n    guides(fill = guide_legend(label.position = \"bottom\", reverse = TRUE, \n                direction = \"horizontal\", nrow = 1, \n                title = \"Highest Educational Attainment\", title.position = \"top\")) \nrm(tmp)\n\n\n\nAs you might expect, younger Danes tend to have higher levels of educational attainment than Danes over 60. That’s not uncommmon in countries with post-secondary education policies designed to increase access and attainment. As in the US this is a post-WWII development, though accelerated in the 1960s and after.\nIt’s been successful public policy in Denmark for many decades to increase access to schooling beyond the Grundskole (primary years…to about grade 9 in a US context). More than 45% of Danes between 25-40 have at least a Bachelor’s degree, and 20% have a Master’s. There would of course be more nuance in disaggregated age groups, but I wanted to keep this first story a bit more simple.\ncreated and posted March 24, 2025"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2025/index.html#prompt2",
    "href": "posts/30-day-chart-challenge-2025/index.html#prompt2",
    "title": "30 Day Chart Challenge 2025",
    "section": "Prompt #2 - Slope",
    "text": "Prompt #2 - Slope\nBuilding on what we saw in Prompt 1 for educational attainment by age group in 2023, let’s use the slope prompt to compare 2023 to 2005, the furthest back we can get educational attainment by age statistics via Danmarks Statistik’s StatBank.\nThe process for getting data is more or less the same as in Prompt 1, so I won’t repeat the code here. The only thing I did differently was collapse the 2-year and Bachelor’s degrees into one “college” category and collapsed Master’s and PhD into one post-bacc group.\nFor this chart I want to see if there are differences in highest educational attainment level by age group in two distinct years. Instead of one chart with too many lines, essentially a spaghetti graph, I made four charts and used the patchwork package (created by a Dane!) to facet out the individual charts into a (hopefully) coherent whole.\nKeep in mind that while of course many people will be represented in both 2005 and 2023, this is not a longitudinal study. It’s a snapshot of the Danish population by age groups at two distinct points in time.\nWe have the data, and it’s time to build the charts. But instead of copy-pasting the ggplot code to build each charts, let’s create a function and use that four times.\n\n\nShow code for the chart function\n# plotdf is the placeholder name in the function for the data you pass to run it\nslope_graph &lt;- function(plotdf) {\n    plotdf %&gt;%\n    ggplot(aes(x = year, y = age_ed_pct, group = age_group, color = age_group)) +\n        geom_line(size = 1) +\n        geom_point(size = .2) +\n        scale_x_continuous(\n            breaks = c(2005, 2023),\n            labels = c(\"2005\", \"2023\")) +\n        scale_y_continuous(limits = c(0, .5),\n            labels = scales::label_percent()) +\n        scale_color_brewer(palette = \"Set2\") +\n        labs(x = \"\", y = \"\") +\n        theme_minimal() +\n        theme(legend.position = \"bottom\", legend.spacing.x = unit(0, 'cm'),\n            legend.key.width = unit(3, 'cm'), legend.margin=margin(-10, 0, 0, 0),\n            legend.text = element_text(size = 10), legend.title = element_text(size = 12),\n            plot.title = element_text(hjust = .5, size = 16),\n            plot.subtitle = element_markdown(size = 14, vjust = -.5),\n            plot.caption = element_markdown(size = 12, face = \"italic\"),\n            axis.text.x = element_text(size = 11),\n            axis.text.y = element_text(size = 11),\n            panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n            strip.background = element_blank()) +\n        guides(color = guide_legend(label.position = \"bottom\", reverse = FALSE, \n            direction = \"horizontal\", nrow = 1,\n            title = \"Age Group\", title.position = \"top\"))\n}\n\n\nNow let’s put the function to use. We’ll create four plots, then stitch them together. Each plot will have its own descriptive title and subtitle, and the final chart will have one as well. Notice that we’re using the titles to tell a quick story or share an insight, not to dryly name what the chart is doing. The ggtext package gives us markdown enhancements for chart elements like the title and subtitle.\n\n\nShow code for making the final chart\n# create individual plots with titles and annotations\nplot_grundsk &lt;-\n    edattain2 %&gt;%\n    filter(ed_group2 == \"Grundskole/Primary\") %&gt;%\n    slope_graph() +\n    labs(title = \"Primary school (thru grade 10)\",\n             subtitle = \"*In 2023 Danes of all ages were much less likely to have stopped their education at&lt;br&gt;primary school\n             than they were in 2005.&lt;br&gt;*\")\n\nplot_hs &lt;-\n    edattain2 %&gt;%\n    filter(ed_group2 == \"Secondary\") %&gt;%\n    slope_graph() +\n    labs(title = \"Gymnasium & Vocational (High School)\",\n             subtitle = \"*From 2005 to 2023, there was a steep decline in the percentage of Danes&lt;br&gt;\n             aged 25-49 who were finished with education at the high school level, especially&lt;br&gt;\n             Danes under 40. For Danes older than 50 there was a very slight increase.*\")\n\nplot_colldegs &lt;-\n    edattain2 %&gt;%\n    filter(ed_group2 == \"Tertiary - 2yr/Bach\") %&gt;%\n    slope_graph() +\n    labs(title = \"2-year & Bachelor's Degrees\",\n             subtitle = \"*For Danes of all age groups, but especially those under 50, there was a&lt;br&gt;\n             noticable increase in the percentage earning 2 or 4 year degrees.*\")\n\nplot_masters &lt;-\n    edattain2 %&gt;%\n    filter(ed_group2 == \"Tertiary - Masters+\") %&gt;%\n    slope_graph() +\n    labs(title = \"Master's & PhD Degrees\",\n             subtitle = \"*The percentage of Danes earning a Master's or PhD increased across all age groups between 2005 and 2025;&lt;br&gt;\n             the increase was strongest among Danes under 50.*\")\n\nplot_grundsk + plot_hs + plot_colldegs + plot_masters +\n    plot_annotation(\n        title = \"Danes of all ages have become more likely to continue their education beyond primary level. Danes under 50 have over time become likely to earn a Master's.\",\n        subtitle = \"*Highest level of education earned by age groups, in 2005 and 2023.*\",\n        caption = \"*Data from Danmarks Statistik via danstat package. Groups are not longitudinal - age is for the person in the year of data collection.*\") &\n    theme(plot.title = element_text(size = 16), plot.subtitle = element_markdown(),\n        plot.caption = element_markdown(),\n        # plot.background add the grey lines between plots\n        plot.background = element_rect(colour = \"grey\", fill=NA))\n\n\n\nThe story here is hopefully clear. In 2005, more Danes, especially Danes 60+ years of age, had ended their education at primary level. By 2023 many more Danes were likely to have progressed beyond the primary level, and younger Danes especially were finishing Bachelor’s degrees and earning Master’s degrees.\nIt’s been government policy to support furthering education levels via free tuition and living stipends. This is made possible by a democratic-socialism approach to the economy, with higher tax rates on some earners than you might see in the US or UK, but for those taxes you received and your kids get at least 4 years of college paid for, and a small living stipend to boot.\nIn the US public higher education affordability models vary from state to state. Some states try to keep cost-of-attendance low via more direct state support. But not all students get full rides, even with need-based aid. I am much more a fan of the socialized approach to college affordability. the data shows it works in terms of increasing overall educational attainment.\ncreated and posted March 24, 2025"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2025/index.html#prompt3",
    "href": "posts/30-day-chart-challenge-2025/index.html#prompt3",
    "title": "30 Day Chart Challenge 2025",
    "section": "Prompt #3 - Circular",
    "text": "Prompt #3 - Circular\nWent wild with the circles here, sixty of them altogether. I took inspiration from this Albert Rapp post on using a bubble chart in place of a heat map.\nWe are still pulling data from from Danmarks Statistik via the danstat package. This time we’ll use the “Labour & income” domain to look at earnings by employment sector and highest level of education attained, the LONS11 table; løn is the Danish word for wage or salary. Of the twenty-five available earnings indicators, we’re using the MDRSNIT field, which is standardized monthly earnings. I couldn’t easily find the documentation on exactly how it was imputed, but given the analysis is across sectors, I thought it made the most sense. We’ll also pull in whether the employee was salaried or hourly (spoiler alert…it matters).\nLet’s first get and clean the data.\n\n\nShow code for getting and cleaning data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(danstat) # package to get Danish statistics via api\nlibrary(ggtext) # enhancements for text in ggplot\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n# lons11\ntable_meta &lt;- danstat::get_table_metadata(table_id = \"lons11\", variables_only = TRUE)\n\n# create variable list using the ID value in the variable\nvariables_ed &lt;- list(\n    list(code = \"uddannelse\", values = c(\"H10\", \"H20\", \"H30\", \"H40\", \"H50\", \"H60\", \"H70\", \"H80\")),\n    list(code = \"sektor\", values = c(1016, 1020, 1025, 1046)),\n    list(code = \"lønmål\", values = \"MDRSNIT\"), # avg monthly\n#   list(code = \"køn\", values = c(\"M\", \"K\")),\n  list(code = \"afloen\", values = c(\"TIME\", \"FAST\")),\n    list(code = \"tid\", values = 2023))\n\nsal1 &lt;- get_data(\"lons11\", variables_ed, language = \"en\") %&gt;%\n    as_tibble() %&gt;%\n    clean_names()\n\n# earnings came in character form, so changing to numeric and rounding up to whole numbers\n# group the ed levels and change the names, and change names on sector. make them factors with orders.\nsal2 &lt;- sal1 %&gt;%\n    mutate(income = as.numeric(indhold)) %&gt;%\n    mutate(income = round(income, 2)) %&gt;%\n    mutate(sector = case_when(\n        sektor == \"Corporations and organizations\" ~ \"Private sector\",\n        sektor == \"Government including social security funds\" ~ \"Gov't - National\",\n        sektor == \"Municipal government\" ~ \"Gov't - Municipal\",\n        sektor == \"Regional government\" ~ \"Gov't - Regional\")) %&gt;%\n    mutate(sector =\n        factor(sector,\n        levels = c(\"Gov't - Municipal\", \"Gov't - Regional\",\n        \"Gov't - National\", \"Private sector\"))) %&gt;%\n    mutate(ed_level =\n        case_when(uddannelse == \"H10 Primary education\" ~ \"Primary\",\n        uddannelse == \"H20 Upper secondary education\" ~ \"HS-Academic\",\n        uddannelse == \"H30 Vocational Education and Training (VET)\" ~\n        \"HS-Vocational\",\n        uddannelse == \"H40 Short cycle higher education\" ~\n        \"Short-cycle college\",\n        uddannelse == \"H50 Vocational bachelors educations\" ~\n        \"Bachelor-Vocational\",\n        uddannelse == \"H60 Bachelors programs\" ~ \"Bachelor-Academic\",\n        uddannelse == \"H70 Masters programs\" ~ \"Masters\",\n        uddannelse == \"H80 PhD programs\" ~ \"PhD\")) %&gt;%\n    mutate(ed_level =\n        factor(ed_level,\n        levels = c(\"Primary\", \"HS-Academic\", \"HS-Vocational\",\n        \"Short-cycle college\", \"Bachelor-Vocational\", \n        \"Bachelor-Academic\", \"Masters\", \"PhD\")))\n\n\nOk, we have the data, onto making the circles. Again, we’re doing a bubble chart that functions as a heat map by pegging the bubble size to the value of the earnings. I also added text labels next to the bubbles.\n\n\nShow code for making the chart\nsal2 %&gt;%\n    ggplot(aes(x = sector, y = ed_level)) +\n    geom_point(aes(col = income, fill = income, size = income), shape = 21) +\n    theme_minimal() +\n    scale_size_area(max_size = 15) +\n    labs(x = \"\", y = \"\",\n             title = \"More education = higher monthly earnings across all sectors. Larger effect for salaried vs hourly workers\",\n             subtitle = \"Monthly standardized earnings by education level & earner category\",\n             caption = \"*Data from Danmarks Statistik via danstat package*\") +\n    facet_wrap(~ afloen) +\n    theme(legend.position = 'top',\n                legend.justification = c(.95,0),\n                plot.title = element_text(size = 20),\n                plot.subtitle = element_text(size = 16),\n                plot.caption = element_markdown(size = 12, face = \"italic\")) +\n    guides(\n        col = guide_none(),\n        size = guide_none(),\n        fill = guide_colorbar(\n            barheight = unit(0.5, 'cm'),\n            barwidth = unit(10, 'cm'),\n            title.position = 'top',\n            title = \"Standardized Monthly Earnings (Danish Kroner)\")) +\n    scale_fill_continuous(limit = c(25000, 80000),\n        breaks = c(30000, 40000, 50000, 60000, 70000, 80000)) +\n    geom_text(data = subset(sal2, !is.na(income)),\n        aes(label =\n                    paste0(round(income, 0), \" DKK\")), nudge_x = 0.35, size = 3)\n\n\nI see the story here being that education level matters when it comes to how much Danish workers earn in a month. If you work in the private sector, having a Masters 8,000 to 10,000 more DKK per month than Bachelor’s degrees. The difference is even greater working for regional government agencies. Also, hourly workers with the same education earn less per month than salaried workers. But without the component parts of the total earnings I can’t explain exactly why.\nThe bottom line is, as we’re seeing so far in the charts, there’s both an increase in the number of people earning college and post-bacc degrees, and the payoff seems to be there for the extra year of school it takes for most programs.\nBut again, given that education is socialized and subsidized through taxes, the opportunity cost for that extra year is nothing, because the basic higher education coverage is for five years, for tuition and living stipend. And higher wage earners pay more taxes, helping to boost everyone’s opportunity to earn the degrees they desire, without being worried about cost.\nIt’s worth mentioning here, that there’s discussion ongoing in the Danish government to squeeze education spending and spots available in the gymnasiums (essentially high school) that feed university programs. I need to read up on that a bit more as I tell this Danish education story while doing the chart challenge.\nOh, the exchange rate is usually around 6.5 or so DKK to USD. So while it may seem amazing to earn 50000 per month, and while that’s not bad in this market, adjust your bearings if you’re thinking in USD or EURO.\n\ncreated and posted April 2, 2025"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2025/index.html#prompt4",
    "href": "posts/30-day-chart-challenge-2025/index.html#prompt4",
    "title": "30 Day Chart Challenge 2025",
    "section": "Prompt #4 - Big or Small",
    "text": "Prompt #4 - Big or Small\nFor this post I’ll stay with a circle theme and make a simple circle packing chart, sort of a heat map or tree map but with circles, the size of the circles corresponding to the variable value. I’m using the code-through from the R Graph Gallery to get going, as I’ve never done this type of chart before. The packcircles package does the computational work to create the circle objects.\nKeeping with the thematic constraint I’ve set, I’ll once again be using Danish education data from Danmarks Statistik via the danstat package.\nThis chart will display Bachelor’s degrees awarded in 2023, by top-line degree field (Science, Humanities, Education, etc). For instance, Law is a sub-field in Social Sciences, one of many. To make it a bit easier I’ll limit this chart to academic Bachelor’s, not the vocational degrees. There’s definitely a more focused project worth doing that displays the sub-fields within the top-line areas, shown as nested smaller circles within larger circles, or something interactive. for now this will suffice.\nTo start, let’s get the data. It’s from a different table than the last chart, so I’ll show the work.\n\n\nShow code for getting and cleaning data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(danstat) # package to get Danish statistics via api\nlibrary(packcircles) # makes the circles!\nlibrary(ggtext) # enhancements for text in ggplot\nlibrary(ggrepel) # helps with label offsets\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n# this table is UDDAKT60, set to all lower-case in the table_id call\ntable_meta &lt;- danstat::get_table_metadata(table_id = \"uddakt60\", variables_only = TRUE)\n\n# create variable list using the ID value in the variable\n# the uddanelse codes are the top-line degree fields.\nvariables_ed &lt;- list(\n    list(code = \"uddannelse\", values = c(\"H6020\", \"H6025\", \"H6030\", \"H6035\",\n                 \"H6039\", \"H6059\", \"H6075\", \"H6080\", \"H6090\")),\n    list(code = \"fstatus\", values = c(\"F\")),\n    list(code = \"tid\", values = 2023))\n\n# start with a larger dataset in case we want to do more analysis or viz with it\ndegs1 &lt;- get_data(\"uddakt60\", variables_ed, language = \"en\") %&gt;%\n    as_tibble() %&gt;%\n    # shorten the given degree field names\n    mutate(deg_field = case_when(UDDANNELSE == \"H6020 Educational, BACH\" ~ \"Educ.\",\n            UDDANNELSE == \"H6025 Humanities and theological, BACH\" ~ \"Humanities\",\n            UDDANNELSE == \"H6030 Arts, BACH\" ~ \"Arts\",\n            UDDANNELSE == \"H6035 Science, BACH\" ~ \"Science\",\n            UDDANNELSE == \"H6039 Social Sciences, BACH\" ~ \"Social Science\",\n            UDDANNELSE == \"H6059 Technical sciences, BACH\" ~ \"Tech Science\",\n            UDDANNELSE == \"H6075 Food, biotechnology and laboratory technology, BACH\" \n                        ~ \"Food/Biotech/LabTech\",\n            UDDANNELSE == \"H6080 Agriculture, nature and environment, BACH\"\n                     ~ \"Agricultural Science\",\n            UDDANNELSE == \"H6090 Health science, BACH\" ~ \"Health Sciences\")) \n\n# keep only what we ween to do the circles\ndegs2 &lt;- degs1 %&gt;%\n    select(group = deg_field, value = INDHOLD, text)\n\n# this creates x & y coordinates and circle radius values.\ndegs_packing &lt;- circleProgressiveLayout(degs2$value, sizetype='area')\n# new tibble putting it together\ndegs3 &lt;- cbind(degs2, degs_packing)\n# not sure exactly what this does, but it's an important step\ndegs.gg &lt;- circleLayoutVertices(degs_packing, npoints=50)\n\n\nOk, we now have a nice clean tibble, time for the plot. What took the most time here was working on the labels…getting them to the correct size, adjusting for circle size. You’ll notice one label set off to the side thanks to the ggrepel package.\n\n\nShow code for making the chart\nggplot() +\n    geom_polygon_interactive(\n        data = degs.gg,\n        aes(x, y, group = id, fill=id, tooltip = data$text[id], data_id = id),\n        colour = \"black\", alpha = 0.6) +\n    scale_fill_viridis() +\n    geom_text(data = degs3 %&gt;% filter(value &gt; 6000),\n            aes(x, y, label = paste0(group, \"\\n\", format(value, big.mark=\",\"))),\n            size=7, color=\"black\") +\n    geom_text(data = degs3 %&gt;% filter(between(value, 2410, 3350)),\n            aes(x, y, label = paste0(group, \"\\n\", format(value, big.mark=\",\"))),\n            size=7, color=\"black\") +\n    geom_text(data = degs3 %&gt;% filter(between(value, 600, 2410)),\n            aes(x, y, label = paste0(group, \"\\n\", format(value, big.mark=\",\"))),\n            size=6, color=\"black\") +\n    geom_text_repel(data = degs3 %&gt;% filter(between(value, 200, 400)),\n            aes(x, y, label = paste0(group, \"\\n\", format(value, big.mark=\",\"))),\n            size=4, color=\"black\",\n            max.overlaps = Inf, nudge_x = -110, nudge_y = 50,\n            segment.curvature = 0,\n            segment.ncp = 8,\n            segment.angle = 30) +\n    labs(x = \"\", y = \"\",\n             title = \"Social Sciences were the most popular Bachelor's degrees awarded by Danish universities in 2023\",\n             subtitle = \"*Labels not diplayed: Education = 134, Food Science = 61*\",\n             caption = \"*Data from Danmarks Statistik via danstat package*\") +\n    theme_void() +\n    theme(legend.position=\"none\", plot.margin=unit(c(0,0,0,0),\"cm\"),\n            plot.title = element_markdown(size = 16),\n            plot.subtitle = element_markdown(size = 12),\n            plot.caption = element_markdown(size = 8)) +\n    coord_equal()\n\n\n\nThere you have it, academic Bachelor’s degrees awarded by Danish universities in 2023. The Social Sciences category accounts for more than twice as many degrees as the next field. Again, the next interesting step would be to visualize the disaggregation of the main fields, and see what subfields are the most popular. It would also be interesting to track the numbers over time, see which majors rose or fell.\ncreated and posted April 2, 2025"
  },
  {
    "objectID": "posts/30-day-chart-challenge-2025/index.html#prompt5",
    "href": "posts/30-day-chart-challenge-2025/index.html#prompt5",
    "title": "30 Day Chart Challenge 2025",
    "section": "Prompt #5 - Ranking",
    "text": "Prompt #5 - Ranking\nFor this prompt I desperately wanted to do something relating to music from the band The English Beat because of Ranking Roger. But I’d already committed to the Danish education bit, and Spotify shut down all the interesting API endpoints and don’t seem to be reanimating them any time soon. Something about “safety”, but it’s likely AI crawlers, the same ones increasing hosting costs for Wikipedia and other websites as they steal content for their models. And funny for Spotify to complain about someone els using their work to monetize a product.\nBut anyway, let’s talk ranks. Specifically let’s look at the rankings of Bachelor degrees awarded in Denmark in 2023. I’ll focus on the academic (4-year) degrees because I wanted to look at majors within broader disciplines and look at differences by gender.\nAgain, data comes from Danmarks Statistik via the danstat package.\nSo first, let’s get the data. It’s from the same table used in prompt 4 but retreived slightly differently so I’ll show it again. This time we’re getting all the majors within the broader disciplines so in the list call we use list(code = \"uddannelse\", values = \"*\").\n\n\nShow code for getting and cleaning data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(danstat) # package to get Danish statistics via api\nlibrary(packcircles) # makes the circles!\nlibrary(ggtext) # enhancements for text in ggplot\nlibrary(ggrepel) # helps with label offsets\n\n# some custom functions\nsource(\"~/Data/r/basic functions.R\")\n\n# this table is UDDAKT60, set to all lower-case in the table_id call\ntable_meta &lt;- danstat::get_table_metadata(table_id = \"uddakt60\", variables_only = TRUE)\n\n# create variable list using the ID value in the variable.\nvariables_ed &lt;- list(\n    list(code = \"uddannelse\", values = \"*\"),\n    list(code = \"fstatus\", values = c(\"F\")),\n    list(code = \"køn\", values = c(\"M\", \"K\")),\n    #list(code = \"alder\", values = c(\"TOT\")),\n    list(code = \"tid\", values = 2023))\n\nbacdegs1 &lt;- get_data(\"uddakt60\", variables_ed, language = \"en\") %&gt;%\n    as_tibble()\n\nbacdegs &lt;- bacdegs1 %&gt;%\n    # clean up the degree text and code field\n    mutate(deg_code = str_extract(UDDANNELSE, \"^[^ ]+\")) %&gt;%\n    mutate(deg_name = sub(\"^\\\\S+\\\\s+\", '', UDDANNELSE)) %&gt;%\n    mutate(deg_name = str_remove(deg_name, \", BACH\")) %&gt;%\n    mutate(deg_name = str_remove(deg_name, \" BACH\")) %&gt;%\n    mutate(deg_group = case_when(\n        deg_code %in% c(\"H6020\", \"H6025\", \"H6030\", \"H6035\", \"H6039\",\n                                        \"H6059\", \"H6075\", \"H6080\", \"H6090\") ~ \"Main\",\n        deg_code == \"H60\" ~ \"All\",\n        TRUE ~ \"Sub\")) %&gt;%\n    # create discipline category for making individual plots.\n    mutate(deg_field =  case_when(\n        str_detect(deg_code, \"H6020\") ~ \"Education\",\n        str_detect(deg_code, \"H6025\") ~ \"Humanities\",\n        str_detect(deg_code, \"H6030\") ~ \"Arts\",\n        str_detect(deg_code, \"H6035\") ~ \"Science\",\n        str_detect(deg_code, \"H6039\") ~ \"Social Sciences\",\n        str_detect(deg_code, \"H6059\") ~ \"Technical sciences\",\n        str_detect(deg_code, \"H6075\") ~ \"Technical sciences\",\n        str_detect(deg_code, \"H6080\") ~ \"Agriculture & Veterinary\",\n        str_detect(deg_code, \"H6090\") ~ \"Health science\",\n        TRUE ~ \"no\")) %&gt;%\n    rename(degs_n = INDHOLD, sex = KØN)\n\n\nNow that we have tidy data, let’s make the charts. This time out it’s horizontal bar charts, ordered by number of degrees awarded. Note that because I’m faceting by gender, and because I want to show the top majors in descending order for each gender, I use the scales = \"free_y\" option in the facet function.\nA few things before we see the work. First, I had hoped to write a function and output all the by-discipline charts but it was too complicated to get each chart to look exactly as I wanted, and I realized I’d have to do lots of individual formatting anyway. So sadly it’s “copy-paste-amend” for all the charts. I’ll only show the code for the chart showing all degrees and for one of the discipline charts, as the rest of the discipline charts were essentially the same, just minor edits on the geom_text() code.\nSecond, I had to make a design choice regarding the x axes. I decided to keep them standard to the maximum number of degrees by major within each discipline, so the x axes for men & women is the same. To me, having different x axes maximums in the same chart was distorting the scope of the bars, making different numbers look the same.\nThird, I am only displaying degree counts, not percentages. I went back and forth on that as well, and decided to go with the counts. Maybe I’ll redo the analysis later and switch to or add percentages.\n\n\nShow code for making the charts\n\nbacdegs %&gt;%\n    filter(deg_group == \"Main\") %&gt;%\n    {. -&gt;&gt; tmp} %&gt;%\n    ggplot(aes(x = degs_n, y = reorder_within(deg_name, degs_n, sex), fill = sex)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"#E66100\", \"#5D3A9B\")) +\n    scale_y_reordered() +\n    scale_x_continuous(labels = comma) +\n    labs(x = \"\", y = \"\",\n             title = \"All Bachelor degrees by field & sex, 2023\",\n             subtitle = glue::glue(\"*Total degrees earned = 19,199: Men = 8,694, Women = 10,505*\"),\n             caption = \"*Data from Danmarks Statistik via danstat package*\") +\n    facet_wrap(~ sex, scales = \"free_y\") +\n    geom_text(data = subset(tmp, degs_n &gt; 3000),\n                        aes(label = comma(degs_n)), hjust = 1.5, color = \"white\") +\n    geom_text(data = subset(tmp, degs_n &lt; 3000),\n                        aes(label = comma(degs_n)), hjust = -.5, color = \"black\") +\n    theme_minimal() +\n    theme(panel.grid = element_blank(),\n                legend.position = \"none\",\n                plot.subtitle = element_markdown(),\n                plot.caption = element_markdown())\n\n\n# agriculture and veterinary sciences\nbacdegs %&gt;%\n    filter(deg_group == \"Sub\") %&gt;%\n    filter(deg_field == \"Agriculture & Veterinary\") %&gt;%\n    {. -&gt;&gt; tmp} %&gt;%\n    ggplot(aes(x = degs_n, y = reorder_within(deg_name, degs_n, sex), fill = sex)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"#E66100\", \"#5D3A9B\")) +\n    scale_y_reordered() +\n    scale_x_continuous(labels = comma) +\n    labs(x = \"\", y = \"\",\n             title = \"Agriculture & Veterinary Science Bachelor degrees by field & sex, 2023\",\n             subtitle = glue::glue(\"*Total degrees earned = 308: Men = 55, Women = 253*\")) +\n    facet_wrap(~ sex, scales = \"free_y\") +\n    geom_text(data = subset(tmp, degs_n &gt; 100),\n                        aes(label = comma(degs_n)), hjust = 1.5, color = \"white\") +\n    geom_text(data = subset(tmp, degs_n &lt; 100),\n                        aes(label = degs_n), hjust = -.5, color = \"black\") +\n    theme_minimal() +\n    theme(panel.grid = element_blank(),\n                legend.position = \"none\",\n                plot.subtitle = element_markdown())\n\n\n\nThis first chart shows all degrees awarded by discipline and sex. The most immediate thing to notice is that more Bachelor degrees were awarded to women than to men. But that tracks with a chart I did last year, showing rates of educational attainment by sex and it’s similar to other countries, including the US.\nSocial science degrees were the most popular for both men and women. After that it changes a bit. Humanities were 2nd most popular for women, technical sciences next for men. And so forth. To be honest, it’s kind of a shock to see degree fields so starkly gendered now. I would have though some disciplines would have been a bit more evenly split.\nBelow are the rest of the charts, one for each major discipline. They are lightboxed, so click on the image to enlarge it, and click esc or outside the image to restore it to the page.\nThe only one I did not make was for Education degrees because there’s only one major. There were 134 degrees awarded, 117 to women, 17 to men. If you’re interested you can comb through and see which majors were most popular in each discipline.\nThis makes me think I probably should have done this as a Tableau chart to let the user filter by discipline. Or I could have done parameterized charts in Quarto. Then I could have added other degree levels. Lots of data prep, but it would be a useful chart. Maybe after the 30 Day Challenge is over.\nAgriculture & Veterinary Sciences\n\n\nArts\n\n\nHealth Sciences\n\n\nHumanities\n\n\nScience\n\n\nSocial Science\n\n\nTechnical Science\n\n\ncreated and posted April 3, 2025"
  }
]