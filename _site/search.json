[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gregers kjerulf dubrow",
    "section": "",
    "text": "Hi! I’m in the midst of migrating from blogdown/hugo to Quarto, so things are a bit barren until I’ve converted all the old posts from rmd to qmd, tested the publishing workflow, and made sure things work well.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nMigration from Hugo to Quarto\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\nhowto\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\n…and we’re back. Migration and resurrection\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, February 2, 2021 - HBCU Enrollment\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nhigher education\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2021\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, November 27, 2018 - Maryland Bridges\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, November 24, 2020 - Hiking Trails in WA State\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "gregers kjerulf dubrow",
    "section": "",
    "text": "I’m a data analyst/data scientist living in København Denmark, with experitise in higher education systems, policy, undergraduate admissions & enrollment management, and student success.\nI work in r for everything from data import & cleaning, to analysis, modeling & visualization. I have also used SAS and excel and have working knowledge of SQL and a passing familiarity with Tableau.\nI was born in Denmark and grew up in the US. After many years in San Francisco I lived in Lyon France from May to December 2022 , working as an ESL instructor after earning a CELTA via the ELT Hub.\nIn this space I’ll mostly be posting code-throughs and results from my personal data projects; music, football (the soccer kind), education, #tidytuesday data projects and other work. All views expressed here are my own.\nOver the years I’ve made some music with Gosta Berling, Slowness, Big Still and Idle Wilds. My photography is at Flickr."
  },
  {
    "objectID": "posts/tidy_tuesday_maryland_bridges/index.html",
    "href": "posts/tidy_tuesday_maryland_bridges/index.html",
    "title": "Tidy Tuesday, November 27, 2018 - Maryland Bridges",
    "section": "",
    "text": "This dataset was posted to the #TidyTuesday repo back in November 2018. I worked on it a bit then, but didn’t properly finish it until March of 2020. With the blog finally set up figured I might as well post it as an entry.\nUpdate for migration from Hugo to Quarto…now that code-fold is native to code chunks, I’ll sometimes use if for long bits of code. Just click the down arrow to show code\n\n# readin in data, create df for plots\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\nFirst let’s read in the file from the raw data file on github\n\ntt_balt_gh <-\nread_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-27/baltimore_bridges.csv\",\n         progress = show_progress())\n\n\n\nOrganize and clean the data for analysis\n\nclean up some date and name issues\na decade-built field, factored for sorting\ntime since last inspection\n\n\n\nshow data cleaning code\n# keeping the comparison date March 21 when I originally did analysis\ntoday <- as.Date(c(\"2020-03-21\"))\n#Sys.Date()\ntoday_yr <- as.numeric(format(today, format=\"%Y\"))\n\ntt_mdbrdf <- as.data.frame(tt_balt_gh) %>%\n  mutate(age = today_yr - yr_built) %>%\n  #  mutate(vehicles_n = as.numeric(str_remove(vehicles, \" vehicles\")))\n  ## not needed, avg_daily_traffic has same info\n  mutate(inspection_yr = inspection_yr + 2000) %>%\n  mutate(county = ifelse(county == \"Baltimore city\", \"Baltimore City\", county)) %>%\n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  mutate(bridge_condition = factor(bridge_condition, levels = c(\"Good\", \"Fair\", \"Poor\"))) %>%\n  mutate(decade_built = case_when(yr_built <= 1899 ~ \"pre 1900\", \n                                  yr_built >= 1900 & yr_built <1910 ~ \"1900-09\",\n                                  yr_built >= 1910 & yr_built <1920 ~ \"1910-19\",\n                                  yr_built >= 1920 & yr_built <1930 ~ \"1920-29\",\n                                  yr_built >= 1930 & yr_built <1940 ~ \"1930-39\",\n                                  yr_built >= 1940 & yr_built <1950 ~ \"1940-49\",\n                                  yr_built >= 1950 & yr_built <1960 ~ \"1950-59\",\n                                  yr_built >= 1960 & yr_built <1970 ~ \"1960-69\",\n                                  yr_built >= 1970 & yr_built <1980 ~ \"1970-79\",\n                                  yr_built >= 1980 & yr_built <1990 ~ \"1980-89\",\n                                  yr_built >= 1990 & yr_built <2000 ~ \"1990-99\",\n                                  yr_built >= 2000 & yr_built <2010 ~ \"2000-09\",\n                                  TRUE ~ \"2010-19\")) %>%\n  mutate(decade_built = factor(decade_built, levels = \n                                 c(\"pre 1900\", \"1900-09\", \"1910-19\", \"1920-29\", \"1930-39\",\n                                   \"1940-49\", \"1950-59\", \"1960-69\", \"1970-79\", \n                                   \"1980-89\", \"1990-99\", \"2000-09\", \"2010-19\"))) %>%\n  mutate(inspect_mmyy = ISOdate(year = inspection_yr, month = inspection_mo, day = \"01\")) %>%\n  mutate(inspect_mmyy = as.Date(inspect_mmyy, \"%m/%d/%y\")) %>%\n  mutate(inspect_days = today - inspect_mmyy) %>%\n  mutate(inspect_daysn = as.numeric(inspect_days)) %>%\n  mutate(inspect_years = inspect_daysn/ 365.25) %>%\n  mutate(inspect_months = inspect_daysn / 30.417)\n\n\n\n\nThe first few charts look at bridges built by decade, the condition of all bridges by county, and how long since last inspection.\n\ntt_mdbrdf %>% \n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  count(decade_built) %>%\n  ggplot(aes(decade_built, n)) +\n  geom_bar(stat = \"identity\", fill = \"navy\") +\n  geom_text(aes(label = n), color = \"white\", vjust = 1.2) +\n  labs(title = \"Peak bridge-building years in Maryland were 1950s to 1990s\" ,\n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nBaltimore City has the lowest percentage of bridges in good condition, Anne Arundel the most. Baltimore City & Harford County seems to have the largest percentage of bridges in poor condition.\n\n\nshow stacked bar chart code\n## percent bridge condition by county\n# need to create df object to do subset label call in bar chart\nbrcondcty <- \ntt_mdbrdf %>%\n  count(county, bridge_condition) %>%\n  group_by(county) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() \n\nggplot(brcondcty, aes(x = county, y = pct, fill = bridge_condition)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(data = subset(brcondcty, bridge_condition != \"Poor\"), \n            aes(label = percent(pct)), position = \"stack\", \n            color= \"#585858\", vjust = 1, size = 3.5) +\n  scale_y_continuous(label = percent_format()) +\n  labs(title = \"Percent bridge condition by county\" , \n        x = \"\", y = \"\", fill = \"Bridge Condition\") +\n  scale_fill_brewer(type = \"seq\", palette = \"Blues\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nGiven the condition percentages in Baltimore County & City and Harford County, it’s no surprise that their bridges are older than in other counties.\n\n\nshow bar chart code\n## median age of bridges by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medage = median(age)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medage)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medage, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median bridge age by county\" , \n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt’s somewhat reassuring then that Baltimore City bridges at least have less time in months since last inspection than do the counties.\n\n\nshow bar chart code\n## median months since last inspection by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medinsp = median(inspect_months)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medinsp)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medinsp, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median months since last inspection, by county\",\n       subtitle = \"as of March 2020\",\n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt might be the outliers pulling the smoothing line straight, but there doesn’t seem to be too much of a relationship between age and time since last inspection.\n\n\nshow scatterplot code\n## age by months since last inspection\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\n\nAnd in fact, removing the outliers shows a slight relationship; the older bridges do seem to get inspected more frequently. In terms of a better visualization, looking at this again, I wonder if some jittering or another type of plot might have been more visually appealing, givne the clustering of most recent inspections.\n\n# same code as above but with outliers removed\ntt_mdbrdf %>%\n  filter(age <150, inspect_months <60) %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(title = \"Months since inspection, outliers removed\", \n       x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\nNot sure if scatter-plot with colors by county is best way to go for this idea. Maybe a tree map?\n\n\nshow scatterplot code\n# same but colors by county\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  labs(title = \"Months since last inspection (from current date)\", \n       x = \"Months since last inspection (from current date)\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.8, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nFunky distributions here…Anne Arundel & Baltimore City have the highest median daily riders, but Howard County’s upper quartile is way out there.\nshowing the code here to illustrate how I like to run the mean & interquartiles in the same code as rendering the plot.\n\n# median & interquartiles of daily riders of bridges by county -\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medtraf = median(avg_daily_traffic),\n            lq = quantile(avg_daily_traffic, 0.25),\n            uq = quantile(avg_daily_traffic, 0.75)) %>%\n  ungroup() %>%\n  ggplot(aes(county, medtraf)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"navy\") +\n  geom_point(size = 3, color = \"orange\", alpha = .8) +\n  geom_text(aes(label = comma(medtraf, digits = 0)), \n            size = 4, color = \"orange\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = comma(uq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = comma(lq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Median & interquartile ranges of average daily riders per bridge, by county\" , \n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nAs with the other scatterplot with colors for county, might need a different way to see relationship between bridge age and daily traffic by county.\n\n\nshow scatterplot code\n## age by avg daily riders by county\ntt_mdbrdf %>%\n  ggplot(aes(avg_daily_traffic, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  scale_x_continuous(labels = comma) +\n  labs(title = \"Average daily traffic per bridge, by county\" , \n        x = \"Average daily traffic\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.75, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nCover image for post (CC BY-SA 4.0) from Wikipedia\nThis post was last updated on 2023-05-14"
  },
  {
    "objectID": "posts/migrating-from-hugo-to-quarto/index.html",
    "href": "posts/migrating-from-hugo-to-quarto/index.html",
    "title": "Migration from Hugo to Quarto",
    "section": "",
    "text": "The Saône & The Rhône converge in Lyon\n\n\nAs I mentioned in the Migration and Resurrection post, some issues with my old Hugo build and Netlify playing nicely with Hugo & Wowchemy led to me deciding to rebuild things in Quarto. I was thinking about it anyway, as Quarto seems robust and even more user-friendly (to me) than blogdown & rmarkdown, which I still love. I just love Quarto more.\nThe basic idea of the blog remains the same - present the code & analysis for topics that interest me. While switching from SAS to r, I learned so much from reading tons of data blogs and watching how-to videos that I wanted to give back to the rstats community by showing and explaining my work. It also helps me as my own reference for code and approach to visualization when I forget how I did a pivot-long or cleaned and set-up data to work well with the visualization I wanted to do.\nI decided to set some ground rules for the old posts:\n\nMinimal text edits…only for typos and occasional clunky language.\nBut no editing the code. The projects I posted showed where I was in using r at the time. Though to be honest, if anything I’ve regressed a bit as I haven’t used r that much since I left my last data job. Besides, the rmarkdown -> quarto migration already entailed enough editing in the YAML and code chunk headers.\nThe only exception to the code edit rule was adding code-fold options to some code chunks. It wasn’t easily doable in blogdown & Hugo when I first launched the site but it’s a native functionality to Quarto, so hoorah!\nRepost the projects with date-stamps from the original posting. I want this to still accurately document my own data analysis/data science progression, even with that long gap between posts.\n\nSo how did I do it?\nFirst I spent a sunny weekend afternoon in May watching two presentations about Quarto.\nThis Welcome to Quarto Workshop from August 2022 led by Thomas Mock.\n\n\n\n\n\n\n\n\n\n\nAfter that it was Isabella Velásquez’s presentation on building a blog with Quarto.\nI also read the extensive Quarto documentation.\nEach of their personal blogs are nicely done in Quarto so I’ll also be poking around their github repos.\nAs I had already working with blogdown & rmarkdown, the transition to Quarto was smooth. Minor grammatical differences in the YAML and code chunks, but nothing that didn’t make sense.\nSetting up the blog is as simple as starting any new project in r. Just go to: File -> New Project -> New Directory -> Quarto blog and fill in the name and directory location.\nAfter setting up the basic design using one of the packaged themes and drafting the landing and About pages, I pushed the new files to github and hoped that Netlify would play nicely and render the new site.\nOn the first commit, which wiped out all of my old content and replaced with the new files, Netlify did its thing but I got the dreaded Error 404, site not found. With a little digging I found out that I had to go the Build & Deploy -> Continuous Deployment -> Build Settings section and add _site to the Publish Directory box like this:\n\n\n\n\n\nDid that, did another git commit and voila, up and running.\nNext step was to spend a sunny Sunday afternoon redoing my old rmd files to qmd, and navigating the differences in YAML and code chunk options.\n\n\n\ncode chunk options in rmd\n\n\n\n\n\ncode chunk options in qmd\n\n\nI also like the intelligent completion bits in Quarto \nand using ctrl + space to see all the parameters for the option you’re setting.\nquick aside…used veed.io for convert a screen-capture movie to animated gif…quick and easy\nGoing forward I’ll probably tweak the design now and then as I learn a bit more customization and functionality in Quarto and learn CSS and other styling tools for things like wrapping text around images and other tweaks and enhancements. But for now the site looks good and it’s time to get back to adding new data posts.\nThis post was last updated on 2023-05-14"
  },
  {
    "objectID": "posts/blog-migration-and-resurrection/index.html",
    "href": "posts/blog-migration-and-resurrection/index.html",
    "title": "…and we’re back. Migration and resurrection",
    "section": "",
    "text": "So much has happened since my last post here in February 2021. The blog was set up during the early stages of COVID lockdown when all of a sudden I had more time on my hands.\nSoon after that last post my wife and I made the decision to move to Europe in early 2022, so started preparations for that - arrange for a shipping container, learn French, apply for a CELTA program.\nThe world started to repoen a bit in late summer 2021, but then my wife broke her ankle. So all of a sudden I was doing lots of home health care & keeping the house running on top of work. The wasn’t much brain space for independent data projects & keeping the blog running.\nThen in late April 2022 we moved. The first stop was Lyon France\n\n\n\n\n\nwhere I earned the CELTA via the ELT Hub.\nI had expected teaching gigs to trickle in and I’d do independent data work while also spending time learning French and exploring Lyon and more of France. But the gigs came in abundance, and they were all new preps. So free time was at a minimum.\nThen in December 2022 we moved again. A job at CIEE in Denmark\n\n\n\n\n\ncame up so we accelerated the ultimate end goal of moving here - back to where I was born and still have lots of family and friends. Now that we’re settled in our first non-sublet in almost a year and I’ve gotten settled at work, there’s finally some time to get my head back into data work. I was partly inspired by serving as a mentor in the DDSA mentoring program. I also started using r at work to automate a few mundane tasks and to do a little analysis on our students.\nI’ll write about the migration process in a bit more detail in another post, but the tl/dr as to why is because in trying to do a simple update to my old main picture (me in a mask, early in lockdown) the build was failing in Netlify. So rather than try and diagnose the Hugo/Wowchemy issues (which turned out to be the Ubuntu deploy image…an easy reset), I saw that Posit’s new Quarto platform is very robust and a bit easier to publish with, so I decided to just rebuild the entire site.\nTo read about the migration details, go here. All of the old posts are up, time-stamped with the original posting dates. And hopefully soon, new posts as I work through a fairly long list of project ideas and try to get back to working on #tidytuesday datasets.\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/tidy-tuesday-maryland-bridges/index.html",
    "href": "posts/tidy-tuesday-maryland-bridges/index.html",
    "title": "Tidy Tuesday, November 27, 2018 - Maryland Bridges",
    "section": "",
    "text": "The Francis Scott Key Bridge in Baltimore\n\n\nThis dataset was posted to the #TidyTuesday repo back in November 2018. I worked on it a bit then, but didn’t properly finish it until March of 2020. With the blog finally set up figured I might as well post it as an entry.\nUpdate for migration from Hugo to Quarto…now that code-fold is native to code chunks, I’ll sometimes use if for long bits of code. Just click the down arrow to show code\n\n# readin in data, create df for plots\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\nFirst let’s read in the file from the raw data file on github\n\ntt_balt_gh <-\nread_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-27/baltimore_bridges.csv\",\n         progress = show_progress())\n\n\n\nOrganize and clean the data for analysis\n\nclean up some date and name issues\na decade-built field, factored for sorting\ntime since last inspection\n\n\n\nshow data cleaning code\n# keeping the comparison date March 21 when I originally did analysis\ntoday <- as.Date(c(\"2020-03-21\"))\n#Sys.Date()\ntoday_yr <- as.numeric(format(today, format=\"%Y\"))\n\ntt_mdbrdf <- as.data.frame(tt_balt_gh) %>%\n  mutate(age = today_yr - yr_built) %>%\n  #  mutate(vehicles_n = as.numeric(str_remove(vehicles, \" vehicles\")))\n  ## not needed, avg_daily_traffic has same info\n  mutate(inspection_yr = inspection_yr + 2000) %>%\n  mutate(county = ifelse(county == \"Baltimore city\", \"Baltimore City\", county)) %>%\n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  mutate(bridge_condition = factor(bridge_condition, levels = c(\"Good\", \"Fair\", \"Poor\"))) %>%\n  mutate(decade_built = case_when(yr_built <= 1899 ~ \"pre 1900\", \n                                  yr_built >= 1900 & yr_built <1910 ~ \"1900-09\",\n                                  yr_built >= 1910 & yr_built <1920 ~ \"1910-19\",\n                                  yr_built >= 1920 & yr_built <1930 ~ \"1920-29\",\n                                  yr_built >= 1930 & yr_built <1940 ~ \"1930-39\",\n                                  yr_built >= 1940 & yr_built <1950 ~ \"1940-49\",\n                                  yr_built >= 1950 & yr_built <1960 ~ \"1950-59\",\n                                  yr_built >= 1960 & yr_built <1970 ~ \"1960-69\",\n                                  yr_built >= 1970 & yr_built <1980 ~ \"1970-79\",\n                                  yr_built >= 1980 & yr_built <1990 ~ \"1980-89\",\n                                  yr_built >= 1990 & yr_built <2000 ~ \"1990-99\",\n                                  yr_built >= 2000 & yr_built <2010 ~ \"2000-09\",\n                                  TRUE ~ \"2010-19\")) %>%\n  mutate(decade_built = factor(decade_built, levels = \n                                 c(\"pre 1900\", \"1900-09\", \"1910-19\", \"1920-29\", \"1930-39\",\n                                   \"1940-49\", \"1950-59\", \"1960-69\", \"1970-79\", \n                                   \"1980-89\", \"1990-99\", \"2000-09\", \"2010-19\"))) %>%\n  mutate(inspect_mmyy = ISOdate(year = inspection_yr, month = inspection_mo, day = \"01\")) %>%\n  mutate(inspect_mmyy = as.Date(inspect_mmyy, \"%m/%d/%y\")) %>%\n  mutate(inspect_days = today - inspect_mmyy) %>%\n  mutate(inspect_daysn = as.numeric(inspect_days)) %>%\n  mutate(inspect_years = inspect_daysn/ 365.25) %>%\n  mutate(inspect_months = inspect_daysn / 30.417)\n\n\n\n\nThe first few charts look at bridges built by decade, the condition of all bridges by county, and how long since last inspection.\n\ntt_mdbrdf %>% \n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  count(decade_built) %>%\n  ggplot(aes(decade_built, n)) +\n  geom_bar(stat = \"identity\", fill = \"navy\") +\n  geom_text(aes(label = n), color = \"white\", vjust = 1.2) +\n  labs(title = \"Peak bridge-building years in Maryland were 1950s to 1990s\" ,\n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nBaltimore City has the lowest percentage of bridges in good condition, Anne Arundel the most. Baltimore City & Harford County seems to have the largest percentage of bridges in poor condition.\n\n\nshow stacked bar chart code\n## percent bridge condition by county\n# need to create df object to do subset label call in bar chart\nbrcondcty <- \ntt_mdbrdf %>%\n  count(county, bridge_condition) %>%\n  group_by(county) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() \n\nggplot(brcondcty, aes(x = county, y = pct, fill = bridge_condition)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(data = subset(brcondcty, bridge_condition != \"Poor\"), \n            aes(label = percent(pct)), position = \"stack\", \n            color= \"#585858\", vjust = 1, size = 3.5) +\n  scale_y_continuous(label = percent_format()) +\n  labs(title = \"Percent bridge condition by county\" , \n        x = \"\", y = \"\", fill = \"Bridge Condition\") +\n  scale_fill_brewer(type = \"seq\", palette = \"Blues\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nGiven the condition percentages in Baltimore County & City and Harford County, it’s no surprise that their bridges are older than in other counties.\n\n\nshow bar chart code\n## median age of bridges by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medage = median(age)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medage)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medage, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median bridge age by county\" , \n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt’s somewhat reassuring then that Baltimore City bridges at least have less time in months since last inspection than do the counties.\n\n\nshow bar chart code\n## median months since last inspection by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medinsp = median(inspect_months)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medinsp)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medinsp, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median months since last inspection, by county\",\n       subtitle = \"as of March 2020\",\n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt might be the outliers pulling the smoothing line straight, but there doesn’t seem to be too much of a relationship between age and time since last inspection.\n\n\nshow scatterplot code\n## age by months since last inspection\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\n\nAnd in fact, removing the outliers shows a slight relationship; the older bridges do seem to get inspected more frequently. In terms of a better visualization, looking at this again, I wonder if some jittering or another type of plot might have been more visually appealing, givne the clustering of most recent inspections.\n\n# same code as above but with outliers removed\ntt_mdbrdf %>%\n  filter(age <150, inspect_months <60) %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(title = \"Months since inspection, outliers removed\", \n       x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\nNot sure if scatter-plot with colors by county is best way to go for this idea. Maybe a tree map?\n\n\nshow scatterplot code\n# same but colors by county\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  labs(title = \"Months since last inspection (from current date)\", \n       x = \"Months since last inspection (from current date)\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.8, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nFunky distributions here…Anne Arundel & Baltimore City have the highest median daily riders, but Howard County’s upper quartile is way out there.\nshowing the code here to illustrate how I like to run the mean & interquartiles in the same code as rendering the plot.\n\n# median & interquartiles of daily riders of bridges by county -\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medtraf = median(avg_daily_traffic),\n            lq = quantile(avg_daily_traffic, 0.25),\n            uq = quantile(avg_daily_traffic, 0.75)) %>%\n  ungroup() %>%\n  ggplot(aes(county, medtraf)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"navy\") +\n  geom_point(size = 3, color = \"orange\", alpha = .8) +\n  geom_text(aes(label = comma(medtraf, digits = 0)), \n            size = 4, color = \"orange\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = comma(uq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = comma(lq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Median & interquartile ranges of average daily riders per bridge, by county\" , \n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nAs with the other scatterplot with colors for county, might need a different way to see relationship between bridge age and daily traffic by county.\n\n\nshow scatterplot code\n## age by avg daily riders by county\ntt_mdbrdf %>%\n  ggplot(aes(avg_daily_traffic, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  scale_x_continuous(labels = comma) +\n  labs(title = \"Average daily traffic per bridge, by county\" , \n        x = \"Average daily traffic\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.75, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nCover image for post (CC BY-SA 4.0) from Wikipedia\nThis post was last updated on 2023-05-14"
  },
  {
    "objectID": "blog-migration-and-resurrection/index.html",
    "href": "blog-migration-and-resurrection/index.html",
    "title": "…and we’re back. Migration and resurrection",
    "section": "",
    "text": "Lots of things happening since last post in February 2021. The blog was set up during the early stages of COVID lockdown when all of a sudden I had more time on my hands.\nBut since that last post my wife and I made the decision to move to Europe in early 2022 and started preparations for that. The world started to repoen a bit in late summer 2021, and then my wife broke her ankle. So all of a sudden I was doing lots of home health care & keeping the house running on top of work. The wasn’t much brain space for independent data projects & keeping the blog running.\nThen in April 2022 we moved. The first stop was Lyon France\n\n\n\n\n\nwhere I earned a CELTA via the ELT Hub.\nI had expected teaching gigs to trickle in and I’d do independent data work while also spending time learning French and exploring Lyon and more of France. But the gigs came in abundance, and they were all new preps. So free time was at a minimum.\nThen in December 2022 we moved again. A job at CIEE in Denmark\n\n\n\n\n\ncame up and we accelerated the ultimate end goal of moving here - back to where I was born and still have lots of family and friends. Now that we’re settled in our first non-sublet in almost a year and I’ve gotten settled at work, there’s finally some time to get my head back into data work. I was partly inspired by serving as a mentor in the DDSA mentoring program. I also started using r at work to automate a few mundane tasks and to do a little analysis on our students.\nI’ll write about the migration process in a bit more detail in another post, but the tl/dr as to why is because in trying to do a simple update to my old main picture (me in a mask, early in lockdown) the build was failing in Netlify. So rather than try and diagnose the Hugo/Wowchemy issues (which turned out to be the Ubuntu deploy image…an easy reset), I saw that Posit’s new Quarto platform is very robust and a bit easier to publish with, so I decided to just rebuild the entire site.\nTo read about the migration details, go here. All of the old posts are up, time-stamped with the original posting dates. And hopefully soon, new posts as I work through a fairly long list of project ideas."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "© Copyright Gregers Kjerulf Dubrow\nThis is my personal website. All views are mine, and nothing I post here should be taken as an endorsement by my employer or any organizations of which I am a member.\nContent on this site is provided under a Creative Commons (CC-BY) 4.0 license. You may reuse this content as long as you indicate cite me as the author and provide a link back to the original material. Source code of the site is provided under the MIT license and may be reused without restriction."
  },
  {
    "objectID": "posts/tidy-tuesday-feb-02-2021-hbcu/index.html",
    "href": "posts/tidy-tuesday-feb-02-2021-hbcu/index.html",
    "title": "Tidy Tuesday, February 2, 2021 - HBCU Enrollment",
    "section": "",
    "text": "When the the Tidy Tuesday HBCU dataset was posted I got excited because here was something right in my scope of knowledge; enrollment in higher education. It was a great first Tidy Tuesday set for Black History Month. To keep this from being a journal-length piece, I won’t put on my history of higher ed teacher hat & get into how HBCUs came to be. Here’s a good overview.\nOne of my strengths as a data analyst is a brain that’s constantly asking questions of a dataset:\n* what’s in there?\n* what relationships exist between variables?\n* what I can add to the dataset to glean more insight?\nFor Tidy Tuesday though that slows things down – it’s mostly meant to be a quick turnaround thing where you share results to Twitter. So my deep-dive habits mean I’m usually a bit behind getting Tidy Tuesday analysis done the week the data are posted. My goal for this analysis is to show:\n* changes over time in the racial and gender make-up of students at HBCUs\n* changes over time in overall enrollment at HBCUs by sector (public 4yr, private 4y, etc), relative to non-HBCUs\n* changes over time in tuition & fees by sector, HBCUs vs non-HBCUs\nThe Tidy Tuesday data has some of what I need - it comes from Data.World via IPEDS (essentially the US Department of Education’s higher ed data library) . I’m supplementing it with data from the Delta Cost Project, which aggrgated years worth of data from IPEDS.\nSo let’s dive into the data.\nFirst we load the packages we’ll be using…\n\n# load packages\nlibrary(tidytuesdayR) # load the tidy tuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot\nlibrary(ggrepel) # helper functions for ggplot\n\n…then lets make a couple of things we’ll use a lot going forward.\n\n# create notin operator to help with cleaning & analysis\n`%notin%` <- negate(`%in%`)\n\n# function for year-over-year percent changes\npctchange <- function(x) {\n  (x - lag(x)) / lag(x)\n}\n\nNow we’ll load the sets from TidyTuesday and then the Delta Cost Project files.\nI’ll load the Tidy Tuesday data objects via the tidytuesdayR package and clean up the data. This week’s data came in a few separate files. For my analysis I need a dataframe of HBCU enrollments with Black and non-Black students. So I need to combine the two HBCU enrollment files (one each for Black students and all students) into one and subtract Black from All to get non-Black. I’ll show code for loading the Black student set in, cleaning it up a bit, and skip to the subtraction & joins. For the full code experience, head to my Tidy Tuesday repo\n\n\nshow tt_hbcu_black data load & cleaning code\ntt_hbcu_load <- tt_load(\"2021-02-02\")\n\ntt_hbcu_black <- as_tibble(tt_hbcu_load$hbcu_black) %>%\n  clean_names() %>%\n  mutate(year = as.character(year)) %>%\n  mutate(ethnicity = \"Black\") %>%\n  select(year, ethnicity, enroll_n = total_enrollment, women = females, men = males, \n         four_year_all = x4_year, two_year_all = x2_year,\n         total_public, four_year_pub = x4_year_public, two_year_pub = x2_year_public, \n         total_private, four_year_pri = x4_year_private, two_year_pri = x2_year_private) \n\n\n\n\nshow tt_hbcu_all data load & cleaning code\ntt_hbcu_all <- as_tibble(tt_hbcu_load$hbcu_all) %>%\n  clean_names() %>%\n  mutate(ethnicity = \"All\") %>%\n  mutate(year = as.character(year)) %>%\n  select(year, ethnicity, enroll_n = total_enrollment, women = females, men = males, \n         four_year_all = x4_year, two_year_all = x2_year,\n         total_public, four_year_pub = x4_year_public, two_year_pub = x2_year_public, \n         total_private, four_year_pri = x4_year_private, two_year_pri = x2_year_private)  \n\n\nLet’s join tt_hbcu_black with tt_hbcu_all and create the non-Black ethnicity group. There is probably a better way to do the subtractions programmatically with a function but my r skills aren’t there yet. The final dataframes I’ll use for plots are tt_hbcu_enr_eth_sex & tt_hbcu_enr_eth_sect. Because of the way the source data was arrayed I could do ethnicty by gender and ethnicity by sector but not all three vectors. So, two sets…\n\n\nshow dataframe join code\ntt_hbcu_notblack = as.data.frame(tt_hbcu_all) %>%\n  select(-ethnicity) %>%\n    # renaming multiple columns in one line of code!\n  rename_with(~ paste(.x, \"all\", sep = \"_\"), .cols = (2:12)) %>%\n  bind_cols(tt_hbcu_black) %>%\n  select(-year...13, -ethnicity) %>%\n  rename(year = year...1) %>%\n  # the subtractions by column to get non-black. again, might be a better way to do it w/ a function.\n  mutate(enroll_n_nb = enroll_n_all - enroll_n) %>%\n  mutate(women_nb = women_all - women) %>%\n  mutate(men_nb = men_all - men) %>%\n  mutate(four_year_all_nb = four_year_all_all - four_year_all) %>%\n  mutate(two_year_all_nb = two_year_all_all - two_year_all) %>%\n  mutate(total_public_nb = total_public_all - total_public) %>%\n  mutate(four_year_pub_nb = four_year_pub_all - four_year_pub) %>%\n  mutate(two_year_pub_nb = two_year_pub_all - two_year_pub) %>%\n  mutate(total_private_nb = total_private_all - total_private) %>%\n  mutate(four_year_pri_nb = four_year_pri_all - four_year_pri) %>%\n  mutate(two_year_pri_nb = two_year_pri_all - two_year_pri) %>%\n  mutate(ethnicity = \"Not Black\") %>%\n  select(year, ethnicity, enroll_n_nb:two_year_pri_nb) %>%\n  rename_with(~ str_remove(.x, \"_nb\"), .cols = (3:13))\n\n# create final dataframes, turn wide to long\n# note sex pct is by eth group\ntt_hbcu_enr_eth_sex = as.data.frame(rbind(tt_hbcu_all, tt_hbcu_black)) %>%\n  rbind(tt_hbcu_notblack) %>%\n  select(-four_year_all:-two_year_pri) %>%\n  arrange(year, ethnicity) %>% \n  group_by(year) %>%\n   mutate(enroll_eth_pct = enroll_n / first(enroll_n)) %>%\n  ungroup() %>%\n  pivot_longer(cols = women:men,\n               names_to = \"sex\",\n               values_to = \"sex_n\") %>%\n  arrange(year, ethnicity) %>%\n  group_by(year, ethnicity) %>%\n  mutate(enroll_sex_pct = sex_n / sum(sex_n)) %>%\n  ungroup() %>%\n  select(year, ethnicity, enroll_n, enroll_eth_pct, sex, sex_n, enroll_sex_pct) %>%\n  arrange(year, ethnicity, sex)\n\n# note pct_sect_eth is by eth group by year, pct_eth_sect is pct eth w/in sector\ntt_hbcu_enr_eth_sect = rbind(tt_hbcu_all, tt_hbcu_black) %>%\n  rbind(tt_hbcu_notblack) %>%\n  select(-women, -men) %>%\n  arrange(year, ethnicity) %>%\n  pivot_longer(cols = four_year_all:two_year_pri,\n               names_to = \"sector\", \n               values_to = \"sector_n\") %>%\n  arrange(year, ethnicity) %>%\n  mutate(pct_sect_eth = sector_n / enroll_n) %>%\n  arrange(year, sector) %>%\n  group_by(year, sector) %>%\n  mutate(pct_eth_sect = sector_n / (sum(sector_n) /2)) %>%\n  ungroup()\n\n\nOk, the Tidy Tuesday data is sorted, so let’s use the haven package load in the Delta Cost Project (DCP) data they provided as SAS files.\nThey’ve split their data into two files, one from 1987 to 1999 and one from 2000 to 2015. There’s a ton of data in the set, but all I want for this analysis is enrollments and tuition - I’ll pull those fields from each set, then rbind together into a dataframe called tuitenr_8715_agg for year-over-year by-sector analysis. Unfortunately while DCP has enrollment by ethnicity, they only do it for total enrollment - grad and undergrad combined - so I can’t do some of the HBCU vs non-HBCU comparisons. There are other sources which I’ll describe at the end of the post.\nOk, loading the 2000 to 2015 data…\n\n\nshow DCP 2000 - 2015 load code\ndelta0015all <- (haven::read_sas(\"~/Data/ipeds/delta_public_release_00_15.sas7bdat\", NULL))\ndelta0015_tuitenr <- delta0015all %>%\n  filter(between(sector_revised, 1, 6)) %>%\n  mutate(sector_desc = case_when(sector_revised == 1 ~  \"Public 4yr\",\n                                 sector_revised == 2    ~ \"Private nonprofit 4yr\",\n                                 sector_revised == 3    ~ \"Private for-profit 4yr\",\n                                 sector_revised == 4    ~ \"Public 2yr\",\n                                 sector_revised == 5    ~ \"Private nonprofit 2yr\",\n                                 sector_revised == 6    ~ \"Private for-profit 2yr\")) %>%\n # mutate(total_undergraduates = ifelse(is.na(total_undergraduates), 0, total_undergraduates)) %>%\n  select(unitid, instname, sector_revised, sector_desc, hbcu,\n         year = academicyear, tuition_fee_ug_in = tuitionfee02_tf, \n         tuition_fee_ug_oos = tuitionfee03_tf, \n         total_undergraduates)\n\n\nNow let’s load the 1987 to 1999 data…\n\n\nshow DCP 1987 - 1999 load code\ndelta8799all <- (haven::read_sas(\"~/Data/ipeds/delta_public_release_87_99.sas7bdat\", NULL))\n\ndelta8799_tuitenr <- as_tibble(delta8799all) %>%\n  filter(between(sector_revised, 1, 6)) %>%\n  mutate(sector_desc = case_when(sector_revised == 1 ~  \"Public 4yr\",\n                                 sector_revised == 2    ~ \"Private nonprofit 4yr\",\n                                 sector_revised == 3    ~ \"Private for-profit 4yr\",\n                                 sector_revised == 4    ~ \"Public 2yr\",\n                                 sector_revised == 5    ~ \"Private nonprofit 2yr\",\n                                 sector_revised == 6    ~ \"Private for-profit 2yr\")) %>%\n # mutate(total_undergraduates = ifelse(is.na(total_undergraduates), 0, total_undergraduates)) %>%\n  select(unitid, instname, sector_revised, sector_desc, hbcu,\n         year = academicyear, tuition_fee_ug_in = tuitionfee02_tf, \n         tuition_fee_ug_oos = tuitionfee03_tf, \n         total_undergraduates)\n\n\n…and let’s join and munge the DCP files…\n\n\nshow DCP join & clean code\ntuitenr_8715 <- rbind(delta0015_tuitenr, delta8799_tuitenr)\n\ntuitenr_8715_agg <-\n  tuitenr_8715 %>%\n  # filter(!sector_desc == \"Private for-profit 2yr\" &\n  #          !sector_desc == \"Private for-profit 4yr\") %>%\n#  filter(!is.na(tuition_fee_ug_in)) %>%\n  group_by(year, sector_desc, hbcu) %>%\n  summarise(enr_tot = sum(total_undergraduates, na.rm = TRUE),\n            mean_tuit_in = mean(tuition_fee_ug_in, na.rm = TRUE),\n            mean_tuit_oos = mean(tuition_fee_ug_oos, na.rm = TRUE)\n            ) %>%\n  mutate(level = ifelse(str_detect(sector_desc, \"2yr\"), \"2yr\", \"4yr\")) %>%\n#  summarise(mean_tuit_in = mean(tuition_fee_ug_in, na.rm = T)) %>%\n  ungroup() %>%\n  mutate(hbcu_f = ifelse(hbcu == 1, \"HBCU\", \"Not HBCU\")) %>%\n  mutate(hbcu_f = factor(hbcu_f, levels = c(\"HBCU\", \"Not HBCU\"))) \n\n\nBecause I’ll be doing some year-over-year percent change analysis, I’ll create a separate dataframe from tuitenr_8715_agg (the DCP data) so I can compare HBCU to non-HBCU changes by year.\nRemember the pctchange function I created when I loaded the packages? Here it’s put to good use. I’ll only show code for one of the frames - I created frames for each sector, HBCU and non-HBCU. The final data set will rbind 10 frames into tuit_8715_pctchgh. BTW, this is another example of when I wish I were a bit better at purrr/mapping and functions so I could have done it with less code.\nSince the data provided starts in 1987, I’ll filter that year out of the dataframes.\n\ntuit_8715_pctchgh1 <- tuitenr_8715_agg %>%\n  filter(hbcu == 1, sector_desc == \"Public 4yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\n\n\nshow the remaining sector files code\ntuit_8715_pctchgh2 <- tuitenr_8715_agg %>%\n  filter(hbcu == 1, sector_desc == \"Private nonprofit 4yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh3 <- tuitenr_8715_agg %>%\n  filter(hbcu == 1, sector_desc == \"Public 2yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh4 <- tuitenr_8715_agg %>%\n  filter(hbcu == 1, sector_desc == \"Private nonprofit 2yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh5 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Public 4yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh6 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Private nonprofit 4yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh7 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Public 2yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh8 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Private nonprofit 2yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh9 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Private for-profit 4yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh10 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Private for-profit 2yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh <- rbind(tuit_8715_pctchgh1, tuit_8715_pctchgh2) %>%\n  rbind(tuit_8715_pctchgh3) %>%\n  rbind(tuit_8715_pctchgh4) %>%\n  rbind(tuit_8715_pctchgh5) %>%\n  rbind(tuit_8715_pctchgh6) %>%\n  rbind(tuit_8715_pctchgh7) %>%\n  rbind(tuit_8715_pctchgh8) %>%\n  rbind(tuit_8715_pctchgh9) %>%\n  rbind(tuit_8715_pctchgh10) \n\n\nOk, the data is in place, let’s see what it tells us! I’ll fold the code for the more basic charts, click on the arrow to see it. You’ll note that this analysis is mostly line graphs. That’a because I’m mostly doing trend analysis, and I like line graphs for time series.\nFirst up we’ll use the Tidy Tuesday data to look at HBCU enrollment by ethnicity (Black and non-Black) from 1990 to 2015. What do we see? Overall, periods of increase and decrease in total enrollment. But interestingly we see that non-Black enrollment kept increasing, even as Black enrollment started to drop. Of course Black students make up a vast majority of HBCU enrollment, so their trends drive overall numbers. It is worth noting though that the changes started in the 2010s, coinciding with changes in how NCES counted ethncity. It’s beyond the scope of this quick (ha) analysis, but worth digging deeper to see the effect of changes on the ethnic mix of HBCUs.\n\n\nshow the enrollment by ethnicity line charts code\n## tt hbcu data, black men v women over time\ntt_hbcu_enr_eth_sex %>%\n  filter(year > 1989) %>%\n  ggplot(aes(year, enroll_n, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format(), breaks = scales::pretty_breaks()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  facet_grid(ethnicity ~ ., scales = \"free_y\") +\n  labs(title = \"Black Enrollment at HBCUs Rising & Falling Since 1990\",\n       subtitle = \"Non-Black enrollment slowly & streadily increasing\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Total UG Enrollment\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity line charts code\n\ntt_hbcu_enr_eth_sex %>%\n  filter(year > 1989) %>%\n  filter(ethnicity == \"Black\") %>%\n  distinct(year, ethnicity, .keep_all = T) %>%\n  ggplot(aes(year, enroll_eth_pct, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1.25) +\n  scale_y_continuous(limits = c(.5, 1), \n                     labels = scales::percent_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Slight Decrease Over Time in Percentage of Black Students Enrolled at HBCUs\",\n       subtitle = \"Might be due to changes in how ethnicity is coded by US Dept of Ed\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Pct Black students\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), \n        plot.title = element_text(size = 11),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nNext let’s look at enrollment by ethnicity & sex. HBCU enrollment mirrors that of higher ed enrollment in general - since the early 1980s women outnumber men. There are differences by sector (4-year, 2-year, public & private) and field of study but the oveall trend has been consistent. We do see that for non-Black students at HBCUs the percentage of women is a bit less than that of Black students.\n\n\nshow the enrollment by ethnicity & sex line charts code\ntt_hbcu_enr_eth_sex %>%\n  filter(year > 1989) %>%\n  ggplot(aes(year, sex_n, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  # scale_y_continuous(limits = c(0, 250000),\n  #                    breaks = c(0, 50000, 100000, 150000, 200000, 250000),\n  #                    labels = scales::comma_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Black Women Enroll in Higher Numbers Than Men at HBCUs\",\n       subtitle = \"Same Pattern as Overall Undergrad Enrollments in the US\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Total UG Enrollment\") +\n  facet_grid(ethnicity ~ sex, scales = \"free_y\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity & sex line charts code\n\n# sex - women as pct\ntt_hbcu_enr_eth_sex %>%\n  filter(year > 1989) %>%\n  filter(sex == \"women\") %>%\n  ggplot(aes(year, enroll_sex_pct, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(limits = c(.5, .8), \n                     labels = scales::percent_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Women a Greater Percentage of those Enrolled at HBCUs\",\n       subtitle = \"Slightly higher among Black students than not Black\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Pct Women enrolled\") +\n  facet_grid(ethnicity ~ ., scales = \"free_y\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nThe next thing I want to look at is enrollment by sector for HBCUs. In this case we think of sectors along two axes - public and private, and 2-year (community/junior colleges) vs 4-year (baccalaureate granting). What these first two sector charts show us is that most HBCU enrollment is in publics and in 4-year schools. Given the land-grant history of most HBCUs, that makes sense.\nI am leaving in the code for the first chart to highlight how one can a) have two different colored lines within the same chart in a faceted visualization (the scale_color_manual call) and b) use the amazing ggtext package to get two different colors of text in the subtitle line, so it does the work of a legend. The <span style = </span> and the element_markdown call in the theme specification is how you make it happen.\nThe third chart takes a slightly different view of the data, looking at percent of enrollment by sector within each of the ethnic categories we have in the data, Black and not-Black. What the first two charts showed is confirmed here - that Black students in HBCUs are mostly in public 4-years, and that trend has held steady since at least 1990.\n\ntt_hbcu_enr_eth_sect %>%\n  filter(year > 1989) %>%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %>%\n  filter(sector %in% c(\"total_private\", \"total_public\")) %>%\n  ggplot(aes(year, pct_sect_eth, group = sector, color = sector)) +\n  geom_line(size = 1) +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"Public HBCUs Enroll Greater Pct of Students \",\n       subtitle = \"Greater Pct Black students in Private HBCU than non-Black students; \n       <span style = 'color:#56B4E9;'>Blue = Public</span> \n       <span style = 'color:#E69F00;'>Orange = Private</span>\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by Publc & Private HBCUs\") +\n  facet_grid(ethnicity ~ .) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_markdown(size = 8, face = \"italic\"),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\n\nshow the enrollment by ethnicity & sector charts code\ntt_hbcu_enr_eth_sect %>%\n  filter(year > 1989) %>%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %>%\n  filter(sector %in% c(\"four_year_all\", \"two_year_all\")) %>%\n  ggplot(aes(year, pct_sect_eth, group = sector, color = sector)) +\n  geom_line(size = 1) +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"4-year HBCUs Enroll ~ 90% of all Students\",\n       subtitle = \"Black students more likely to be in 4-year HBCU than non-Black students; \n       <span style = 'color:#56B4E9;'>Blue = 4-year</span> \n       <span style = 'color:#E69F00;'>Orange = 2-year</span>\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by 4yr & 2yr HBCUs\") +\n  facet_grid(ethnicity ~ .) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_markdown(size = 8, face = \"italic\"),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity & sector charts code\n\ntt_hbcu_enr_eth_sect %>%\n  filter(year > 1989) %>%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %>%\n  filter(sector %in% c(\"four_year_pub\", \"four_year_pri\", \"two_year_pri\", \"two_year_pub\")) %>%\n  mutate(sector = str_replace(sector, \"four_year_pub\", \"4 Year Public\")) %>%\n  mutate(sector = str_replace(sector, \"two_year_pub\", \"2 Year Public\")) %>%\n  mutate(sector = str_replace(sector, \"four_year_pri\", \"4 Year Private\")) %>%\n  mutate(sector = str_replace(sector, \"two_year_pri\", \"2 Year Private\")) %>%\n#  ggplot(aes(year, enroll_sect_pct, group = sector, color = sector)) +\n  ggplot(aes(year, pct_sect_eth, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"Black Students most likely to be in 4-year HBCUs\",\n       subtitle = \"Non-black students moving to 2-year HBCUs from 4-yr; \n       <span style = 'color:#E69F00;'>Percents sum to 100% across ethnic groups</span>\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by Sector\") +\n  facet_grid(ethnicity ~ sector) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        axis.text.x = element_text(size = 6),\n        plot.subtitle = element_markdown(face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nSo we’ve done what we can with the Tidy Tuesday data, time to compare HBCUs to non-HBCUs using the Delta Cost Project data. We’ll stay with sector analysis and look at enrollments by sector. For the chart output I wanted, I used Thomas Lin Pederson’s patchwork package. I’ll fold the code that creates the component charts and just show the patchwork call.\n\n\nshow the enrollment by sector charts code\nenr19902015hbcu <-\ntuitenr_8715_agg %>%\n  filter(hbcu == 1) %>% \n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(title = \"HBCU Enrollment Across Sector 1990-2015\",\n       #caption = \"Source: Delta Cost Project\",\n    x = \"\", y = \"Total UG enrollment\") +\n  facet_wrap(~ sector_desc, scales = \"free\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n                axis.text.y = element_text(size = 7), \n        #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\nenr19902015nothbcu <-\n  tuitenr_8715_agg %>%\n  filter(hbcu == 2) %>%\n#  filter(level == \"2yr\") %>% \n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#56B4E9\", group = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(title = \"non-HBCU Enrollment Across Sector 1990-2015\",\n       caption = \"Source: Delta Cost Project\",\n       x = \"\", y = \"Total UG enrollment\") +\n  facet_wrap(~ sector_desc, scales = \"free\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#56B4E9\"),\n                axis.text.y = element_text(size = 7), \n        #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\nenr19902015all <- enr19902015hbcu / enr19902015nothbcu\nenr19902015all\n\n\n\n\nFor a change of pace let’s see a slope graph of percent change in tuition by sector from 1990 to 2015. I found a helpful code-thru of a similar-ish slope graph here and used that as my guide. I used patchwork for this too, and because in this instance I used patchwork for title and other adornments, I’ll show code for all steps.\nTwo observations stand out - first, HBCUs are less expensive by sector than non-HBCUs. But for both HBCUs and non-HBCUs, the price gap between private 4-year schools and the other sectors widened during this time period.\nBut these are sticker prices and don’t include institutional aid, or “tuition discount”. If a college offers you a $3000 institutional grant (not a federal or state grant or loan), the money is essentially foregone revenue for the school, or a discount to the stated tuition price.\n\n\nshow the slope graph code\ntuitsect_hbcu <-\n    tuitenr_8715_agg %>%\n    filter(hbcu == 1 & (year == 1990 | year == 2015)) %>%\n    #  filter(level == \"2yr\") %>% \n    ggplot(aes(year, mean_tuit_in, group = sector_desc)) +\n    geom_line(aes(color = sector_desc)) +\n    geom_point(aes(color = sector_desc)) +\n    scale_color_manual(values = c(\"Public 4yr\" = \"#999999\", \n                                                                \"Public 2yr\" = \"#E69F00\", \n                                                                \"Private nonprofit 2yr\" = \"#56B4E9\", \n                                                                \"Private nonprofit 4yr\" = \"#009E73\")) +\n    geom_text_repel(data = tuitenr_8715_agg %>%\n                                        filter(hbcu == 1 & year == 2015),\n                                    aes(label = sector_desc), hjust = \"left\", nudge_x = .5,\n                                    direction = \"y\", size = 4) +\n    annotate(\"text\", x = 1991, y = 12000, label = \"HBCUs\", size = 5, fontface = \"italic\") +\n    scale_x_continuous(breaks = c(1990, 2015)) +\n    scale_y_continuous(labels = scales::dollar_format(),\n                                         limits = c(0, 15000),\n                                         breaks = c(0, 2500, 5000, 7500, 10000, 12500, 15000)) +\n    labs(x = \"\", y = \"Average Tuition & Fees\") +\n    theme_light() +\n    theme(panel.grid.major = element_blank(), panel.grid.major.x = element_blank(), \n                panel.grid.minor = element_blank(), panel.grid.minor.y = element_blank(),\n                panel.background = element_blank(), \n                strip.background = element_rect(fill = \"#56B4E9\"),\n                panel.border = element_blank(), legend.position = \"none\",  \n                axis.text.y = element_text(size = 8), \n                axis.ticks = element_blank(), axis.text.x = element_blank(), \n                axis.title.y = element_text(hjust = -.07),\n                #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n                plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\ntuitsect_nonhbcu <-\n    tuitenr_8715_agg %>%\n    filter(hbcu == 2 & (year == 1990 | year == 2015)) %>%\n    #  filter(level == \"2yr\") %>% \n    ggplot(aes(year, mean_tuit_in, group = sector_desc)) +\n    geom_line(aes(color = sector_desc)) +\n    geom_point(aes(color = sector_desc)) +\n    scale_color_manual(values = c(\"Public 4yr\" = \"#999999\", \n                                                                \"Public 2yr\" = \"#E69F00\", \n                                                                \"Private nonprofit 2yr\" = \"#56B4E9\", \n                                                                \"Private nonprofit 4yr\" = \"#009E73\",\n                                                                \"Private for-profit 2yr\" = \"#0072B2\", \n                                                                \"Private for-profit 4yr\" = \"#CC79A7\")) +\n    geom_text_repel(data = tuitenr_8715_agg %>%\n                                        filter(hbcu == 2 & year == 2015),\n                                    aes(label = sector_desc), hjust = \"left\", nudge_x = .5, \n                                    direction = \"y\", size = 3) +\n    annotate(\"text\", x = 1992, y = 20000, label = \"not HBCUs\", size = 5, fontface = \"italic\") +\n    scale_x_continuous(breaks = c(1990, 2015)) +\n    scale_y_continuous(labels = scales::dollar_format(),\n                                         limits = c(0, 25200),\n                                         breaks = c(0, 5000, 10000, 15000, 20000, 25000)) +\n    labs(x = \"\", y = \"\") +\n    theme_light() +\n    theme(panel.grid.major = element_blank(), panel.grid.major.x = element_blank(), \n                panel.grid.minor = element_blank(), panel.grid.minor.y = element_blank(),\n                panel.background = element_blank(), \n                strip.background = element_rect(fill = \"#56B4E9\"),\n                panel.border     = element_blank(),  axis.text.y = element_text(size = 8), \n                legend.position = \"none\", axis.ticks       = element_blank(),\n                #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n                plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\ntuitsec <- tuitsect_hbcu / tuitsect_nonhbcu  + plot_annotation(\n    title = 'Tuition & Fees at HBCUs Lower by Sector than non-HBCUs',\n    subtitle = 'Private non-profit 4-yr tuition increased more than other sectors',\n    caption = \"Source: Delta Cost Project\", \n    theme = theme(plot.subtitle = element_text(face = \"italic\", size = 9)))\ntuitsec\n\n\n\n\nFinally, let’s check out some year-over-year percent changes in enrollment & tuition, using the line graphs I love so much. Code is basic ggplot, so not worth showing here. If there’s anything of interest it was in adding a geom_hline call to add the light gray reference line at 0%.\nAs we’d seen in eariler charts, enrollment at HBCUs ping-ponged up and down. There was a bit less volatility in non-HBCUS, and in general enrollments didn’t decrease from 2000 onward.\nAt both HBCUs and non-HBCUs, public & private 4-years raised tuition & fees every year. The publics had a couple of instances of sharper increases, likely in response to recession-hit state budgets. The 2-year schools, especially 2-year HBCUs, had wider up & down swings in tuition.\n\n\nshow the faceted line graph code\ntuit_8715_pctchgh %>%\n  mutate(hbcu_f = case_when(hbcu == 1 ~ \"HBCU\", TRUE ~ \"Not HBCU\")) %>%\n  #  filter(hbcu == 1 & year >= 1990) %>% \n  filter(year >= 1990) %>%\n  filter(sector_desc %in% c(\"Public 4yr\", \"Private nonprofit 4yr\", \"Public 2yr\")) %>%\n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  geom_hline(yintercept=0, color = \"gray\") +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(title = \"Year-over-year Percent Changes to UG Enrollment\",\n       subtitle = \"HBCU sectors had wider positive & negative swings\", \n       caption = \"Source: Delta Cost Project\", \n       x = \"\", y = \"Pct Chnage in-state tuition & fees\") +\n  facet_grid(hbcu_f ~ sector_desc) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the faceted line graph code\n\ntuit_8715_pctchgh %>%\n  mutate(hbcu_f = case_when(hbcu == 1 ~ \"HBCU\", TRUE ~ \"Not HBCU\")) %>%\n  #  filter(hbcu == 1 & year >= 1990) %>% \n  filter(year >= 1990) %>%\n  filter(sector_desc %in% c(\"Public 4yr\", \"Private nonprofit 4yr\", \"Public 2yr\")) %>%\n  ggplot(aes(year, mean_tuit_in)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  geom_hline(yintercept=0, color = \"gray\") +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(title = \"Year-over-year Percent Changes to Tuition & Fees\",\n       subtitle = \"More volatility in 2-year sector, steady increases in every sector\", \n       caption = \"Source: Delta Cost Project\", \n       x = \"\", y = \"Pct Chnage in-state tuition & fees\") +\n  facet_grid(hbcu_f ~ sector_desc) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nThis was designed to be broad-scope, more-or-less exploratory analysis. Just looking for general trends, what might be worth a deeper dive. Like with the percent changes in tuition or enrollment…what was happening in the years with the wider + or - swings? The 2001 recession? The 2009/10 economic collapse? Which specific schools had more success with enrollment? Which HBCUs are the most & least expensive?\nAnd what other questions one could answer with the data I used here? Well for one, the tuition discount rates by sectors and HBCU/not HBCU would be an interesting thing to look at. Did HBCUs need to more aggressively discount tution to meet enrollment targets? The Delta Cost Project set includes a tuition discount variable. There’s a regression analysis in there somewhere, as well as other predictive analysis.\nI’d also want to look at which ethnic groups made up the growth in non-Black enrollments starting in 2010. Or was it that the changes to how ethnicity was recorded meant that students who used to be classified as Black become Hispanic or multi-ethnic. I know from my time in undergrad admissions at UC Berkeley that this accounted for a decline in the federal count of African-American students. However, when we compared the numbers with the UC definitions, we didn’t see a decline.\nTo get those numbers I could download IPEDS data directly from NCES or use r tools like the Urban Insitute’s educationdata package which is an API wrapper to that scrapes NCES and other websites for data.\nThere is a ton of higher education data out there, and it’s never been easier to get at scale than now. Trust me, as someone who has done individual downloads, ordered CD-ROMs, even used the late-great WebCASPAR, if you have an education policy or outcomes related question, there is publicly available data for analysis.\nCover image for post from GWU CCAS Graduate Blog\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/tidy-tuesday-nov-11-2020-hiking-trails-in-wash-state/index.html",
    "href": "posts/tidy-tuesday-nov-11-2020-hiking-trails-in-wash-state/index.html",
    "title": "Tidy Tuesday, November 24, 2020 - Hiking Trails in WA State",
    "section": "",
    "text": "Trail-head lake in the Cascade Mountains.\n\n\n\nDiving deep into the Tidy Tuesday pool…\nEvery week I’m so impressed by the amount of beautiful, creative and informative analyses and data viz efforts that spring from #TidyTuesday, the weekly data project that evolved out of the R4DS Learning Community. And every week I say to myself, “I should really give it a go, grab the data and post some results”. Then life intervenes, work gets crazy, and before I know it…\nWell, around Thanksgiving, with some slack time and some inspiration, I finally got around to it. A resulting benefit was it got me motivated to finally get around to getting this blog set up. My github repo has a folder for any tidytuesday projects I get around to.\nHere’s the code and resulting charts & tables.\n\n# load packages\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\n\nFirst let’s read in the file using the tidytuesdayR package. We’ll also look at the raw data\n\ntt_watrail <- tt_load(\"2020-11-24\")\n#> \n#>  Downloading file 1 of 1: `hike_data.rds`\n\nglimpse(tt_watrail$hike_data)\n#> Rows: 1,958\n#> Columns: 8\n#> $ name        <chr> \"Lake Hills Greenbelt\", \"Snow Lake\", \"Skookum Flats\", \"Ten…\n#> $ location    <chr> \"Puget Sound and Islands -- Seattle-Tacoma Area\", \"Snoqual…\n#> $ length      <chr> \"2.3 miles, roundtrip\", \"7.2 miles, roundtrip\", \"7.8 miles…\n#> $ gain        <chr> \"50\", \"1800\", \"300\", \"1585\", \"500\", \"500\", \"425\", \"450\", \"…\n#> $ highpoint   <chr> \"330.0\", \"4400.0\", \"2550.0\", \"2370.0\", \"1000.0\", \"2200.0\",…\n#> $ rating      <chr> \"3.67\", \"4.16\", \"3.68\", \"3.92\", \"4.14\", \"3.14\", \"5.00\", \"2…\n#> $ features    <list> <\"Dogs allowed on leash\", \"Wildlife\", \"Good for kids\", \"L…\n#> $ description <chr> \"Hike through a pastoral area first settled and farmed in …\n\n\n\nThere are a few things we want to do with the data for the working dataframe:\n\ncreate columns for miles, direction, type from length\ncreate specific location columns frolm location\nchange rating, gain and highpoint to numeric\ncreate a rating group\nchange features to character vector, also unnest; makes the resulting df long. we’ll use distinct when we only need 1 obs per trail\n\n\ntt_watraildf <- tt_watrail$hike_data %>%\n  mutate(length_miles = parse_number(length)) %>%\n  mutate(across(gain:rating, as.numeric)) %>%\n  mutate(rating_grp = case_when(rating == 0 ~ \"0\",\n                                rating >0 & rating < 2 ~ \"1\",\n                                rating >=2 & rating < 3 ~ \"2\",\n                                rating >=3 & rating < 4 ~ \"3\",\n                                rating >=4 & rating < 5 ~ \"4\",\n                                rating == 5 ~ \"5\")) %>%\n  mutate(trail_type = case_when(grepl(\"roundtrip\", length) ~ \"Round trip\",\n                          grepl(\"one-way\", length) ~ \"One Way\",\n                          grepl(\"of trails\", length) ~ \"Trails\")) %>% \n  mutate(location_split = location) %>%\n  separate(location_split, c(\"location_region\",\"location_specific\"), sep = ' -- ') %>%\n  mutate(features = lapply(features, sort, na.last = TRUE)) %>%\n  mutate(feature_v = sapply(features,FUN = function(x) if (all(is.na(x))) NA else paste(x,collapse = \", \"))) %>%\n  mutate(feature_v = str_trim(feature_v)) %>%\n  mutate(features_unnest = features) %>%\n  unnest(cols = c(features_unnest), keep_empty = TRUE) %>% \n  mutate(feature_v = ifelse(is.na(feature_v), \"none\", feature_v)) %>%\n  mutate(features_unnest = ifelse(is.na(features_unnest), \"none\", features_unnest)) %>%\n  mutate(feature_init = case_when(features_unnest == \"Dogs allowed on leash\" ~ \"DA\",\n                                  features_unnest == \"Dogs not allowed\" ~ \"DN\",\n                                  features_unnest == \"Wildlife\" ~ \"Wl\",\n                                  features_unnest == \"Good for kids\" ~ \"GK\",\n                                  features_unnest == \"Lakes\" ~ \"Lk\",\n                                  features_unnest == \"Fall foliage\" ~ \"FF\",\n                                  features_unnest == \"Ridges/passes\" ~ \"RP\",\n                                  features_unnest == \"Established campsites\" ~ \"EC\",\n                                  features_unnest == \"Mountain views\" ~ \"MV\",\n                                  features_unnest == \"Old growth\" ~ \"OG\",\n                                  features_unnest == \"Waterfalls\" ~ \"Wf\",\n                                  features_unnest == \"Wildflowers/Meadows\" ~ \"WM\",\n                                  features_unnest == \"Rivers\" ~ \"Ri\",\n                                  features_unnest == \"Coast\" ~ \"Co\",\n                                  features_unnest == \"Summits\" ~ \"Su\")) %>%\n  mutate(feature_init = ifelse(is.na(feature_init), \"none\", feature_init)) %>%\n  mutate(feature_type = if_else(feature_init %in% c(\"DA\",\"DN\",\"GK\"), \"Companion\", \"Feature\")) %>%\n  mutate(feature_type = ifelse(feature_init == \"none\", \"none\", feature_type)) %>%\n  group_by(name) %>%\n  mutate(feature_n = n()) %>%\n  ungroup() %>%\n  mutate(feature_n = ifelse(feature_init == \"none\", 0, feature_n)) %>%\n  select(name, location_region, location_specific, trail_type, length_miles, \n         gain, highpoint, rating, rating_grp, features, feature_v, features_unnest, \n         feature_init, feature_type, feature_n, description, location, length)\n\n\n\nTo get a sense of what the data look like, I’ll run some historgrams and scatterplots to see how things cluster, if there are outliers or anything else especially noticable.\nUsing log10 for the length scale to even out the spread. The patchwork package stitches the plots together in a neat panel.\n\nhist_length <-\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles)) +\n  geom_histogram(alpha = 0.8) +\n  scale_x_log10() +\n  labs(x = \"Length (miles), log10\")\n\nhist_gain <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(gain)) +\n  geom_histogram(alpha = 0.8) +\n  scale_x_log10()\n\nhist_high <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(highpoint)) +\n  geom_histogram(alpha = 0.8) \n\nhist_rate <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(rating)) +\n  geom_histogram(alpha = 0.8) \n\n(hist_length | hist_gain) /\n  (hist_high | hist_rate)\n\n\n\n\nFor the scatterplots, I plotted length by gain, faceting by ratings groups and then by region. We do have to be careful with ratings, as they are user-generated and some trails have very few votes. Log10 used again for length.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles, gain)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"Length (miles) log10\", y = \"Total Gain\",\n       title = \"Length v Gain, by Rating Group\") +\n  facet_wrap(vars(rating_grp))\n\n\n\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles, gain)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"Length (miles) log 10\", y = \"Total Gain\",\n       title = \"Length v Gain, by Region\") +\n  facet_wrap(vars(location_region))\n\n\n\n\nThe outliers in terms of gain & length clustered in a few regions, so I wanted to see which they were. Not a surprise they clustered in the Cascades & Rainier.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  filter(gain > 15000) %>%\n  filter(length_miles > 90) %>%\n  select(location_region, name, length_miles, gain) %>%\n  arrange(name) \n#> # A tibble: 5 × 4\n#>   location_region      name                                   length_miles  gain\n#>   <chr>                <chr>                                         <dbl> <dbl>\n#> 1 Southwest Washington Pacific Crest Trail (PCT) Section H -…         148. 27996\n#> 2 South Cascades       Pacific Crest Trail (PCT) Section I -…          99  17771\n#> 3 Central Cascades     Pacific Crest Trail (PCT) Section K -…         117  26351\n#> 4 North Cascades       Pacific Northwest Trail - Pasayten Tr…         119  21071\n#> 5 Mount Rainier Area   Wonderland Trail                                93  22000\n\n\n\nNow that we see how the length, gain, highpoint & ratings spread out, I want build a table to see the averages by region.\nI’ve been wanting to take a deeper dive into gt & reactable. I’ve got some basic gt calls down, but for this excercise I wanted to learn how to conditionally format columns based on value. So inspired by Thomas Mock’s gt primer, a basic table with heatmap-like formatting for some columns. See his explainer for details on the code, and for more features than I’m including.\n\n# create by region averages df\nbyregion <-  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  group_by(location_region) %>%\n  summarise(n_region = n(),\n            avglength = mean(length_miles),\n            avgrating = mean(rating),\n            avggain = mean(gain),\n            avghigh = mean(highpoint),\n            minhigh = min(highpoint),\n            maxhigh = max(highpoint)) %>%\n  mutate_at(vars(avglength:avgrating), round, 2) %>%\n  mutate_at(vars(avggain:avghigh), round, 0) \n\nbyregion %>%\n  gt() %>%\n  fmt_number(columns = vars(avggain, avghigh, minhigh, maxhigh), decimals = 0, use_seps = TRUE) %>%\n  # sets the columns and palette to format cell color by value range\n  data_color(\n    columns = vars(avglength, avgrating, avggain, avghigh, minhigh, maxhigh),\n    colors = scales::col_numeric(\n      palette = c(\"#ffffff\", \"#f2fbd2\", \"#c9ecb4\", \"#93d3ab\", \"#35b0ab\"),\n      domain = NULL)) %>%\n#  tab_style calls add border boxes first to column labels, then body cells\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_column_labels(\n        columns = gt::everything()\n        ))) %>%\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_body(\n        rows = gt::everything()\n      ))) %>%\n    cols_align(columns = TRUE, align = \"center\") %>%\n  cols_align(columns = \"location_region\", align = \"left\") %>%\n  cols_width(vars(location_region) ~ px(150),\n             vars(n_region) ~ px(60),\n             starts_with(\"avg\") ~ px(80),\n             ends_with(\"high\") ~ px(90)\n             ) %>%\n  tab_header(title = \"Regional Averages\",\n             subtitle = md(\"_North Cascades have longest trails,\n                           all mountain areas have lots of gain and highest points_\")) %>%\n  cols_label(location_region = \"Region\", n_region = \"N\", avglength = \"Avg Length (miles)\",\n            avgrating = \"Avg Rating\", avggain = \"Avg Gain (ft)\",avghigh = \"Avg high point\",\n             minhigh = \"Lowest high point\", maxhigh = \"Max high point\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Regional Averages\n    \n    \n      North Cascades have longest trails,\nall mountain areas have lots of gain and highest points\n    \n    \n      Region\n      N\n      Avg Length (miles)\n      Avg Rating\n      Avg Gain (ft)\n      Avg high point\n      Lowest high point\n      Max high point\n    \n  \n  \n    Central Cascades\n219\n9.53\n3.04\n2,276\n4,752\n600\n9,511\n    Central Washington\n79\n5.71\n2.78\n823\n2,260\n240\n6,876\n    Eastern Washington\n142\n9.33\n2.15\n1,592\n4,410\n300\n7,310\n    Issaquah Alps\n76\n5.03\n2.53\n984\n1,518\n250\n3,004\n    Mount Rainier Area\n193\n8.19\n3.35\n1,881\n5,222\n800\n10,080\n    North Cascades\n292\n11.24\n3.08\n2,535\n5,111\n125\n9,200\n    Olympic Peninsula\n209\n8.13\n3.32\n1,572\n2,821\n20\n6,988\n    Puget Sound and Islands\n190\n4.25\n2.80\n452\n573\n10\n3,750\n    Snoqualmie Region\n216\n8.71\n3.15\n2,198\n4,467\n450\n9,416\n    South Cascades\n188\n8.44\n3.07\n1,641\n4,732\n922\n12,276\n    Southwest Washington\n120\n6.58\n2.69\n1,171\n1,774\n20\n7,800\n  \n  \n  \n\n\n\n\n\n\nNow let’s look at the effect of trail features on rating.\nFirst we’ll look at average rating by feature, then fit a model. First, a scatter-plot of number of features listed for a trail with user rating. Looks like at a certain point, it’s diminshing returns on trail features in terms of effect on rating.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(feature_n, rating)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"# of features on a trail\", y = \"User rating\",\n       title = \"Features and Rating by Trail Region\") +\n  facet_wrap(vars(location_region))\n\n\n\n\nHere’s a table similar to the one for averages by region. I used the unnested features, so trails will be represented more than once. Dog-free trails do get the highest ratings, but it’s likely because they also tend to have highest high points, so offer views, are challenging, and so get good ratings.\n\nbyfeature <- \ntt_watraildf %>%\n  group_by(features_unnest) %>%\n  summarise(n_feature = n(),\n            avgrating = mean(rating),\n            avglength = mean(length_miles),\n            avggain = mean(gain),\n            avghigh = mean(highpoint),\n            minhigh = min(highpoint),\n            maxhigh = max(highpoint)) %>%\n  mutate_at(vars(avglength:avgrating), round, 2) %>%\n  mutate_at(vars(avggain:avghigh), round, 0) %>%\n  arrange(desc(avgrating))\n\n## create table\nbyfeature %>%\n  gt() %>%\n  fmt_number(columns = vars(n_feature, avggain, avghigh, minhigh, maxhigh), decimals = 0, use_seps = TRUE) %>%\n  # sets the columns and palette to format cell color by value range\n  data_color(\n    columns = vars(avglength, avgrating, avggain, avghigh, minhigh, maxhigh),\n    colors = scales::col_numeric(\n      palette = c(\"#ffffff\", \"#f2fbd2\", \"#c9ecb4\", \"#93d3ab\", \"#35b0ab\"),\n      domain = NULL)) %>%\n  # tab_style calls add border boxes first to column labels, then body cells\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_column_labels(\n        columns = gt::everything()\n      ))) %>%\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_body(\n        rows = gt::everything()\n      ))) %>%\n  tab_header(title = \"Averages by Feature\",\n             subtitle = md(\"_Dog-free trails with waterfalls & high peaks earn high ratings_\")) %>%\n  cols_align(columns = TRUE, align = \"center\") %>%\n  cols_align(columns = \"features_unnest\", align = \"left\") %>%\n  cols_width(vars(features_unnest) ~ px(150),\n             vars(n_feature) ~ px(60),\n             starts_with(\"avg\") ~ px(80),\n             ends_with(\"high\") ~ px(90)\n             ) %>%\n  cols_label(features_unnest = \"Feature\", n_feature = \"N\", avglength = \"Avg Length (miles)\",\n             avgrating = \"Avg Rating\", avggain = \"Avg Gain (ft)\",avghigh = \"Avg high point\",\n             minhigh = \"Lowest high point\", maxhigh = \"Max high point\") \n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Averages by Feature\n    \n    \n      Dog-free trails with waterfalls & high peaks earn high ratings\n    \n    \n      Feature\n      N\n      Avg Rating\n      Avg Length (miles)\n      Avg Gain (ft)\n      Avg high point\n      Lowest high point\n      Max high point\n    \n  \n  \n    Dogs not allowed\n255\n3.52\n9.28\n1,921\n4,173\n10\n10,080\n    Waterfalls\n282\n3.46\n9.64\n1,938\n3,648\n150\n10,080\n    Established campsites\n396\n3.40\n12.80\n2,380\n4,487\n25\n12,276\n    Ridges/passes\n496\n3.25\n12.24\n2,864\n5,575\n400\n9,511\n    Lakes\n583\n3.19\n9.92\n1,988\n4,231\n20\n9,511\n    Old growth\n534\n3.16\n9.00\n1,746\n3,364\n25\n8,096\n    Mountain views\n1,175\n3.13\n9.72\n2,201\n4,621\n20\n12,276\n    Summits\n454\n3.11\n10.40\n2,854\n5,250\n200\n12,276\n    Wildflowers/Meadows\n952\n3.10\n9.26\n1,967\n4,243\n10\n9,511\n    Rivers\n547\n3.05\n9.76\n1,731\n3,205\n10\n12,276\n    Good for kids\n694\n3.01\n4.63\n569\n2,080\n10\n8,245\n    Wildlife\n747\n3.00\n8.84\n1,541\n3,241\n10\n10,080\n    Dogs allowed on leash\n1,045\n2.94\n7.04\n1,379\n3,183\n20\n12,276\n    Fall foliage\n508\n2.94\n8.28\n1,618\n3,334\n20\n9,249\n    Coast\n106\n2.89\n4.12\n351\n433\n10\n6,454\n    none\n68\n2.38\n7.26\n1,758\n3,849\n60\n8,970\n  \n  \n  \n\n\n\n\n\n\nAnd finally a quick model to see what might affect a trail rating.\nIt’s a simple linear model using length, gain, highpoint, & number of features to predict rating. The elevation of the highest point and number of features are both significant. I’d need to do more digging to see what the power of the estimate is on the rating. It’s also slightly counter-intuitive given that we saw in the charts that length, elevation and gain seem to positively affect rating. But then the model only accounts for 4% of varaince, so it’s not telling us much.\n\n# creat df with distinct observations for each trail \ntt_watraildf_dist <- tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) \n\nwtmodel1 <- lm(rating ~ length_miles + gain + highpoint + feature_n, data = tt_watraildf_dist)\nsummary(wtmodel1)\n#> \n#> Call:\n#> lm(formula = rating ~ length_miles + gain + highpoint + feature_n, \n#>     data = tt_watraildf_dist)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.6984 -0.3776  0.3716  0.9284  2.4565 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.205e+00  8.942e-02  24.663  < 2e-16 ***\n#> length_miles -6.565e-03  5.678e-03  -1.156    0.248    \n#> gain         -3.590e-05  3.100e-05  -1.158    0.247    \n#> highpoint     8.318e-05  1.742e-05   4.775 1.93e-06 ***\n#> feature_n     1.272e-01  1.484e-02   8.576  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.398 on 1919 degrees of freedom\n#> Multiple R-squared:  0.0488, Adjusted R-squared:  0.04682 \n#> F-statistic: 24.61 on 4 and 1919 DF,  p-value: < 2.2e-16\n\nThere’s plenty more to do with the set, and some responses I’ve seen on Twitter have been creative…network graphs, better models…but I was able to brush up on gt, learned how to unnest and keep obs where the list was empty. So a successful #tudytuesday.\nThis post was last updated on 2023-05-18"
  }
]