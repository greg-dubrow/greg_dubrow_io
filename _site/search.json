[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gregers kjerulf dubrow",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nMigration from Hugo to Quarto\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\nhowto\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\n…and we’re back. Migration and resurrection\n\n\n\n\n\n\n\npost\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nSad Songs & Pretty Charts - a Gosta Berling Music Data Visualization\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nmusic\n\n\nspotify\n\n\ngosta berling\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2021\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nInvited Talk at University of San Francisco, February 2020\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nhigher education\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2021\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, February 2, 2021 - HBCU Enrollment\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nhigher education\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2021\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, April 07, 2020 - Le Tour! (Stage 2, charts!)\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nsports\n\n\ncycling\n\n\ntour de france\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, April 07, 2020 - Le Tour! (Stage 1, cleaning the data)\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\nsports\n\n\ncycling\n\n\ntour de france\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, November 27, 2018 - Maryland Bridges\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday, November 24, 2020 - Hiking Trails in WA State\n\n\n\n\n\n\n\npost\n\n\ntidytuesday\n\n\nrstats\n\n\nggplot\n\n\ndataviz\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2020\n\n\ngregers kjerulf dubrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "gregers kjerulf dubrow",
    "section": "",
    "text": "I’m a data analyst/data scientist living in København Denmark, with experitise in higher education systems, policy, undergraduate admissions & enrollment management, and student success.\nI work in r for everything from data import & cleaning, to analysis, modeling & visualization. I have also used SAS and excel and have working knowledge of SQL and a passing familiarity with Tableau.\nI was born in Denmark and grew up in the US. After many years in San Francisco I lived in Lyon France from May to December 2022 , working as an ESL instructor after earning a CELTA via the ELT Hub.\nIn this space I’ll mostly be posting code-throughs and results from my personal data projects; music, football (the soccer kind), education, #tidytuesday data projects and other work. All views expressed here are my own.\nOver the years I’ve made some music with Gosta Berling, Slowness, Big Still and Idle Wilds. My photography is at Flickr."
  },
  {
    "objectID": "posts/tidy_tuesday_maryland_bridges/index.html",
    "href": "posts/tidy_tuesday_maryland_bridges/index.html",
    "title": "Tidy Tuesday, November 27, 2018 - Maryland Bridges",
    "section": "",
    "text": "This dataset was posted to the #TidyTuesday repo back in November 2018. I worked on it a bit then, but didn’t properly finish it until March of 2020. With the blog finally set up figured I might as well post it as an entry.\nUpdate for migration from Hugo to Quarto…now that code-fold is native to code chunks, I’ll sometimes use if for long bits of code. Just click the down arrow to show code\n\n# readin in data, create df for plots\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\nFirst let’s read in the file from the raw data file on github\n\ntt_balt_gh <-\nread_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-27/baltimore_bridges.csv\",\n         progress = show_progress())\n\n\n\nOrganize and clean the data for analysis\n\nclean up some date and name issues\na decade-built field, factored for sorting\ntime since last inspection\n\n\n\nshow data cleaning code\n# keeping the comparison date March 21 when I originally did analysis\ntoday <- as.Date(c(\"2020-03-21\"))\n#Sys.Date()\ntoday_yr <- as.numeric(format(today, format=\"%Y\"))\n\ntt_mdbrdf <- as.data.frame(tt_balt_gh) %>%\n  mutate(age = today_yr - yr_built) %>%\n  #  mutate(vehicles_n = as.numeric(str_remove(vehicles, \" vehicles\")))\n  ## not needed, avg_daily_traffic has same info\n  mutate(inspection_yr = inspection_yr + 2000) %>%\n  mutate(county = ifelse(county == \"Baltimore city\", \"Baltimore City\", county)) %>%\n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  mutate(bridge_condition = factor(bridge_condition, levels = c(\"Good\", \"Fair\", \"Poor\"))) %>%\n  mutate(decade_built = case_when(yr_built <= 1899 ~ \"pre 1900\", \n                                  yr_built >= 1900 & yr_built <1910 ~ \"1900-09\",\n                                  yr_built >= 1910 & yr_built <1920 ~ \"1910-19\",\n                                  yr_built >= 1920 & yr_built <1930 ~ \"1920-29\",\n                                  yr_built >= 1930 & yr_built <1940 ~ \"1930-39\",\n                                  yr_built >= 1940 & yr_built <1950 ~ \"1940-49\",\n                                  yr_built >= 1950 & yr_built <1960 ~ \"1950-59\",\n                                  yr_built >= 1960 & yr_built <1970 ~ \"1960-69\",\n                                  yr_built >= 1970 & yr_built <1980 ~ \"1970-79\",\n                                  yr_built >= 1980 & yr_built <1990 ~ \"1980-89\",\n                                  yr_built >= 1990 & yr_built <2000 ~ \"1990-99\",\n                                  yr_built >= 2000 & yr_built <2010 ~ \"2000-09\",\n                                  TRUE ~ \"2010-19\")) %>%\n  mutate(decade_built = factor(decade_built, levels = \n                                 c(\"pre 1900\", \"1900-09\", \"1910-19\", \"1920-29\", \"1930-39\",\n                                   \"1940-49\", \"1950-59\", \"1960-69\", \"1970-79\", \n                                   \"1980-89\", \"1990-99\", \"2000-09\", \"2010-19\"))) %>%\n  mutate(inspect_mmyy = ISOdate(year = inspection_yr, month = inspection_mo, day = \"01\")) %>%\n  mutate(inspect_mmyy = as.Date(inspect_mmyy, \"%m/%d/%y\")) %>%\n  mutate(inspect_days = today - inspect_mmyy) %>%\n  mutate(inspect_daysn = as.numeric(inspect_days)) %>%\n  mutate(inspect_years = inspect_daysn/ 365.25) %>%\n  mutate(inspect_months = inspect_daysn / 30.417)\n\n\n\n\nThe first few charts look at bridges built by decade, the condition of all bridges by county, and how long since last inspection.\n\ntt_mdbrdf %>% \n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  count(decade_built) %>%\n  ggplot(aes(decade_built, n)) +\n  geom_bar(stat = \"identity\", fill = \"navy\") +\n  geom_text(aes(label = n), color = \"white\", vjust = 1.2) +\n  labs(title = \"Peak bridge-building years in Maryland were 1950s to 1990s\" ,\n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nBaltimore City has the lowest percentage of bridges in good condition, Anne Arundel the most. Baltimore City & Harford County seems to have the largest percentage of bridges in poor condition.\n\n\nshow stacked bar chart code\n## percent bridge condition by county\n# need to create df object to do subset label call in bar chart\nbrcondcty <- \ntt_mdbrdf %>%\n  count(county, bridge_condition) %>%\n  group_by(county) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() \n\nggplot(brcondcty, aes(x = county, y = pct, fill = bridge_condition)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(data = subset(brcondcty, bridge_condition != \"Poor\"), \n            aes(label = percent(pct)), position = \"stack\", \n            color= \"#585858\", vjust = 1, size = 3.5) +\n  scale_y_continuous(label = percent_format()) +\n  labs(title = \"Percent bridge condition by county\" , \n        x = \"\", y = \"\", fill = \"Bridge Condition\") +\n  scale_fill_brewer(type = \"seq\", palette = \"Blues\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nGiven the condition percentages in Baltimore County & City and Harford County, it’s no surprise that their bridges are older than in other counties.\n\n\nshow bar chart code\n## median age of bridges by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medage = median(age)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medage)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medage, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median bridge age by county\" , \n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt’s somewhat reassuring then that Baltimore City bridges at least have less time in months since last inspection than do the counties.\n\n\nshow bar chart code\n## median months since last inspection by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medinsp = median(inspect_months)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medinsp)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medinsp, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median months since last inspection, by county\",\n       subtitle = \"as of March 2020\",\n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt might be the outliers pulling the smoothing line straight, but there doesn’t seem to be too much of a relationship between age and time since last inspection.\n\n\nshow scatterplot code\n## age by months since last inspection\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\n\nAnd in fact, removing the outliers shows a slight relationship; the older bridges do seem to get inspected more frequently. In terms of a better visualization, looking at this again, I wonder if some jittering or another type of plot might have been more visually appealing, givne the clustering of most recent inspections.\n\n# same code as above but with outliers removed\ntt_mdbrdf %>%\n  filter(age <150, inspect_months <60) %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(title = \"Months since inspection, outliers removed\", \n       x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\nNot sure if scatter-plot with colors by county is best way to go for this idea. Maybe a tree map?\n\n\nshow scatterplot code\n# same but colors by county\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  labs(title = \"Months since last inspection (from current date)\", \n       x = \"Months since last inspection (from current date)\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.8, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nFunky distributions here…Anne Arundel & Baltimore City have the highest median daily riders, but Howard County’s upper quartile is way out there.\nshowing the code here to illustrate how I like to run the mean & interquartiles in the same code as rendering the plot.\n\n# median & interquartiles of daily riders of bridges by county -\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medtraf = median(avg_daily_traffic),\n            lq = quantile(avg_daily_traffic, 0.25),\n            uq = quantile(avg_daily_traffic, 0.75)) %>%\n  ungroup() %>%\n  ggplot(aes(county, medtraf)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"navy\") +\n  geom_point(size = 3, color = \"orange\", alpha = .8) +\n  geom_text(aes(label = comma(medtraf, digits = 0)), \n            size = 4, color = \"orange\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = comma(uq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = comma(lq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Median & interquartile ranges of average daily riders per bridge, by county\" , \n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nAs with the other scatterplot with colors for county, might need a different way to see relationship between bridge age and daily traffic by county.\n\n\nshow scatterplot code\n## age by avg daily riders by county\ntt_mdbrdf %>%\n  ggplot(aes(avg_daily_traffic, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  scale_x_continuous(labels = comma) +\n  labs(title = \"Average daily traffic per bridge, by county\" , \n        x = \"Average daily traffic\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.75, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nCover image for post (CC BY-SA 4.0) from Wikipedia\nThis post was last updated on 2023-05-14"
  },
  {
    "objectID": "posts/migrating-from-hugo-to-quarto/index.html",
    "href": "posts/migrating-from-hugo-to-quarto/index.html",
    "title": "Migration from Hugo to Quarto",
    "section": "",
    "text": "The Saône & The Rhône converge in Lyon\n\n\nAs I mentioned in the Migration and Resurrection post, some issues with my old Hugo build and Netlify playing nicely with Hugo & Wowchemy led to me deciding to rebuild things in Quarto. I was thinking about it anyway, as Quarto seems robust and even more user-friendly (to me) than blogdown & rmarkdown, which I still love. I just love Quarto more.\nThe basic idea of the blog remains the same - present the code & analysis for topics that interest me. While switching from SAS to r, I learned so much from reading tons of data blogs and watching how-to videos that I wanted to give back to the rstats community by showing and explaining my work. It also helps me as my own reference for code and approach to visualization when I forget how I did a pivot-long or cleaned and set-up data to work well with the visualization I wanted to do.\nI decided to set some ground rules for the old posts:\n\nMinimal text edits…only for typos and occasional clunky language.\nBut no editing the code. The projects I posted showed where I was in using r at the time. Though to be honest, if anything I’ve regressed a bit as I haven’t used r that much since I left my last data job. Besides, the rmarkdown -> quarto migration already entailed enough editing in the YAML and code chunk headers.\nThe only exception to the code edit rule was changing the chunk options to the #| syntax and adding code-fold options to some code chunks. It wasn’t easily doable in blogdown & Hugo when I first launched the site but it’s a native functionality to Quarto, so hoorah!\nRepost the projects with date-stamps from the original posting. I want this to still accurately document my own data analysis/data science progression, even with that long gap between posts.\n\nSo how did I do it?\nFirst I spent a sunny weekend afternoon in May watching two presentations about Quarto.\nThis Welcome to Quarto Workshop from August 2022 led by Thomas Mock.\n\n\n\n\n\n\n\n\n\n\nAfter that it was Isabella Velásquez’s presentation on building a blog with Quarto.\nI also read the extensive Quarto documentation.\nEach of their personal blogs are nicely done in Quarto so I’ll also be poking around their github repos.\nAs I had already working with blogdown & rmarkdown, the transition to Quarto was smooth. Minor grammatical differences in the YAML and code chunks, but nothing that didn’t make sense.\nSetting up the blog is as simple as starting any new project in r. Just go to: File -> New Project -> New Directory -> Quarto blog and fill in the name and directory location.\nAfter setting up the basic design using one of the packaged themes and drafting the landing and About pages, I pushed the new files to github and hoped that Netlify would play nicely and render the new site.\nOn the first commit, which wiped out all of my old content and replaced with the new files, Netlify did its thing but I got the dreaded Error 404, site not found. With a little digging I found out that I had to go the Build & Deploy -> Continuous Deployment -> Build Settings section and add _site to the Publish Directory box like this:\n\n\n\n\n\nDid that, did another git commit and voila, up and running.\nNext step was to spend a sunny Sunday afternoon redoing my old rmd files to qmd, and navigating the differences in YAML and code chunk options.\n\n\n\ncode chunk options in rmd\n\n\n\n\n\ncode chunk options in qmd\n\n\nI also like the intelligent completion bits in Quarto \nand using ctrl + space to see all the parameters for the option you’re setting.\nquick aside…used veed.io for convert a screen-capture movie to animated gif…quick and easy\nGoing forward I’ll probably tweak the design now and then as I learn a bit more customization and functionality in Quarto and learn CSS and other styling tools for things like wrapping text around images and other tweaks and enhancements. But for now the site looks good and it’s time to get back to adding new data posts.\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/blog-migration-and-resurrection/index.html",
    "href": "posts/blog-migration-and-resurrection/index.html",
    "title": "…and we’re back. Migration and resurrection",
    "section": "",
    "text": "So much has happened since my last post here in February 2021. The blog was set up during the early stages of COVID lockdown when all of a sudden I had more time on my hands.\nSoon after that last post my wife and I made the decision to move to Europe in early 2022, so started preparations for that - arrange for a shipping container, learn French, apply for a CELTA program.\nThe world started to repoen a bit in late summer 2021, but then my wife broke her ankle. So all of a sudden I was doing lots of home health care & keeping the house running on top of work. The wasn’t much brain space for independent data projects & keeping the blog running.\nThen in late April 2022 we moved. The first stop was Lyon France\n\n\n\n\n\nwhere I earned the CELTA via the ELT Hub.\nI had expected teaching gigs to trickle in and I’d do independent data work while also spending time learning French and exploring Lyon and more of France. But the gigs came in abundance, and they were all new preps. So free time was at a minimum.\nThen in December 2022 we moved again. A job at CIEE in Denmark\n\n\n\n\n\ncame up so we accelerated the ultimate end goal of moving here - back to where I was born and still have lots of family and friends. Now that we’re settled in our first non-sublet in almost a year and I’ve gotten settled at work, there’s finally some time to get my head back into data work. I was partly inspired by serving as a mentor in the DDSA mentoring program. I also started using r at work to automate a few mundane tasks and to do a little analysis on our students.\nI’ll write about the migration process in a bit more detail in another post, but the tl/dr as to why is because in trying to do a simple update to my old main picture (me in a mask, early in lockdown) the build was failing in Netlify. So rather than try and diagnose the Hugo/Wowchemy issues (which turned out to be the Ubuntu deploy image…an easy reset), I saw that Posit’s new Quarto platform is very robust and a bit easier to publish with, so I decided to just rebuild the entire site.\nTo read about the migration details, go here. All of the old posts are up, time-stamped with the original posting dates. And hopefully soon, new posts as I work through a fairly long list of project ideas and try to get back to working on #tidytuesday datasets.\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/tidy-tuesday-maryland-bridges/index.html",
    "href": "posts/tidy-tuesday-maryland-bridges/index.html",
    "title": "Tidy Tuesday, November 27, 2018 - Maryland Bridges",
    "section": "",
    "text": "The Francis Scott Key Bridge in Baltimore\n\n\nThis dataset was posted to the #TidyTuesday repo back in November 2018. I worked on it a bit then, but didn’t properly finish it until March of 2020. With the blog finally set up figured I might as well post it as an entry.\nUpdate for migration from Hugo to Quarto…now that code-fold is native to code chunks, I’ll sometimes use if for long bits of code. Just click the down arrow to show code\n\n# readin in data, create df for plots\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\nFirst let’s read in the file from the raw data file on github\n\ntt_balt_gh <-\nread_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-27/baltimore_bridges.csv\",\n         progress = show_progress())\n\n\n\nOrganize and clean the data for analysis\n\nclean up some date and name issues\na decade-built field, factored for sorting\ntime since last inspection\n\n\n\nshow data cleaning code\n# keeping the comparison date March 21 when I originally did analysis\ntoday <- as.Date(c(\"2020-03-21\"))\n#Sys.Date()\ntoday_yr <- as.numeric(format(today, format=\"%Y\"))\n\ntt_mdbrdf <- as.data.frame(tt_balt_gh) %>%\n  mutate(age = today_yr - yr_built) %>%\n  #  mutate(vehicles_n = as.numeric(str_remove(vehicles, \" vehicles\")))\n  ## not needed, avg_daily_traffic has same info\n  mutate(inspection_yr = inspection_yr + 2000) %>%\n  mutate(county = ifelse(county == \"Baltimore city\", \"Baltimore City\", county)) %>%\n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  mutate(bridge_condition = factor(bridge_condition, levels = c(\"Good\", \"Fair\", \"Poor\"))) %>%\n  mutate(decade_built = case_when(yr_built <= 1899 ~ \"pre 1900\", \n                                  yr_built >= 1900 & yr_built <1910 ~ \"1900-09\",\n                                  yr_built >= 1910 & yr_built <1920 ~ \"1910-19\",\n                                  yr_built >= 1920 & yr_built <1930 ~ \"1920-29\",\n                                  yr_built >= 1930 & yr_built <1940 ~ \"1930-39\",\n                                  yr_built >= 1940 & yr_built <1950 ~ \"1940-49\",\n                                  yr_built >= 1950 & yr_built <1960 ~ \"1950-59\",\n                                  yr_built >= 1960 & yr_built <1970 ~ \"1960-69\",\n                                  yr_built >= 1970 & yr_built <1980 ~ \"1970-79\",\n                                  yr_built >= 1980 & yr_built <1990 ~ \"1980-89\",\n                                  yr_built >= 1990 & yr_built <2000 ~ \"1990-99\",\n                                  yr_built >= 2000 & yr_built <2010 ~ \"2000-09\",\n                                  TRUE ~ \"2010-19\")) %>%\n  mutate(decade_built = factor(decade_built, levels = \n                                 c(\"pre 1900\", \"1900-09\", \"1910-19\", \"1920-29\", \"1930-39\",\n                                   \"1940-49\", \"1950-59\", \"1960-69\", \"1970-79\", \n                                   \"1980-89\", \"1990-99\", \"2000-09\", \"2010-19\"))) %>%\n  mutate(inspect_mmyy = ISOdate(year = inspection_yr, month = inspection_mo, day = \"01\")) %>%\n  mutate(inspect_mmyy = as.Date(inspect_mmyy, \"%m/%d/%y\")) %>%\n  mutate(inspect_days = today - inspect_mmyy) %>%\n  mutate(inspect_daysn = as.numeric(inspect_days)) %>%\n  mutate(inspect_years = inspect_daysn/ 365.25) %>%\n  mutate(inspect_months = inspect_daysn / 30.417)\n\n\n\n\nThe first few charts look at bridges built by decade, the condition of all bridges by county, and how long since last inspection.\n\ntt_mdbrdf %>% \n  mutate(county = str_replace(county, \" County\", \"\")) %>%\n  count(decade_built) %>%\n  ggplot(aes(decade_built, n)) +\n  geom_bar(stat = \"identity\", fill = \"navy\") +\n  geom_text(aes(label = n), color = \"white\", vjust = 1.2) +\n  labs(title = \"Peak bridge-building years in Maryland were 1950s to 1990s\" ,\n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nBaltimore City has the lowest percentage of bridges in good condition, Anne Arundel the most. Baltimore City & Harford County seems to have the largest percentage of bridges in poor condition.\n\n\nshow stacked bar chart code\n## percent bridge condition by county\n# need to create df object to do subset label call in bar chart\nbrcondcty <- \ntt_mdbrdf %>%\n  count(county, bridge_condition) %>%\n  group_by(county) %>%\n  mutate(pct = n / sum(n)) %>%\n  ungroup() \n\nggplot(brcondcty, aes(x = county, y = pct, fill = bridge_condition)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(data = subset(brcondcty, bridge_condition != \"Poor\"), \n            aes(label = percent(pct)), position = \"stack\", \n            color= \"#585858\", vjust = 1, size = 3.5) +\n  scale_y_continuous(label = percent_format()) +\n  labs(title = \"Percent bridge condition by county\" , \n        x = \"\", y = \"\", fill = \"Bridge Condition\") +\n  scale_fill_brewer(type = \"seq\", palette = \"Blues\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nGiven the condition percentages in Baltimore County & City and Harford County, it’s no surprise that their bridges are older than in other counties.\n\n\nshow bar chart code\n## median age of bridges by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medage = median(age)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medage)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medage, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median bridge age by county\" , \n        x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt’s somewhat reassuring then that Baltimore City bridges at least have less time in months since last inspection than do the counties.\n\n\nshow bar chart code\n## median months since last inspection by county\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medinsp = median(inspect_months)) %>%\n  ungroup() %>%\n  ggplot(aes(x = county, y = medinsp)) +\n  geom_bar(stat = \"identity\", fill= \"navy\") +\n  geom_text(aes(label = round(medinsp, digits = 1)), \n            size = 5, color = \"white\", vjust = 1.6) +\n  ylim(0, 60) +\n  labs(title = \"Median months since last inspection, by county\",\n       subtitle = \"as of March 2020\",\n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nIt might be the outliers pulling the smoothing line straight, but there doesn’t seem to be too much of a relationship between age and time since last inspection.\n\n\nshow scatterplot code\n## age by months since last inspection\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\n\nAnd in fact, removing the outliers shows a slight relationship; the older bridges do seem to get inspected more frequently. In terms of a better visualization, looking at this again, I wonder if some jittering or another type of plot might have been more visually appealing, givne the clustering of most recent inspections.\n\n# same code as above but with outliers removed\ntt_mdbrdf %>%\n  filter(age <150, inspect_months <60) %>%\n  ggplot(aes(inspect_months, age)) +\n  geom_point(color = \"navy\") +\n  geom_smooth() +\n  labs(title = \"Months since inspection, outliers removed\", \n       x = \"Months since last inspection (as of March 2020)\",\n       y = \"Age _(in years)_\") +\n  theme_minimal()\n\n\n\n\nNot sure if scatter-plot with colors by county is best way to go for this idea. Maybe a tree map?\n\n\nshow scatterplot code\n# same but colors by county\ntt_mdbrdf %>%\n  ggplot(aes(inspect_months, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  labs(title = \"Months since last inspection (from current date)\", \n       x = \"Months since last inspection (from current date)\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.8, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nFunky distributions here…Anne Arundel & Baltimore City have the highest median daily riders, but Howard County’s upper quartile is way out there.\nshowing the code here to illustrate how I like to run the mean & interquartiles in the same code as rendering the plot.\n\n# median & interquartiles of daily riders of bridges by county -\ntt_mdbrdf %>%\n  group_by(county) %>%\n  summarise(medtraf = median(avg_daily_traffic),\n            lq = quantile(avg_daily_traffic, 0.25),\n            uq = quantile(avg_daily_traffic, 0.75)) %>%\n  ungroup() %>%\n  ggplot(aes(county, medtraf)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"navy\") +\n  geom_point(size = 3, color = \"orange\", alpha = .8) +\n  geom_text(aes(label = comma(medtraf, digits = 0)), \n            size = 4, color = \"orange\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = comma(uq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = comma(lq, digits = 0)), \n            size = 4, color = \"navy\", hjust = 1.2) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Median & interquartile ranges of average daily riders per bridge, by county\" , \n       x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nAs with the other scatterplot with colors for county, might need a different way to see relationship between bridge age and daily traffic by county.\n\n\nshow scatterplot code\n## age by avg daily riders by county\ntt_mdbrdf %>%\n  ggplot(aes(avg_daily_traffic, age, color = county)) +\n  geom_point() +\n  scale_color_brewer(palette=\"Dark2\") +\n  scale_x_continuous(labels = comma) +\n  labs(title = \"Average daily traffic per bridge, by county\" , \n        x = \"Average daily traffic\",\n       y = \"Age (in years)\") +\n  theme_minimal() +\n  theme(legend.position = c(.75, .95),\n        legend.justification = c(\"right\", \"top\"),\n        legend.box.just = \"right\",\n        legend.margin = margin(6, 6, 6, 6))\n\n\n\n\n\nCover image for post (CC BY-SA 4.0) from Wikipedia\nThis post was last updated on 2023-05-14"
  },
  {
    "objectID": "blog-migration-and-resurrection/index.html",
    "href": "blog-migration-and-resurrection/index.html",
    "title": "…and we’re back. Migration and resurrection",
    "section": "",
    "text": "Lots of things happening since last post in February 2021. The blog was set up during the early stages of COVID lockdown when all of a sudden I had more time on my hands.\nBut since that last post my wife and I made the decision to move to Europe in early 2022 and started preparations for that. The world started to repoen a bit in late summer 2021, and then my wife broke her ankle. So all of a sudden I was doing lots of home health care & keeping the house running on top of work. The wasn’t much brain space for independent data projects & keeping the blog running.\nThen in April 2022 we moved. The first stop was Lyon France\n\n\n\n\n\nwhere I earned a CELTA via the ELT Hub.\nI had expected teaching gigs to trickle in and I’d do independent data work while also spending time learning French and exploring Lyon and more of France. But the gigs came in abundance, and they were all new preps. So free time was at a minimum.\nThen in December 2022 we moved again. A job at CIEE in Denmark\n\n\n\n\n\ncame up and we accelerated the ultimate end goal of moving here - back to where I was born and still have lots of family and friends. Now that we’re settled in our first non-sublet in almost a year and I’ve gotten settled at work, there’s finally some time to get my head back into data work. I was partly inspired by serving as a mentor in the DDSA mentoring program. I also started using r at work to automate a few mundane tasks and to do a little analysis on our students.\nI’ll write about the migration process in a bit more detail in another post, but the tl/dr as to why is because in trying to do a simple update to my old main picture (me in a mask, early in lockdown) the build was failing in Netlify. So rather than try and diagnose the Hugo/Wowchemy issues (which turned out to be the Ubuntu deploy image…an easy reset), I saw that Posit’s new Quarto platform is very robust and a bit easier to publish with, so I decided to just rebuild the entire site.\nTo read about the migration details, go here. All of the old posts are up, time-stamped with the original posting dates. And hopefully soon, new posts as I work through a fairly long list of project ideas."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "© Copyright Gregers Kjerulf Dubrow\nThis is my personal website. All views are mine, and nothing I post here should be taken as an endorsement by my employer or any organizations of which I am a member.\nContent on this site is provided under a Creative Commons (CC-BY) 4.0 license. You may reuse this content as long as you indicate cite me as the author and provide a link back to the original material. Source code of the site is provided under the MIT license and may be reused without restriction."
  },
  {
    "objectID": "posts/tidy-tuesday-feb-02-2021-hbcu/index.html",
    "href": "posts/tidy-tuesday-feb-02-2021-hbcu/index.html",
    "title": "Tidy Tuesday, February 2, 2021 - HBCU Enrollment",
    "section": "",
    "text": "When the the Tidy Tuesday HBCU dataset was posted I got excited because here was something right in my scope of knowledge; enrollment in higher education. It was a great first Tidy Tuesday set for Black History Month. To keep this from being a journal-length piece, I won’t put on my history of higher ed teacher hat & get into how HBCUs came to be. Here’s a good overview.\nOne of my strengths as a data analyst is a brain that’s constantly asking questions of a dataset:\n* what’s in there?\n* what relationships exist between variables?\n* what I can add to the dataset to glean more insight?\nFor Tidy Tuesday though that slows things down – it’s mostly meant to be a quick turnaround thing where you share results to Twitter. So my deep-dive habits mean I’m usually a bit behind getting Tidy Tuesday analysis done the week the data are posted. My goal for this analysis is to show:\n* changes over time in the racial and gender make-up of students at HBCUs\n* changes over time in overall enrollment at HBCUs by sector (public 4yr, private 4y, etc), relative to non-HBCUs\n* changes over time in tuition & fees by sector, HBCUs vs non-HBCUs\nThe Tidy Tuesday data has some of what I need - it comes from Data.World via IPEDS (essentially the US Department of Education’s higher ed data library) . I’m supplementing it with data from the Delta Cost Project, which aggrgated years worth of data from IPEDS.\nSo let’s dive into the data.\nFirst we load the packages we’ll be using…\n\n# load packages\nlibrary(tidytuesdayR) # load the tidy tuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot\nlibrary(ggrepel) # helper functions for ggplot\n\n…then lets make a couple of things we’ll use a lot going forward.\n\n# create notin operator to help with cleaning & analysis\n`%notin%` <- negate(`%in%`)\n\n# function for year-over-year percent changes\npctchange <- function(x) {\n  (x - lag(x)) / lag(x)\n}\n\nNow we’ll load the sets from TidyTuesday and then the Delta Cost Project files.\nI’ll load the Tidy Tuesday data objects via the tidytuesdayR package and clean up the data. This week’s data came in a few separate files. For my analysis I need a dataframe of HBCU enrollments with Black and non-Black students. So I need to combine the two HBCU enrollment files (one each for Black students and all students) into one and subtract Black from All to get non-Black. I’ll show code for loading the Black student set in, cleaning it up a bit, and skip to the subtraction & joins. For the full code experience, head to my Tidy Tuesday repo\n\n\nshow tt_hbcu_black data load & cleaning code\ntt_hbcu_load <- tt_load(\"2021-02-02\")\n\ntt_hbcu_black <- as_tibble(tt_hbcu_load$hbcu_black) %>%\n  clean_names() %>%\n  mutate(year = as.character(year)) %>%\n  mutate(ethnicity = \"Black\") %>%\n  select(year, ethnicity, enroll_n = total_enrollment, women = females, men = males, \n         four_year_all = x4_year, two_year_all = x2_year,\n         total_public, four_year_pub = x4_year_public, two_year_pub = x2_year_public, \n         total_private, four_year_pri = x4_year_private, two_year_pri = x2_year_private) \n\n\n\n\nshow tt_hbcu_all data load & cleaning code\ntt_hbcu_all <- as_tibble(tt_hbcu_load$hbcu_all) %>%\n  clean_names() %>%\n  mutate(ethnicity = \"All\") %>%\n  mutate(year = as.character(year)) %>%\n  select(year, ethnicity, enroll_n = total_enrollment, women = females, men = males, \n         four_year_all = x4_year, two_year_all = x2_year,\n         total_public, four_year_pub = x4_year_public, two_year_pub = x2_year_public, \n         total_private, four_year_pri = x4_year_private, two_year_pri = x2_year_private)  \n\n\nLet’s join tt_hbcu_black with tt_hbcu_all and create the non-Black ethnicity group. There is probably a better way to do the subtractions programmatically with a function but my r skills aren’t there yet. The final dataframes I’ll use for plots are tt_hbcu_enr_eth_sex & tt_hbcu_enr_eth_sect. Because of the way the source data was arrayed I could do ethnicty by gender and ethnicity by sector but not all three vectors. So, two sets…\n\n\nshow dataframe join code\ntt_hbcu_notblack = as.data.frame(tt_hbcu_all) %>%\n  select(-ethnicity) %>%\n    # renaming multiple columns in one line of code!\n  rename_with(~ paste(.x, \"all\", sep = \"_\"), .cols = (2:12)) %>%\n  bind_cols(tt_hbcu_black) %>%\n  select(-year...13, -ethnicity) %>%\n  rename(year = year...1) %>%\n  # the subtractions by column to get non-black. again, might be a better way to do it w/ a function.\n  mutate(enroll_n_nb = enroll_n_all - enroll_n) %>%\n  mutate(women_nb = women_all - women) %>%\n  mutate(men_nb = men_all - men) %>%\n  mutate(four_year_all_nb = four_year_all_all - four_year_all) %>%\n  mutate(two_year_all_nb = two_year_all_all - two_year_all) %>%\n  mutate(total_public_nb = total_public_all - total_public) %>%\n  mutate(four_year_pub_nb = four_year_pub_all - four_year_pub) %>%\n  mutate(two_year_pub_nb = two_year_pub_all - two_year_pub) %>%\n  mutate(total_private_nb = total_private_all - total_private) %>%\n  mutate(four_year_pri_nb = four_year_pri_all - four_year_pri) %>%\n  mutate(two_year_pri_nb = two_year_pri_all - two_year_pri) %>%\n  mutate(ethnicity = \"Not Black\") %>%\n  select(year, ethnicity, enroll_n_nb:two_year_pri_nb) %>%\n  rename_with(~ str_remove(.x, \"_nb\"), .cols = (3:13))\n\n# create final dataframes, turn wide to long\n# note sex pct is by eth group\ntt_hbcu_enr_eth_sex = as.data.frame(rbind(tt_hbcu_all, tt_hbcu_black)) %>%\n  rbind(tt_hbcu_notblack) %>%\n  select(-four_year_all:-two_year_pri) %>%\n  arrange(year, ethnicity) %>% \n  group_by(year) %>%\n   mutate(enroll_eth_pct = enroll_n / first(enroll_n)) %>%\n  ungroup() %>%\n  pivot_longer(cols = women:men,\n               names_to = \"sex\",\n               values_to = \"sex_n\") %>%\n  arrange(year, ethnicity) %>%\n  group_by(year, ethnicity) %>%\n  mutate(enroll_sex_pct = sex_n / sum(sex_n)) %>%\n  ungroup() %>%\n  select(year, ethnicity, enroll_n, enroll_eth_pct, sex, sex_n, enroll_sex_pct) %>%\n  arrange(year, ethnicity, sex)\n\n# note pct_sect_eth is by eth group by year, pct_eth_sect is pct eth w/in sector\ntt_hbcu_enr_eth_sect = rbind(tt_hbcu_all, tt_hbcu_black) %>%\n  rbind(tt_hbcu_notblack) %>%\n  select(-women, -men) %>%\n  arrange(year, ethnicity) %>%\n  pivot_longer(cols = four_year_all:two_year_pri,\n               names_to = \"sector\", \n               values_to = \"sector_n\") %>%\n  arrange(year, ethnicity) %>%\n  mutate(pct_sect_eth = sector_n / enroll_n) %>%\n  arrange(year, sector) %>%\n  group_by(year, sector) %>%\n  mutate(pct_eth_sect = sector_n / (sum(sector_n) /2)) %>%\n  ungroup()\n\n\nOk, the Tidy Tuesday data is sorted, so let’s use the haven package load in the Delta Cost Project (DCP) data they provided as SAS files.\nThey’ve split their data into two files, one from 1987 to 1999 and one from 2000 to 2015. There’s a ton of data in the set, but all I want for this analysis is enrollments and tuition - I’ll pull those fields from each set, then rbind together into a dataframe called tuitenr_8715_agg for year-over-year by-sector analysis. Unfortunately while DCP has enrollment by ethnicity, they only do it for total enrollment - grad and undergrad combined - so I can’t do some of the HBCU vs non-HBCU comparisons. There are other sources which I’ll describe at the end of the post.\nOk, loading the 2000 to 2015 data…\n\n\nshow DCP 2000 - 2015 load code\ndelta0015all <- (haven::read_sas(\"~/Data/ipeds/delta_public_release_00_15.sas7bdat\", NULL))\ndelta0015_tuitenr <- delta0015all %>%\n  filter(between(sector_revised, 1, 6)) %>%\n  mutate(sector_desc = case_when(sector_revised == 1 ~  \"Public 4yr\",\n                                 sector_revised == 2    ~ \"Private nonprofit 4yr\",\n                                 sector_revised == 3    ~ \"Private for-profit 4yr\",\n                                 sector_revised == 4    ~ \"Public 2yr\",\n                                 sector_revised == 5    ~ \"Private nonprofit 2yr\",\n                                 sector_revised == 6    ~ \"Private for-profit 2yr\")) %>%\n # mutate(total_undergraduates = ifelse(is.na(total_undergraduates), 0, total_undergraduates)) %>%\n  select(unitid, instname, sector_revised, sector_desc, hbcu,\n         year = academicyear, tuition_fee_ug_in = tuitionfee02_tf, \n         tuition_fee_ug_oos = tuitionfee03_tf, \n         total_undergraduates)\n\n\nNow let’s load the 1987 to 1999 data…\n\n\nshow DCP 1987 - 1999 load code\ndelta8799all <- (haven::read_sas(\"~/Data/ipeds/delta_public_release_87_99.sas7bdat\", NULL))\n\ndelta8799_tuitenr <- as_tibble(delta8799all) %>%\n  filter(between(sector_revised, 1, 6)) %>%\n  mutate(sector_desc = case_when(sector_revised == 1 ~  \"Public 4yr\",\n                                 sector_revised == 2    ~ \"Private nonprofit 4yr\",\n                                 sector_revised == 3    ~ \"Private for-profit 4yr\",\n                                 sector_revised == 4    ~ \"Public 2yr\",\n                                 sector_revised == 5    ~ \"Private nonprofit 2yr\",\n                                 sector_revised == 6    ~ \"Private for-profit 2yr\")) %>%\n # mutate(total_undergraduates = ifelse(is.na(total_undergraduates), 0, total_undergraduates)) %>%\n  select(unitid, instname, sector_revised, sector_desc, hbcu,\n         year = academicyear, tuition_fee_ug_in = tuitionfee02_tf, \n         tuition_fee_ug_oos = tuitionfee03_tf, \n         total_undergraduates)\n\n\n…and let’s join and munge the DCP files…\n\n\nshow DCP join & clean code\ntuitenr_8715 <- rbind(delta0015_tuitenr, delta8799_tuitenr)\n\ntuitenr_8715_agg <-\n  tuitenr_8715 %>%\n  # filter(!sector_desc == \"Private for-profit 2yr\" &\n  #          !sector_desc == \"Private for-profit 4yr\") %>%\n#  filter(!is.na(tuition_fee_ug_in)) %>%\n  group_by(year, sector_desc, hbcu) %>%\n  summarise(enr_tot = sum(total_undergraduates, na.rm = TRUE),\n            mean_tuit_in = mean(tuition_fee_ug_in, na.rm = TRUE),\n            mean_tuit_oos = mean(tuition_fee_ug_oos, na.rm = TRUE)\n            ) %>%\n  mutate(level = ifelse(str_detect(sector_desc, \"2yr\"), \"2yr\", \"4yr\")) %>%\n#  summarise(mean_tuit_in = mean(tuition_fee_ug_in, na.rm = T)) %>%\n  ungroup() %>%\n  mutate(hbcu_f = ifelse(hbcu == 1, \"HBCU\", \"Not HBCU\")) %>%\n  mutate(hbcu_f = factor(hbcu_f, levels = c(\"HBCU\", \"Not HBCU\"))) \n\n\nBecause I’ll be doing some year-over-year percent change analysis, I’ll create a separate dataframe from tuitenr_8715_agg (the DCP data) so I can compare HBCU to non-HBCU changes by year.\nRemember the pctchange function I created when I loaded the packages? Here it’s put to good use. I’ll only show code for one of the frames - I created frames for each sector, HBCU and non-HBCU. The final data set will rbind 10 frames into tuit_8715_pctchgh. BTW, this is another example of when I wish I were a bit better at purrr/mapping and functions so I could have done it with less code.\nSince the data provided starts in 1987, I’ll filter that year out of the dataframes.\n\ntuit_8715_pctchgh1 <- tuitenr_8715_agg %>%\n  filter(hbcu == 1, sector_desc == \"Public 4yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\n\n\nshow the remaining sector files code\ntuit_8715_pctchgh2 <- tuitenr_8715_agg %>%\n  filter(hbcu == 1, sector_desc == \"Private nonprofit 4yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh3 <- tuitenr_8715_agg %>%\n  filter(hbcu == 1, sector_desc == \"Public 2yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh4 <- tuitenr_8715_agg %>%\n  filter(hbcu == 1, sector_desc == \"Private nonprofit 2yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh5 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Public 4yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh6 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Private nonprofit 4yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh7 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Public 2yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh8 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Private nonprofit 2yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh9 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Private for-profit 4yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh10 <- tuitenr_8715_agg %>%\n  filter(hbcu == 2, sector_desc == \"Private for-profit 2yr\") %>%\n  select(year, hbcu, sector_desc, enr_tot, mean_tuit_in, mean_tuit_oos) %>% \n  arrange(year) %>% \n  mutate(across(4:6, pctchange)) %>% \n  select(year, hbcu, sector_desc, everything()) %>%\n  ungroup() %>%\n  filter(year > 1987)\n\ntuit_8715_pctchgh <- rbind(tuit_8715_pctchgh1, tuit_8715_pctchgh2) %>%\n  rbind(tuit_8715_pctchgh3) %>%\n  rbind(tuit_8715_pctchgh4) %>%\n  rbind(tuit_8715_pctchgh5) %>%\n  rbind(tuit_8715_pctchgh6) %>%\n  rbind(tuit_8715_pctchgh7) %>%\n  rbind(tuit_8715_pctchgh8) %>%\n  rbind(tuit_8715_pctchgh9) %>%\n  rbind(tuit_8715_pctchgh10) \n\n\nOk, the data is in place, let’s see what it tells us! I’ll fold the code for the more basic charts, click on the arrow to see it. You’ll note that this analysis is mostly line graphs. That’a because I’m mostly doing trend analysis, and I like line graphs for time series.\nFirst up we’ll use the Tidy Tuesday data to look at HBCU enrollment by ethnicity (Black and non-Black) from 1990 to 2015. What do we see? Overall, periods of increase and decrease in total enrollment. But interestingly we see that non-Black enrollment kept increasing, even as Black enrollment started to drop. Of course Black students make up a vast majority of HBCU enrollment, so their trends drive overall numbers. It is worth noting though that the changes started in the 2010s, coinciding with changes in how NCES counted ethncity. It’s beyond the scope of this quick (ha) analysis, but worth digging deeper to see the effect of changes on the ethnic mix of HBCUs.\n\n\nshow the enrollment by ethnicity line charts code\n## tt hbcu data, black men v women over time\ntt_hbcu_enr_eth_sex %>%\n  filter(year > 1989) %>%\n  ggplot(aes(year, enroll_n, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format(), breaks = scales::pretty_breaks()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  facet_grid(ethnicity ~ ., scales = \"free_y\") +\n  labs(title = \"Black Enrollment at HBCUs Rising & Falling Since 1990\",\n       subtitle = \"Non-Black enrollment slowly & streadily increasing\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Total UG Enrollment\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity line charts code\n\ntt_hbcu_enr_eth_sex %>%\n  filter(year > 1989) %>%\n  filter(ethnicity == \"Black\") %>%\n  distinct(year, ethnicity, .keep_all = T) %>%\n  ggplot(aes(year, enroll_eth_pct, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1.25) +\n  scale_y_continuous(limits = c(.5, 1), \n                     labels = scales::percent_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Slight Decrease Over Time in Percentage of Black Students Enrolled at HBCUs\",\n       subtitle = \"Might be due to changes in how ethnicity is coded by US Dept of Ed\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Pct Black students\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), \n        plot.title = element_text(size = 11),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nNext let’s look at enrollment by ethnicity & sex. HBCU enrollment mirrors that of higher ed enrollment in general - since the early 1980s women outnumber men. There are differences by sector (4-year, 2-year, public & private) and field of study but the oveall trend has been consistent. We do see that for non-Black students at HBCUs the percentage of women is a bit less than that of Black students.\n\n\nshow the enrollment by ethnicity & sex line charts code\ntt_hbcu_enr_eth_sex %>%\n  filter(year > 1989) %>%\n  ggplot(aes(year, sex_n, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  # scale_y_continuous(limits = c(0, 250000),\n  #                    breaks = c(0, 50000, 100000, 150000, 200000, 250000),\n  #                    labels = scales::comma_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Black Women Enroll in Higher Numbers Than Men at HBCUs\",\n       subtitle = \"Same Pattern as Overall Undergrad Enrollments in the US\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Total UG Enrollment\") +\n  facet_grid(ethnicity ~ sex, scales = \"free_y\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity & sex line charts code\n\n# sex - women as pct\ntt_hbcu_enr_eth_sex %>%\n  filter(year > 1989) %>%\n  filter(sex == \"women\") %>%\n  ggplot(aes(year, enroll_sex_pct, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(limits = c(.5, .8), \n                     labels = scales::percent_format()) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  labs(title = \"Women a Greater Percentage of those Enrolled at HBCUs\",\n       subtitle = \"Slightly higher among Black students than not Black\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Pct Women enrolled\") +\n  facet_grid(ethnicity ~ ., scales = \"free_y\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nThe next thing I want to look at is enrollment by sector for HBCUs. In this case we think of sectors along two axes - public and private, and 2-year (community/junior colleges) vs 4-year (baccalaureate granting). What these first two sector charts show us is that most HBCU enrollment is in publics and in 4-year schools. Given the land-grant history of most HBCUs, that makes sense.\nI am leaving in the code for the first chart to highlight how one can a) have two different colored lines within the same chart in a faceted visualization (the scale_color_manual call) and b) use the amazing ggtext package to get two different colors of text in the subtitle line, so it does the work of a legend. The <span style = </span> and the element_markdown call in the theme specification is how you make it happen.\nThe third chart takes a slightly different view of the data, looking at percent of enrollment by sector within each of the ethnic categories we have in the data, Black and not-Black. What the first two charts showed is confirmed here - that Black students in HBCUs are mostly in public 4-years, and that trend has held steady since at least 1990.\n\ntt_hbcu_enr_eth_sect %>%\n  filter(year > 1989) %>%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %>%\n  filter(sector %in% c(\"total_private\", \"total_public\")) %>%\n  ggplot(aes(year, pct_sect_eth, group = sector, color = sector)) +\n  geom_line(size = 1) +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"Public HBCUs Enroll Greater Pct of Students \",\n       subtitle = \"Greater Pct Black students in Private HBCU than non-Black students; \n       <span style = 'color:#56B4E9;'>Blue = Public</span> \n       <span style = 'color:#E69F00;'>Orange = Private</span>\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by Publc & Private HBCUs\") +\n  facet_grid(ethnicity ~ .) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_markdown(size = 8, face = \"italic\"),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\n\nshow the enrollment by ethnicity & sector charts code\ntt_hbcu_enr_eth_sect %>%\n  filter(year > 1989) %>%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %>%\n  filter(sector %in% c(\"four_year_all\", \"two_year_all\")) %>%\n  ggplot(aes(year, pct_sect_eth, group = sector, color = sector)) +\n  geom_line(size = 1) +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"4-year HBCUs Enroll ~ 90% of all Students\",\n       subtitle = \"Black students more likely to be in 4-year HBCU than non-Black students; \n       <span style = 'color:#56B4E9;'>Blue = 4-year</span> \n       <span style = 'color:#E69F00;'>Orange = 2-year</span>\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by 4yr & 2yr HBCUs\") +\n  facet_grid(ethnicity ~ .) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_markdown(size = 8, face = \"italic\"),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the enrollment by ethnicity & sector charts code\n\ntt_hbcu_enr_eth_sect %>%\n  filter(year > 1989) %>%\n  filter(ethnicity %in% c(\"Black\", \"Not Black\")) %>%\n  filter(sector %in% c(\"four_year_pub\", \"four_year_pri\", \"two_year_pri\", \"two_year_pub\")) %>%\n  mutate(sector = str_replace(sector, \"four_year_pub\", \"4 Year Public\")) %>%\n  mutate(sector = str_replace(sector, \"two_year_pub\", \"2 Year Public\")) %>%\n  mutate(sector = str_replace(sector, \"four_year_pri\", \"4 Year Private\")) %>%\n  mutate(sector = str_replace(sector, \"two_year_pri\", \"2 Year Private\")) %>%\n#  ggplot(aes(year, enroll_sect_pct, group = sector, color = sector)) +\n  ggplot(aes(year, pct_sect_eth, group = 1)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_x_discrete(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(labels = scales::percent_format(),\n                     limits = c(0, 1), breaks = c(0, .25, .5, .75, 1)) +\n  labs(title = \"Black Students most likely to be in 4-year HBCUs\",\n       subtitle = \"Non-black students moving to 2-year HBCUs from 4-yr; \n       <span style = 'color:#E69F00;'>Percents sum to 100% across ethnic groups</span>\",\n       caption = \"Source: Tidy Tuesday data\",\n       x = \"\", y = \"Percent by Sector\") +\n  facet_grid(ethnicity ~ sector) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        axis.text.x = element_text(size = 6),\n        plot.subtitle = element_markdown(face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nSo we’ve done what we can with the Tidy Tuesday data, time to compare HBCUs to non-HBCUs using the Delta Cost Project data. We’ll stay with sector analysis and look at enrollments by sector. For the chart output I wanted, I used Thomas Lin Pederson’s patchwork package. I’ll fold the code that creates the component charts and just show the patchwork call.\n\n\nshow the enrollment by sector charts code\nenr19902015hbcu <-\ntuitenr_8715_agg %>%\n  filter(hbcu == 1) %>% \n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(title = \"HBCU Enrollment Across Sector 1990-2015\",\n       #caption = \"Source: Delta Cost Project\",\n    x = \"\", y = \"Total UG enrollment\") +\n  facet_wrap(~ sector_desc, scales = \"free\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n                axis.text.y = element_text(size = 7), \n        #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\nenr19902015nothbcu <-\n  tuitenr_8715_agg %>%\n  filter(hbcu == 2) %>%\n#  filter(level == \"2yr\") %>% \n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#56B4E9\", group = 1) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(title = \"non-HBCU Enrollment Across Sector 1990-2015\",\n       caption = \"Source: Delta Cost Project\",\n       x = \"\", y = \"Total UG enrollment\") +\n  facet_wrap(~ sector_desc, scales = \"free\") +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#56B4E9\"),\n                axis.text.y = element_text(size = 7), \n        #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\nenr19902015all <- enr19902015hbcu / enr19902015nothbcu\nenr19902015all\n\n\n\n\nFor a change of pace let’s see a slope graph of percent change in tuition by sector from 1990 to 2015. I found a helpful code-thru of a similar-ish slope graph here and used that as my guide. I used patchwork for this too, and because in this instance I used patchwork for title and other adornments, I’ll show code for all steps.\nTwo observations stand out - first, HBCUs are less expensive by sector than non-HBCUs. But for both HBCUs and non-HBCUs, the price gap between private 4-year schools and the other sectors widened during this time period.\nBut these are sticker prices and don’t include institutional aid, or “tuition discount”. If a college offers you a $3000 institutional grant (not a federal or state grant or loan), the money is essentially foregone revenue for the school, or a discount to the stated tuition price.\n\n\nshow the slope graph code\ntuitsect_hbcu <-\n    tuitenr_8715_agg %>%\n    filter(hbcu == 1 & (year == 1990 | year == 2015)) %>%\n    #  filter(level == \"2yr\") %>% \n    ggplot(aes(year, mean_tuit_in, group = sector_desc)) +\n    geom_line(aes(color = sector_desc)) +\n    geom_point(aes(color = sector_desc)) +\n    scale_color_manual(values = c(\"Public 4yr\" = \"#999999\", \n                                                                \"Public 2yr\" = \"#E69F00\", \n                                                                \"Private nonprofit 2yr\" = \"#56B4E9\", \n                                                                \"Private nonprofit 4yr\" = \"#009E73\")) +\n    geom_text_repel(data = tuitenr_8715_agg %>%\n                                        filter(hbcu == 1 & year == 2015),\n                                    aes(label = sector_desc), hjust = \"left\", nudge_x = .5,\n                                    direction = \"y\", size = 4) +\n    annotate(\"text\", x = 1991, y = 12000, label = \"HBCUs\", size = 5, fontface = \"italic\") +\n    scale_x_continuous(breaks = c(1990, 2015)) +\n    scale_y_continuous(labels = scales::dollar_format(),\n                                         limits = c(0, 15000),\n                                         breaks = c(0, 2500, 5000, 7500, 10000, 12500, 15000)) +\n    labs(x = \"\", y = \"Average Tuition & Fees\") +\n    theme_light() +\n    theme(panel.grid.major = element_blank(), panel.grid.major.x = element_blank(), \n                panel.grid.minor = element_blank(), panel.grid.minor.y = element_blank(),\n                panel.background = element_blank(), \n                strip.background = element_rect(fill = \"#56B4E9\"),\n                panel.border = element_blank(), legend.position = \"none\",  \n                axis.text.y = element_text(size = 8), \n                axis.ticks = element_blank(), axis.text.x = element_blank(), \n                axis.title.y = element_text(hjust = -.07),\n                #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n                plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\ntuitsect_nonhbcu <-\n    tuitenr_8715_agg %>%\n    filter(hbcu == 2 & (year == 1990 | year == 2015)) %>%\n    #  filter(level == \"2yr\") %>% \n    ggplot(aes(year, mean_tuit_in, group = sector_desc)) +\n    geom_line(aes(color = sector_desc)) +\n    geom_point(aes(color = sector_desc)) +\n    scale_color_manual(values = c(\"Public 4yr\" = \"#999999\", \n                                                                \"Public 2yr\" = \"#E69F00\", \n                                                                \"Private nonprofit 2yr\" = \"#56B4E9\", \n                                                                \"Private nonprofit 4yr\" = \"#009E73\",\n                                                                \"Private for-profit 2yr\" = \"#0072B2\", \n                                                                \"Private for-profit 4yr\" = \"#CC79A7\")) +\n    geom_text_repel(data = tuitenr_8715_agg %>%\n                                        filter(hbcu == 2 & year == 2015),\n                                    aes(label = sector_desc), hjust = \"left\", nudge_x = .5, \n                                    direction = \"y\", size = 3) +\n    annotate(\"text\", x = 1992, y = 20000, label = \"not HBCUs\", size = 5, fontface = \"italic\") +\n    scale_x_continuous(breaks = c(1990, 2015)) +\n    scale_y_continuous(labels = scales::dollar_format(),\n                                         limits = c(0, 25200),\n                                         breaks = c(0, 5000, 10000, 15000, 20000, 25000)) +\n    labs(x = \"\", y = \"\") +\n    theme_light() +\n    theme(panel.grid.major = element_blank(), panel.grid.major.x = element_blank(), \n                panel.grid.minor = element_blank(), panel.grid.minor.y = element_blank(),\n                panel.background = element_blank(), \n                strip.background = element_rect(fill = \"#56B4E9\"),\n                panel.border     = element_blank(),  axis.text.y = element_text(size = 8), \n                legend.position = \"none\", axis.ticks       = element_blank(),\n                #plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n                plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\ntuitsec <- tuitsect_hbcu / tuitsect_nonhbcu  + plot_annotation(\n    title = 'Tuition & Fees at HBCUs Lower by Sector than non-HBCUs',\n    subtitle = 'Private non-profit 4-yr tuition increased more than other sectors',\n    caption = \"Source: Delta Cost Project\", \n    theme = theme(plot.subtitle = element_text(face = \"italic\", size = 9)))\ntuitsec\n\n\n\n\nFinally, let’s check out some year-over-year percent changes in enrollment & tuition, using the line graphs I love so much. Code is basic ggplot, so not worth showing here. If there’s anything of interest it was in adding a geom_hline call to add the light gray reference line at 0%.\nAs we’d seen in eariler charts, enrollment at HBCUs ping-ponged up and down. There was a bit less volatility in non-HBCUS, and in general enrollments didn’t decrease from 2000 onward.\nAt both HBCUs and non-HBCUs, public & private 4-years raised tuition & fees every year. The publics had a couple of instances of sharper increases, likely in response to recession-hit state budgets. The 2-year schools, especially 2-year HBCUs, had wider up & down swings in tuition.\n\n\nshow the faceted line graph code\ntuit_8715_pctchgh %>%\n  mutate(hbcu_f = case_when(hbcu == 1 ~ \"HBCU\", TRUE ~ \"Not HBCU\")) %>%\n  #  filter(hbcu == 1 & year >= 1990) %>% \n  filter(year >= 1990) %>%\n  filter(sector_desc %in% c(\"Public 4yr\", \"Private nonprofit 4yr\", \"Public 2yr\")) %>%\n  ggplot(aes(year, enr_tot)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  geom_hline(yintercept=0, color = \"gray\") +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(title = \"Year-over-year Percent Changes to UG Enrollment\",\n       subtitle = \"HBCU sectors had wider positive & negative swings\", \n       caption = \"Source: Delta Cost Project\", \n       x = \"\", y = \"Pct Chnage in-state tuition & fees\") +\n  facet_grid(hbcu_f ~ sector_desc) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nshow the faceted line graph code\n\ntuit_8715_pctchgh %>%\n  mutate(hbcu_f = case_when(hbcu == 1 ~ \"HBCU\", TRUE ~ \"Not HBCU\")) %>%\n  #  filter(hbcu == 1 & year >= 1990) %>% \n  filter(year >= 1990) %>%\n  filter(sector_desc %in% c(\"Public 4yr\", \"Private nonprofit 4yr\", \"Public 2yr\")) %>%\n  ggplot(aes(year, mean_tuit_in)) +\n  geom_line(color = \"#E69F00\", size = 1) +\n  geom_hline(yintercept=0, color = \"gray\") +\n  scale_color_manual(values = c(\"#56B4E9\", \"#E69F00\")) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(title = \"Year-over-year Percent Changes to Tuition & Fees\",\n       subtitle = \"More volatility in 2-year sector, steady increases in every sector\", \n       caption = \"Source: Delta Cost Project\", \n       x = \"\", y = \"Pct Chnage in-state tuition & fees\") +\n  facet_grid(hbcu_f ~ sector_desc) +\n  theme_light() +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), strip.background = element_rect(fill = \"#E69F00\"),\n        plot.subtitle = element_text(color = \"#E69F00\", face = \"italic\", size = 9),\n        plot.caption = element_text(color = \"#999999\", face = \"italic\", size = 7))\n\n\n\n\n\nThis was designed to be broad-scope, more-or-less exploratory analysis. Just looking for general trends, what might be worth a deeper dive. Like with the percent changes in tuition or enrollment…what was happening in the years with the wider + or - swings? The 2001 recession? The 2009/10 economic collapse? Which specific schools had more success with enrollment? Which HBCUs are the most & least expensive?\nAnd what other questions one could answer with the data I used here? Well for one, the tuition discount rates by sectors and HBCU/not HBCU would be an interesting thing to look at. Did HBCUs need to more aggressively discount tution to meet enrollment targets? The Delta Cost Project set includes a tuition discount variable. There’s a regression analysis in there somewhere, as well as other predictive analysis.\nI’d also want to look at which ethnic groups made up the growth in non-Black enrollments starting in 2010. Or was it that the changes to how ethnicity was recorded meant that students who used to be classified as Black become Hispanic or multi-ethnic. I know from my time in undergrad admissions at UC Berkeley that this accounted for a decline in the federal count of African-American students. However, when we compared the numbers with the UC definitions, we didn’t see a decline.\nTo get those numbers I could download IPEDS data directly from NCES or use r tools like the Urban Insitute’s educationdata package which is an API wrapper to that scrapes NCES and other websites for data.\nThere is a ton of higher education data out there, and it’s never been easier to get at scale than now. Trust me, as someone who has done individual downloads, ordered CD-ROMs, even used the late-great WebCASPAR, if you have an education policy or outcomes related question, there is publicly available data for analysis.\nCover image for post from GWU CCAS Graduate Blog\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/tidy-tuesday-nov-11-2020-hiking-trails-in-wash-state/index.html",
    "href": "posts/tidy-tuesday-nov-11-2020-hiking-trails-in-wash-state/index.html",
    "title": "Tidy Tuesday, November 24, 2020 - Hiking Trails in WA State",
    "section": "",
    "text": "Trail-head lake in the Cascade Mountains.\n\n\n\nDiving deep into the Tidy Tuesday pool…\nEvery week I’m so impressed by the amount of beautiful, creative and informative analyses and data viz efforts that spring from #TidyTuesday, the weekly data project that evolved out of the R4DS Learning Community. And every week I say to myself, “I should really give it a go, grab the data and post some results”. Then life intervenes, work gets crazy, and before I know it…\nWell, around Thanksgiving, with some slack time and some inspiration, I finally got around to it. A resulting benefit was it got me motivated to finally get around to getting this blog set up. My github repo has a folder for any tidytuesday projects I get around to.\nHere’s the code and resulting charts & tables.\n\n# load packages\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # stitch plots together\nlibrary(gt) # lets make tables\nlibrary(RColorBrewer) # colors!\nlibrary(scales) # format chart output\n\n\n\nFirst let’s read in the file using the tidytuesdayR package. We’ll also look at the raw data\n\ntt_watrail <- tt_load(\"2020-11-24\")\n#> \n#>  Downloading file 1 of 1: `hike_data.rds`\n\nglimpse(tt_watrail$hike_data)\n#> Rows: 1,958\n#> Columns: 8\n#> $ name        <chr> \"Lake Hills Greenbelt\", \"Snow Lake\", \"Skookum Flats\", \"Ten…\n#> $ location    <chr> \"Puget Sound and Islands -- Seattle-Tacoma Area\", \"Snoqual…\n#> $ length      <chr> \"2.3 miles, roundtrip\", \"7.2 miles, roundtrip\", \"7.8 miles…\n#> $ gain        <chr> \"50\", \"1800\", \"300\", \"1585\", \"500\", \"500\", \"425\", \"450\", \"…\n#> $ highpoint   <chr> \"330.0\", \"4400.0\", \"2550.0\", \"2370.0\", \"1000.0\", \"2200.0\",…\n#> $ rating      <chr> \"3.67\", \"4.16\", \"3.68\", \"3.92\", \"4.14\", \"3.14\", \"5.00\", \"2…\n#> $ features    <list> <\"Dogs allowed on leash\", \"Wildlife\", \"Good for kids\", \"L…\n#> $ description <chr> \"Hike through a pastoral area first settled and farmed in …\n\n\n\nThere are a few things we want to do with the data for the working dataframe:\n\ncreate columns for miles, direction, type from length\ncreate specific location columns frolm location\nchange rating, gain and highpoint to numeric\ncreate a rating group\nchange features to character vector, also unnest; makes the resulting df long. we’ll use distinct when we only need 1 obs per trail\n\n\ntt_watraildf <- tt_watrail$hike_data %>%\n  mutate(length_miles = parse_number(length)) %>%\n  mutate(across(gain:rating, as.numeric)) %>%\n  mutate(rating_grp = case_when(rating == 0 ~ \"0\",\n                                rating >0 & rating < 2 ~ \"1\",\n                                rating >=2 & rating < 3 ~ \"2\",\n                                rating >=3 & rating < 4 ~ \"3\",\n                                rating >=4 & rating < 5 ~ \"4\",\n                                rating == 5 ~ \"5\")) %>%\n  mutate(trail_type = case_when(grepl(\"roundtrip\", length) ~ \"Round trip\",\n                          grepl(\"one-way\", length) ~ \"One Way\",\n                          grepl(\"of trails\", length) ~ \"Trails\")) %>% \n  mutate(location_split = location) %>%\n  separate(location_split, c(\"location_region\",\"location_specific\"), sep = ' -- ') %>%\n  mutate(features = lapply(features, sort, na.last = TRUE)) %>%\n  mutate(feature_v = sapply(features,FUN = function(x) if (all(is.na(x))) NA else paste(x,collapse = \", \"))) %>%\n  mutate(feature_v = str_trim(feature_v)) %>%\n  mutate(features_unnest = features) %>%\n  unnest(cols = c(features_unnest), keep_empty = TRUE) %>% \n  mutate(feature_v = ifelse(is.na(feature_v), \"none\", feature_v)) %>%\n  mutate(features_unnest = ifelse(is.na(features_unnest), \"none\", features_unnest)) %>%\n  mutate(feature_init = case_when(features_unnest == \"Dogs allowed on leash\" ~ \"DA\",\n                                  features_unnest == \"Dogs not allowed\" ~ \"DN\",\n                                  features_unnest == \"Wildlife\" ~ \"Wl\",\n                                  features_unnest == \"Good for kids\" ~ \"GK\",\n                                  features_unnest == \"Lakes\" ~ \"Lk\",\n                                  features_unnest == \"Fall foliage\" ~ \"FF\",\n                                  features_unnest == \"Ridges/passes\" ~ \"RP\",\n                                  features_unnest == \"Established campsites\" ~ \"EC\",\n                                  features_unnest == \"Mountain views\" ~ \"MV\",\n                                  features_unnest == \"Old growth\" ~ \"OG\",\n                                  features_unnest == \"Waterfalls\" ~ \"Wf\",\n                                  features_unnest == \"Wildflowers/Meadows\" ~ \"WM\",\n                                  features_unnest == \"Rivers\" ~ \"Ri\",\n                                  features_unnest == \"Coast\" ~ \"Co\",\n                                  features_unnest == \"Summits\" ~ \"Su\")) %>%\n  mutate(feature_init = ifelse(is.na(feature_init), \"none\", feature_init)) %>%\n  mutate(feature_type = if_else(feature_init %in% c(\"DA\",\"DN\",\"GK\"), \"Companion\", \"Feature\")) %>%\n  mutate(feature_type = ifelse(feature_init == \"none\", \"none\", feature_type)) %>%\n  group_by(name) %>%\n  mutate(feature_n = n()) %>%\n  ungroup() %>%\n  mutate(feature_n = ifelse(feature_init == \"none\", 0, feature_n)) %>%\n  select(name, location_region, location_specific, trail_type, length_miles, \n         gain, highpoint, rating, rating_grp, features, feature_v, features_unnest, \n         feature_init, feature_type, feature_n, description, location, length)\n\n\n\nTo get a sense of what the data look like, I’ll run some historgrams and scatterplots to see how things cluster, if there are outliers or anything else especially noticable.\nUsing log10 for the length scale to even out the spread. The patchwork package stitches the plots together in a neat panel.\n\nhist_length <-\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles)) +\n  geom_histogram(alpha = 0.8) +\n  scale_x_log10() +\n  labs(x = \"Length (miles), log10\")\n\nhist_gain <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(gain)) +\n  geom_histogram(alpha = 0.8) +\n  scale_x_log10()\n\nhist_high <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(highpoint)) +\n  geom_histogram(alpha = 0.8) \n\nhist_rate <-\n  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(rating)) +\n  geom_histogram(alpha = 0.8) \n\n(hist_length | hist_gain) /\n  (hist_high | hist_rate)\n\n\n\n\nFor the scatterplots, I plotted length by gain, faceting by ratings groups and then by region. We do have to be careful with ratings, as they are user-generated and some trails have very few votes. Log10 used again for length.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles, gain)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"Length (miles) log10\", y = \"Total Gain\",\n       title = \"Length v Gain, by Rating Group\") +\n  facet_wrap(vars(rating_grp))\n\n\n\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(length_miles, gain)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10() +\n  labs(x = \"Length (miles) log 10\", y = \"Total Gain\",\n       title = \"Length v Gain, by Region\") +\n  facet_wrap(vars(location_region))\n\n\n\n\nThe outliers in terms of gain & length clustered in a few regions, so I wanted to see which they were. Not a surprise they clustered in the Cascades & Rainier.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  filter(gain > 15000) %>%\n  filter(length_miles > 90) %>%\n  select(location_region, name, length_miles, gain) %>%\n  arrange(name) \n#> # A tibble: 5 × 4\n#>   location_region      name                                   length_miles  gain\n#>   <chr>                <chr>                                         <dbl> <dbl>\n#> 1 Southwest Washington Pacific Crest Trail (PCT) Section H -…         148. 27996\n#> 2 South Cascades       Pacific Crest Trail (PCT) Section I -…          99  17771\n#> 3 Central Cascades     Pacific Crest Trail (PCT) Section K -…         117  26351\n#> 4 North Cascades       Pacific Northwest Trail - Pasayten Tr…         119  21071\n#> 5 Mount Rainier Area   Wonderland Trail                                93  22000\n\n\n\nNow that we see how the length, gain, highpoint & ratings spread out, I want build a table to see the averages by region.\nI’ve been wanting to take a deeper dive into gt & reactable. I’ve got some basic gt calls down, but for this excercise I wanted to learn how to conditionally format columns based on value. So inspired by Thomas Mock’s gt primer, a basic table with heatmap-like formatting for some columns. See his explainer for details on the code, and for more features than I’m including.\n\n# create by region averages df\nbyregion <-  tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  group_by(location_region) %>%\n  summarise(n_region = n(),\n            avglength = mean(length_miles),\n            avgrating = mean(rating),\n            avggain = mean(gain),\n            avghigh = mean(highpoint),\n            minhigh = min(highpoint),\n            maxhigh = max(highpoint)) %>%\n  mutate_at(vars(avglength:avgrating), round, 2) %>%\n  mutate_at(vars(avggain:avghigh), round, 0) \n\nbyregion %>%\n  gt() %>%\n  fmt_number(columns = vars(avggain, avghigh, minhigh, maxhigh), decimals = 0, use_seps = TRUE) %>%\n  # sets the columns and palette to format cell color by value range\n  data_color(\n    columns = vars(avglength, avgrating, avggain, avghigh, minhigh, maxhigh),\n    colors = scales::col_numeric(\n      palette = c(\"#ffffff\", \"#f2fbd2\", \"#c9ecb4\", \"#93d3ab\", \"#35b0ab\"),\n      domain = NULL)) %>%\n#  tab_style calls add border boxes first to column labels, then body cells\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_column_labels(\n        columns = gt::everything()\n        ))) %>%\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_body(\n        rows = gt::everything()\n      ))) %>%\n    cols_align(columns = TRUE, align = \"center\") %>%\n  cols_align(columns = \"location_region\", align = \"left\") %>%\n  cols_width(vars(location_region) ~ px(150),\n             vars(n_region) ~ px(60),\n             starts_with(\"avg\") ~ px(80),\n             ends_with(\"high\") ~ px(90)\n             ) %>%\n  tab_header(title = \"Regional Averages\",\n             subtitle = md(\"_North Cascades have longest trails,\n                           all mountain areas have lots of gain and highest points_\")) %>%\n  cols_label(location_region = \"Region\", n_region = \"N\", avglength = \"Avg Length (miles)\",\n            avgrating = \"Avg Rating\", avggain = \"Avg Gain (ft)\",avghigh = \"Avg high point\",\n             minhigh = \"Lowest high point\", maxhigh = \"Max high point\")\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Regional Averages\n    \n    \n      North Cascades have longest trails,\nall mountain areas have lots of gain and highest points\n    \n    \n      Region\n      N\n      Avg Length (miles)\n      Avg Rating\n      Avg Gain (ft)\n      Avg high point\n      Lowest high point\n      Max high point\n    \n  \n  \n    Central Cascades\n219\n9.53\n3.04\n2,276\n4,752\n600\n9,511\n    Central Washington\n79\n5.71\n2.78\n823\n2,260\n240\n6,876\n    Eastern Washington\n142\n9.33\n2.15\n1,592\n4,410\n300\n7,310\n    Issaquah Alps\n76\n5.03\n2.53\n984\n1,518\n250\n3,004\n    Mount Rainier Area\n193\n8.19\n3.35\n1,881\n5,222\n800\n10,080\n    North Cascades\n292\n11.24\n3.08\n2,535\n5,111\n125\n9,200\n    Olympic Peninsula\n209\n8.13\n3.32\n1,572\n2,821\n20\n6,988\n    Puget Sound and Islands\n190\n4.25\n2.80\n452\n573\n10\n3,750\n    Snoqualmie Region\n216\n8.71\n3.15\n2,198\n4,467\n450\n9,416\n    South Cascades\n188\n8.44\n3.07\n1,641\n4,732\n922\n12,276\n    Southwest Washington\n120\n6.58\n2.69\n1,171\n1,774\n20\n7,800\n  \n  \n  \n\n\n\n\n\n\nNow let’s look at the effect of trail features on rating.\nFirst we’ll look at average rating by feature, then fit a model. First, a scatter-plot of number of features listed for a trail with user rating. Looks like at a certain point, it’s diminshing returns on trail features in terms of effect on rating.\n\ntt_watraildf %>%\n  distinct(name, .keep_all = TRUE) %>%\n  ggplot(aes(feature_n, rating)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"# of features on a trail\", y = \"User rating\",\n       title = \"Features and Rating by Trail Region\") +\n  facet_wrap(vars(location_region))\n\n\n\n\nHere’s a table similar to the one for averages by region. I used the unnested features, so trails will be represented more than once. Dog-free trails do get the highest ratings, but it’s likely because they also tend to have highest high points, so offer views, are challenging, and so get good ratings.\n\nbyfeature <- \ntt_watraildf %>%\n  group_by(features_unnest) %>%\n  summarise(n_feature = n(),\n            avgrating = mean(rating),\n            avglength = mean(length_miles),\n            avggain = mean(gain),\n            avghigh = mean(highpoint),\n            minhigh = min(highpoint),\n            maxhigh = max(highpoint)) %>%\n  mutate_at(vars(avglength:avgrating), round, 2) %>%\n  mutate_at(vars(avggain:avghigh), round, 0) %>%\n  arrange(desc(avgrating))\n\n## create table\nbyfeature %>%\n  gt() %>%\n  fmt_number(columns = vars(n_feature, avggain, avghigh, minhigh, maxhigh), decimals = 0, use_seps = TRUE) %>%\n  # sets the columns and palette to format cell color by value range\n  data_color(\n    columns = vars(avglength, avgrating, avggain, avghigh, minhigh, maxhigh),\n    colors = scales::col_numeric(\n      palette = c(\"#ffffff\", \"#f2fbd2\", \"#c9ecb4\", \"#93d3ab\", \"#35b0ab\"),\n      domain = NULL)) %>%\n  # tab_style calls add border boxes first to column labels, then body cells\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_column_labels(\n        columns = gt::everything()\n      ))) %>%\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = \"all\", color = \"grey\", weight = px(1))),\n    locations = list(\n      cells_body(\n        rows = gt::everything()\n      ))) %>%\n  tab_header(title = \"Averages by Feature\",\n             subtitle = md(\"_Dog-free trails with waterfalls & high peaks earn high ratings_\")) %>%\n  cols_align(columns = TRUE, align = \"center\") %>%\n  cols_align(columns = \"features_unnest\", align = \"left\") %>%\n  cols_width(vars(features_unnest) ~ px(150),\n             vars(n_feature) ~ px(60),\n             starts_with(\"avg\") ~ px(80),\n             ends_with(\"high\") ~ px(90)\n             ) %>%\n  cols_label(features_unnest = \"Feature\", n_feature = \"N\", avglength = \"Avg Length (miles)\",\n             avgrating = \"Avg Rating\", avggain = \"Avg Gain (ft)\",avghigh = \"Avg high point\",\n             minhigh = \"Lowest high point\", maxhigh = \"Max high point\") \n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Averages by Feature\n    \n    \n      Dog-free trails with waterfalls & high peaks earn high ratings\n    \n    \n      Feature\n      N\n      Avg Rating\n      Avg Length (miles)\n      Avg Gain (ft)\n      Avg high point\n      Lowest high point\n      Max high point\n    \n  \n  \n    Dogs not allowed\n255\n3.52\n9.28\n1,921\n4,173\n10\n10,080\n    Waterfalls\n282\n3.46\n9.64\n1,938\n3,648\n150\n10,080\n    Established campsites\n396\n3.40\n12.80\n2,380\n4,487\n25\n12,276\n    Ridges/passes\n496\n3.25\n12.24\n2,864\n5,575\n400\n9,511\n    Lakes\n583\n3.19\n9.92\n1,988\n4,231\n20\n9,511\n    Old growth\n534\n3.16\n9.00\n1,746\n3,364\n25\n8,096\n    Mountain views\n1,175\n3.13\n9.72\n2,201\n4,621\n20\n12,276\n    Summits\n454\n3.11\n10.40\n2,854\n5,250\n200\n12,276\n    Wildflowers/Meadows\n952\n3.10\n9.26\n1,967\n4,243\n10\n9,511\n    Rivers\n547\n3.05\n9.76\n1,731\n3,205\n10\n12,276\n    Good for kids\n694\n3.01\n4.63\n569\n2,080\n10\n8,245\n    Wildlife\n747\n3.00\n8.84\n1,541\n3,241\n10\n10,080\n    Dogs allowed on leash\n1,045\n2.94\n7.04\n1,379\n3,183\n20\n12,276\n    Fall foliage\n508\n2.94\n8.28\n1,618\n3,334\n20\n9,249\n    Coast\n106\n2.89\n4.12\n351\n433\n10\n6,454\n    none\n68\n2.38\n7.26\n1,758\n3,849\n60\n8,970\n  \n  \n  \n\n\n\n\n\n\nAnd finally a quick model to see what might affect a trail rating.\nIt’s a simple linear model using length, gain, highpoint, & number of features to predict rating. The elevation of the highest point and number of features are both significant. I’d need to do more digging to see what the power of the estimate is on the rating. It’s also slightly counter-intuitive given that we saw in the charts that length, elevation and gain seem to positively affect rating. But then the model only accounts for 4% of varaince, so it’s not telling us much.\n\n# creat df with distinct observations for each trail \ntt_watraildf_dist <- tt_watraildf %>%\n  distinct(name, .keep_all = TRUE) \n\nwtmodel1 <- lm(rating ~ length_miles + gain + highpoint + feature_n, data = tt_watraildf_dist)\nsummary(wtmodel1)\n#> \n#> Call:\n#> lm(formula = rating ~ length_miles + gain + highpoint + feature_n, \n#>     data = tt_watraildf_dist)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.6984 -0.3776  0.3716  0.9284  2.4565 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2.205e+00  8.942e-02  24.663  < 2e-16 ***\n#> length_miles -6.565e-03  5.678e-03  -1.156    0.248    \n#> gain         -3.590e-05  3.100e-05  -1.158    0.247    \n#> highpoint     8.318e-05  1.742e-05   4.775 1.93e-06 ***\n#> feature_n     1.272e-01  1.484e-02   8.576  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.398 on 1919 degrees of freedom\n#> Multiple R-squared:  0.0488, Adjusted R-squared:  0.04682 \n#> F-statistic: 24.61 on 4 and 1919 DF,  p-value: < 2.2e-16\n\nThere’s plenty more to do with the set, and some responses I’ve seen on Twitter have been creative…network graphs, better models…but I was able to brush up on gt, learned how to unnest and keep obs where the list was empty. So a successful #tudytuesday.\nThis post was last updated on 2023-05-18"
  },
  {
    "objectID": "posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/index.html",
    "href": "posts/sad-songs-pretty-charts-a-gosta-berling-music-data-visualization/index.html",
    "title": "Sad Songs & Pretty Charts - a Gosta Berling Music Data Visualization",
    "section": "",
    "text": "For this post, I thought I’d focus on music analytics, given that music and data science/analysis are two things I’ve spent most of my waking hours doing for a number of years now.\nOver the years I’ve made a lot of music in a number of different projects. For most of my time living in the Bay Area I’ve played with some friends in a band called Gosta Berling. We’ve released two EPs and a full album (click on the album covers to give listen)\n  \nOur sound could be called melancholy mood-pop. We like melody, but we were raised on brooding post-punk so a minor key vibe is definitely present. The Spotify API has musical features including danceability, energy, and valence (what they call ‘happiness’). I used Charlie Thompson’s spotifyr package to see how we score. spotifyr has a bunch of functions designed to make it easier to navigate Spotify’s JSON data structure.\nOne quick thing…I’m using Spotify data so in effect validating Spotify. While I appreciate the availability of the data for projects like this, Spotify needs to do much better by way of paying artists. We don’t have tons of streams, but as you can see from this account report… \n…artists get f$ck-all per stream. So sure, use Spotify, it’s a great tool for discovering new music. And while artists pressure them to pay more per stream, you can help by purchasing music from artists you like. The pandemic has killed touring income, so sales are all that many artists have to support themselves. Help them out, buy the music you like. Especially if they’re on Bandcamp and you buy 1st Fridays, when Bandcamp waives their revenue share, meaning the artist gets every penny. Did I mention you can buy our music on Bandcamp? :)\nAnyway, soapbox off…first thing, let’s load the packages we’ll be using:\n\n# load packages\nlibrary(spotifyr) # pull data from spotify\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(janitor) # tools for data cleaning\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot text\n\nlibrary(httr)\nlibrary(stringr) # work with string data\nlibrary(lubridate) # work with dates\nlibrary(GGally) # correlation plots\nlibrary(PerformanceAnalytics) # correlation plots\nlibrary(corrr)  # correlation plots\n\nTo get access the Spotify data, you need a developer key. Charlie’s explained how to do it on the package page, so I won’t repeat that here. To set up the keys in your .Renviron, run usethis::edit_r_environ() and add (where the xs are your codes):\n\nSPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx'\nSPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx'\n\n# or do\nSys.setenv(SPOTIFY_CLIENT_ID = 'xxxxxxxxxxxxxxxxxxxxx')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'xxxxxxxxxxxxxxxxxxxxx')\n\nThis call sets your access token for the data session\n\n\n\nIf you run into redirect issues, see this stackoverflow thread, specifically this comment\nFirst thing is to search the artist data for audio features. I’m pulling in everything into a dataframe. Now initially I had a filter for artist = 'Gosta Berling'. But that was also pulling in data from a Swedish prog-metal band Gösta Berling’s Saga. So I needed to filter on our artist ID and pull in albums and singles (for some reason our EPs were listed as singles, but whatever)\n\n# gets full range of information for tracks from artist\ngosta_audio1 <- get_artist_audio_features(artist = \"4Vb2yqJJthJTAZxKz4Aryn\", include_groups = c(\"album\", \"single\"))\n\nOh…why more than one band with Gosta Berling in their name? Well, The Saga of Gösta Berling was Greta Garbo’s first feature-length film, and based on an old Swedish novel Gösta Berling’s Saga about a drunkard outcast priest seeking redemption. When, like our band, you’re a bunch of movie nerds, and a couple of you are especially obsessed with silent films & old Hollywood, you name your band Gosta Berling. And so does a Swedish band…anyway…more about the data.\nThe code here gets a dataframe for each record. I also needed to add album titles. Next steps were to merge the album dataframes together, extract the song IDs and pass them to the get_track_features() function as a list.\n\n# get album tracks, add album name could merge on other df, easier to quick fix this way\ntravel <- get_album_tracks(id = \"0vBs7ZtBj3ROrRyac3M47q\")\ntravel$album <- \"Travel\"\nsweetheart <- get_album_tracks(id = \"0dJBaJ3VFxOtdG5L9yzALJ\")\nsweetheart$album <- \"Everybody's Sweetheart\"\nwinterland  <- get_album_tracks(id = \"6CMekiY6lCIuBZpzFDInpf\")\nwinterland$album <- \"Winterland\"\n\n# merge album files, output track ids to use for audio features\ngbtracks <- data.table::rbindlist(list(sweetheart, travel, winterland))\n#copy result from console to paste as vector below\ngbtrackids <- dput(as.character(gbtracks$id)) \n\ngosta_audio2 <- \n  get_track_audio_features(c(\"2SotrXjkvjTZf05XSMKGyp\", \"07cTJ65GZ4Lvr6b1CtgPll\", \"4ooz79IN3la97See8IMNRL\", \"7pgCh68iFO0LNUNKWTFFIP\", \"4ZCesDRgGWKEXwq8iKw5FB\", \"4ZdH5B3tijHjWiwyOErgtf\", \"5GWKeBYgOsv3PKutDIQoet\", \"0XXWRsY6URe2Vx7Bxs6k06\", \"0t3AGVXHyF3dEYuhvAYuNz\", \"4ObsuwrVLKUq5aF8whrFqk\", \"0PnjWfIPwsqBtllMILjzxB\", \n\"7uQtlGsKxXOzsSapKTZRFU\", \"3kQuG44stzA3pQf7g61Ipt\", \n\"0YH9wkimhRhCmstNZyxPgO\", \"7rEbjyNO0dTEK6x8HkLqAz\", \"4VgEAtVQtkwIHzKMOROk6X\", \"5R9M4s6QZljNPVVzxoy98h\", \"1FNtHQ0juoKg2yCf9u4VSg\", \"5NWmfmupE7FEJ9O1e9vizu\"),\nauthorization = get_spotify_access_token())\n\nThis gets a dataframe with most of what I want…just a few tweaks needed. First, since they weren’t pulled from the get_track_audio_features() call, I used the track id, name, and album track number from the gbtracks dataframe. Also, because the song key returned as only the numeric value, I created the letter name and mode (major or minor), and ordered the columns.\n\n# get track number and name, merge from gbtracks -\n# need b/c these fields not returned from get_track_audio_features()\ngbtrack2 <- gbtracks %>%\n  select(id, name, album, track_number) %>%\n  rename(track_name = name)\n\n# merge to complete df. add names for key and mode\ngosta_audio <- left_join(gosta_audio2, gbtrack2) %>%\n  mutate(key_name = case_when(key == 0 ~ \"C\", key == 2 ~ \"D\", key == 4 ~ \"E\", key == 5 ~ \"F\",\n                              key == 7 ~ \"G\", key == 9 ~ \"A\", key == 11 ~ \"B\")) %>%\n  mutate(mode_name = case_when(mode == 0 ~ \"Minor\", mode == 1 ~ \"Major\")) %>%\n  mutate(key_mode = paste(key_name, mode_name, sep = \" \")) %>%\n  rename(track_id = id) %>%\n  select(album, track_name, track_number, key_mode, time_signature, duration_ms, \n         danceability, energy, loudness, tempo, valence, \n         acousticness, instrumentalness, liveness, speechiness,\n         key_name, mode_name, key, mode)\n\nOk, we’ve got a nice tidy dataframe, let’s do some analysis & visualization!\nSpotify’s developer pages have good explanations of the data. Some notes from spotify here about elements:\n\nMost of the audio features are 0-1, 1 being highest. e.g. higher speechiness = higher ratio of words::music. Valence is “happiness”, where higher = happier.\nLoundess in dB, tempo is BPM.\n\nSo let’s look at a quick summary of the audio features for our songs.\n\n#>   duration_ms      danceability        energy          loudness      \n#>  Min.   : 75933   Min.   :0.2500   Min.   :0.0476   Min.   :-21.350  \n#>  1st Qu.:295380   1st Qu.:0.3545   1st Qu.:0.3260   1st Qu.:-12.031  \n#>  Median :350053   Median :0.3920   Median :0.5190   Median : -9.943  \n#>  Mean   :334762   Mean   :0.4105   Mean   :0.5233   Mean   :-10.705  \n#>  3rd Qu.:385634   3rd Qu.:0.4820   3rd Qu.:0.7160   3rd Qu.: -7.537  \n#>  Max.   :522760   Max.   :0.5730   Max.   :0.9360   Max.   : -6.014  \n#>      tempo           valence        acousticness     instrumentalness \n#>  Min.   : 82.15   Min.   :0.0349   Min.   :0.00371   Min.   :0.00881  \n#>  1st Qu.:116.51   1st Qu.:0.1620   1st Qu.:0.12920   1st Qu.:0.50800  \n#>  Median :141.83   Median :0.2940   Median :0.39300   Median :0.69800  \n#>  Mean   :131.06   Mean   :0.3105   Mean   :0.41332   Mean   :0.62883  \n#>  3rd Qu.:149.98   3rd Qu.:0.4405   3rd Qu.:0.63750   3rd Qu.:0.84450  \n#>  Max.   :166.01   Max.   :0.6960   Max.   :0.88600   Max.   :0.94400  \n#>     liveness       speechiness     \n#>  Min.   :0.0703   Min.   :0.02540  \n#>  1st Qu.:0.1020   1st Qu.:0.02810  \n#>  Median :0.1160   Median :0.03060  \n#>  Mean   :0.1333   Mean   :0.03699  \n#>  3rd Qu.:0.1265   3rd Qu.:0.03865  \n#>  Max.   :0.3300   Max.   :0.11600\n\nFirst I wanted to look at basic correlations for the values. There are a number of ways to run and visualize correlations in r…a few examples follow. First thing I needed to do was a subset of the gosta_audio df for easier calls with the various correlation packages.\n\n\n\nLet’s try correlations in base r. You get the coefficients in the console or you can output to a dataframe to hard-code the visualization.\n\ncor(gbcorr)\n#>                  duration_ms danceability      energy    loudness      tempo\n#> duration_ms       1.00000000   0.03575546 -0.09957649  0.16485951 -0.1589364\n#> danceability      0.03575546   1.00000000 -0.10466026  0.09671649 -0.2719148\n#> energy           -0.09957649  -0.10466026  1.00000000  0.85748849  0.5140085\n#> loudness          0.16485951   0.09671649  0.85748849  1.00000000  0.4952005\n#> tempo            -0.15893636  -0.27191484  0.51400852  0.49520052  1.0000000\n#> valence          -0.04414383  -0.10232090  0.72025346  0.48053791  0.5519247\n#> acousticness     -0.19009855   0.11222116 -0.74742026 -0.65043898 -0.3612391\n#> instrumentalness  0.12784620   0.06977532 -0.53088295 -0.49709651 -0.4411810\n#> liveness         -0.30987073  -0.25213421  0.49374017  0.30054882  0.5316901\n#> speechiness      -0.30678610  -0.31639826  0.45449667  0.27298422  0.4217976\n#>                      valence acousticness instrumentalness   liveness\n#> duration_ms      -0.04414383   -0.1900986       0.12784620 -0.3098707\n#> danceability     -0.10232090    0.1122212       0.06977532 -0.2521342\n#> energy            0.72025346   -0.7474203      -0.53088295  0.4937402\n#> loudness          0.48053791   -0.6504390      -0.49709651  0.3005488\n#> tempo             0.55192475   -0.3612391      -0.44118097  0.5316901\n#> valence           1.00000000   -0.7793878      -0.29646550  0.4743309\n#> acousticness     -0.77938779    1.0000000       0.39266796 -0.3261889\n#> instrumentalness -0.29646550    0.3926680       1.00000000 -0.3406087\n#> liveness          0.47433091   -0.3261889      -0.34060869  1.0000000\n#> speechiness       0.41684028   -0.3150009      -0.56643572  0.7459700\n#>                  speechiness\n#> duration_ms       -0.3067861\n#> danceability      -0.3163983\n#> energy             0.4544967\n#> loudness           0.2729842\n#> tempo              0.4217976\n#> valence            0.4168403\n#> acousticness      -0.3150009\n#> instrumentalness  -0.5664357\n#> liveness           0.7459700\n#> speechiness        1.0000000\ngbcorrs1 <- as.data.frame(cor(gbcorr))\n\nOr you could let some packages do the viz work for you. First, the GGally package, which returns a nice matrix visualization that shows which fields are most postively and negatively correlated.\n\nggcorr(gbcorr, label = TRUE)\n\n\n\n\nWe see here some strong postive associations with energy::loundess returning a .9 coefficient, and liveness::speechiness and energy::valence each returning at .7 coefficient. The energy::acousticness and loudness::acousticness combinations each return a -.7 coefficient, showing a negative relationship between those music features.\nWith the corrr package I tried a couple of approaches. First a basic matrix that prints to the console, and doesn’t look much different than base r.\n\ngbcorr %>%\n  correlate(use = \"pairwise.complete.obs\", method = \"spearman\")\n#> # A tibble: 10 × 11\n#>    term     duration_ms danceability energy loudness  tempo valence acousticness\n#>    <chr>          <dbl>        <dbl>  <dbl>    <dbl>  <dbl>   <dbl>        <dbl>\n#>  1 duratio…     NA            0.0799 -0.319   -0.189 -0.439  -0.191      -0.0737\n#>  2 danceab…      0.0799      NA      -0.269   -0.124 -0.323  -0.209       0.128 \n#>  3 energy       -0.319       -0.269  NA        0.872  0.658   0.761      -0.725 \n#>  4 loudness     -0.189       -0.124   0.872   NA      0.574   0.458      -0.595 \n#>  5 tempo        -0.439       -0.323   0.658    0.574 NA       0.665      -0.479 \n#>  6 valence      -0.191       -0.209   0.761    0.458  0.665  NA          -0.770 \n#>  7 acousti…     -0.0737       0.128  -0.725   -0.595 -0.479  -0.770      NA     \n#>  8 instrum…      0.135        0.0333 -0.447   -0.586 -0.416  -0.177       0.339 \n#>  9 liveness     -0.319       -0.321   0.319    0.144  0.479   0.488      -0.103 \n#> 10 speechi…     -0.331       -0.715   0.382    0.283  0.640   0.396      -0.209 \n#> # ℹ 3 more variables: instrumentalness <dbl>, liveness <dbl>, speechiness <dbl>\n\nNext, I used their rplot call and then rendered a network graph using the network_plot() call.\n\ngbcorrs2 <- correlate(gbcorr)\nrplot(gbcorrs2)\n\n\n\n   # network graph\ncorrelate(gbcorr) %>% \n  network_plot(min_cor=0.5)\n\n\n\n\nAnd finally the `performance analytics’ package, which was the first of the packages to include significance levels in the default output.\n\n\n\n\n\nGiven the correlations, I was interested in exploring the relationships a bit more. So I ran a few scatterplots, with song titles as data labels, and dots colored by album name (using primary color from the cover) to see also if any of the albums clustered at all along either axis. The ggrepel package is used to move the labels off of the dots.\nThere is a bit of a relationship between the Energy score and Valence - so our more energetic songs are our happiest songs. Another interesting way to explore this would be to do some sentiment analysis on the lyics and see if there’s a relationship between energy, valence and using words considered to be more positive in nature. That’s a project on my to-do list.\n\ngosta_audio %>%\n  ggplot(aes(energy, valence, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) +\n  ylim(0, 1) +\n  xlim(0, 1) +\n  theme_minimal() +\n  labs(x = \"energy\", y = \"valence (happiness)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\nNext I wondered if there’s a relationship between song tempo (beats per minute) & happiness. Our average BPM is 131, which isn’t too far the the mid-range of songs on Spotify. The histogram below used to be on the Spotify API page but they don’t seem to have it up anywhere anymore, so found it via the Wayback Machine\nSo let’s see the resulting scatterplot…\n\n\nshow tempo x valence scatterpplot code\ngosta_audio %>%\n  ggplot(aes(tempo, valence, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) + \n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"tempo (bpm)\", y = \"valence (happiness)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\nIt’s not until we get to about the 130 BPM range is it that our songs start to get to even a .25 valence (happiness) score, and from there the relationship between tempo & happiness really kicks in.\nFinally, tempo and energy…\n\n\nshow tempo x energy scatterpplot code\ngosta_audio %>%\n  ggplot(aes(tempo, energy, color = album)) +\n  geom_point() +\n  geom_smooth(aes(color = NULL)) +\n  geom_text_repel(aes(label = track_name), size = 3) +\n  scale_color_manual(values = c(\"#707070\", \"brown\", \"dark blue\")) +\n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"tempo (bpm)\", y = \"energy\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n\n\n\n\n\nSo yes, most of our songs are in the bottom half of the happy scale. And there does seem to be a bit of a relationship between tempo, energy and happiness and of course a relationship between tempo and energy. Going forward, I’d love to explore our song lyrics via text analysis, especially sentiment analysis to see if the songs Spotify classified as our most sad (low valence) had lyrics that were less positive.\nSo if you like slightly melancholy mood-pop that’s in the 130 +/- BPM range (think The National, Radiohead), I think you’ll like us.\nThanks for reading. And again, give us a listen, and maybe buy some music if you like. :)\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/invited-talk-usf-feb2020/index.html",
    "href": "posts/invited-talk-usf-feb2020/index.html",
    "title": "Invited Talk at University of San Francisco, February 2020",
    "section": "",
    "text": "In early 2020 (back in the days of in-person gatherings) I was invited to give a talk at two budget forums at the University of San Francisco. The general theme was looking at the landscape of enrollments in higher education, with a specific focus on liberal arts colleges, especially Jesuit colleges. Because I collected data from a variety of sources and did much of the work in r, I thought it would make for a good data blog post. Plus, like the Tidy Tuesday HBCU enrollment post, it’s about higher education, which has been my area of professional expertise for a while now.\nI structured the talk around these general questions:\n\nWhat are the major trends affecting the higher education landscape in the US today, particularly traditional liberal arts colleges?\n\nChanging demographics impacting enrollments\nAffordability\n\nWhat social/political threats are on the horizon that colleges should start addressing?\n\nThe talk was divided into four segments:\n\nThe high school graduation picture in California (USF gets most of its students from CA)\nHistorical enrollment at USF, and compared to other Jesuit colleges\nCollege Costs and Affordability\n\nWhat social/political threats are on the horizon that colleges should start addressing?\n\nThe data, code and resulting presentation are in this github repo. What I plan to do in this post is in effect annotate some of the code to explain how I put everything together. And of course to show some charts & tables.\nFirst up is pulling in a few decades of high school graduation and enrollment data and wrangling it all to show historical enrollment and projections through 2029. The full code for that is at the github repo in the file 01_hs enrollment data.R. So what did I do?\nFirst, loaded some packages:\n\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # to stitch together plots\nlibrary(ggtext) # helper functions for ggplot text\nlibrary(ggrepel) # helper functions for ggplot\nlibrary(rCAEDDATA) # aggregated CA HS data\nlibrary(readr) # read in flat files\nlibrary(janitor) # data munging/cleaning utilities\n\nThe rCAEDDATA package was put together by my former SFSU IR colleague David Ranzolin. Though the last update was October 2017, it still contains a trove of data, including public HS graduation numbers from 1993 to 2016. That said, I didn’t actually use the package functions, just downloaded the data included with the package. For later years I manually downloaded files from the CA Department of Education’s data pages: 2017 here, and the later years here. The 2017 file has the same structure as the files in David’s package. Later years need a bit of restructuring:\n\ncahsgrad18 <- read.delim(\"data/cahsgrad18.txt\", stringsAsFactors=FALSE) %>%\n    clean_names() %>% \n    filter(reporting_category == \"TA\") %>%\n    filter(aggregate_level == \"S\") %>%\n    filter(dass == \"All\") %>%\n    filter(charter_school == \"All\") %>%\n    mutate(YEAR = \"2018\") %>%\n    mutate(YEAR = factor(YEAR)) %>%\n    mutate_at(vars(ends_with(\"_code\")), as.character) %>%\n    mutate(county_code = ifelse(nchar(county_code) == 1, \n                str_pad(county_code, 2, \"left\", \"0\"), county_code)) %>%\n    mutate(CDS_CODE = paste(county_code, district_code, school_code, sep = \"\")) %>%\n    mutate(GRADS = as.integer(ifelse(regular_hs_diploma_graduates_count == \"*\", \n                0, regular_hs_diploma_graduates_count))) %>%\n    mutate(UC_GRADS = as.integer(ifelse(met_uc_csu_grad_req_s_count == \"*\", \n                0, met_uc_csu_grad_req_s_count))) %>%\n    select(CDS_CODE, GRADS, UC_GRADS, YEAR) %>%\n    group_by(YEAR) %>%\n    summarise(total_grads = sum(GRADS),\n                        uccsu = sum(UC_GRADS),\n                        notuccsu = total_grads - uccsu)\n\nNext, some projected HS graduation data from the CA Department of Finance. I did a quick transposing of the “HS Grads Table” tab in excel and used that to read into r. You can see the file I used at the github repo’s data folder.\n\ngrproj_to2028 <- readxl::read_excel(\"data/capublic_k12_enrollproj_to2028.xlsx\",\n                                sheet = \"hsgrads-tr\") %>%\n    filter(year != \"2017-18\") %>%\n    mutate(yearend = str_sub(year, 6, 7)) %>%\n    mutate(YEAR = paste(\"20\", yearend, sep = \"\")) %>%\n    #mutate(YEAR = factor(year_ch)) %>%\n    mutate(uccsu = as.integer(NA)) %>%\n    mutate(notuccsu = as.integer(NA)) %>%\n    mutate(notuccsu = as.integer(NA)) %>%\n    mutate(type = \"Projected\") %>%\n    select(YEAR, total_grads = total, uccsu, notuccsu, type) %>%\n    # amend 2018-19 with actual results from\n    # https://dq.cde.ca.gov/dataquest/dqcensus/CohRateLevels.aspx?cds=00&agglevel=state&year=2018-19\n    mutate(total_grads = ifelse(YEAR == \"2019\", 417496, total_grads)) %>%\n    mutate(uccsu = ifelse(YEAR == \"2019\", 210980, uccsu)) %>%\n    mutate(notuccsu = ifelse(YEAR == \"2019\", total_grads - uccsu, notuccsu)) %>%\n    mutate(type = ifelse(YEAR == \"2019\", \"Actual\", type))\n\nThe projected HS grad stats didn’t have values for UC/CSU grads, so I needed to impute that as part of the merging of the actual & projected files. I also calculated year-over-year percent changes for a few fields. (though I did it manually not with a function like in the Tidy Tuesday HBCU post). The code below gets us the cahsgrads_1993_2028 dataframe.\n\ncahsgrads_1993_2028 <- rbind(cahsgrad93to18_tot, grproj_to2028) %>%\n    mutate(pctucgrads = uccsu / total_grads) %>%\n    arrange(YEAR) %>%\n    # add projected uccsu grads based on constant 2017-18 to 2018-19 increase 0.0061437\n    mutate(pctucgrads = ifelse(YEAR >= \"2020\", lag(pctucgrads) + 0.0061437, pctucgrads)) %>%\n    mutate(pctucgrads = ifelse(YEAR == \"2021\", lag(pctucgrads) + 0.0061437, pctucgrads)) %>%\n    mutate(pctucgrads = ifelse(YEAR == \"2022\", lag(pctucgrads) + 0.0061437, pctucgrads)) %>%\n    mutate(pctucgrads = ifelse(YEAR == \"2023\", lag(pctucgrads) + 0.0061437, pctucgrads)) %>%\n    mutate(pctucgrads = ifelse(YEAR == \"2024\", lag(pctucgrads) + 0.0061437, pctucgrads)) %>%\n    mutate(pctucgrads = ifelse(YEAR == \"2025\", lag(pctucgrads) + 0.0061437, pctucgrads)) %>%\n    mutate(pctucgrads = ifelse(YEAR == \"2026\", lag(pctucgrads) + 0.0061437, pctucgrads)) %>%\n    mutate(pctucgrads = ifelse(YEAR == \"2027\", lag(pctucgrads) + 0.0061437, pctucgrads)) %>%\n    mutate(pctucgrads = ifelse(YEAR == \"2028\", lag(pctucgrads) + 0.0061437, pctucgrads)) %>%\n    mutate(pctucgrads = ifelse(YEAR == \"2029\", lag(pctucgrads) + 0.0061437, pctucgrads)) %>%\n    mutate(uccsu = ifelse(type == \"Projected\", round(pctucgrads * total_grads, 0), uccsu)) %>%\n    mutate(notuccsu = ifelse(type == \"Projected\", round(total_grads -uccsu, 0), notuccsu)) %>%\n    mutate(gr_tot_change = (total_grads - lag(total_grads))) %>%\n    mutate(gr_tot_pct_change = (total_grads/lag(total_grads)- 1)) %>%\n    mutate(gr_uc_change = (uccsu - lag(uccsu))) %>%\n    mutate(gr_uc_pct_change = (uccsu/lag(uccsu) - 1)) %>%\n    mutate(gr_notuc_change = (notuccsu - lag(notuccsu))) %>%\n    mutate(gr_notuc_pct_change = (notuccsu/lag(notuccsu) - 1)) %>%\n    select(YEAR, total_grads, uccsu, notuccsu, type, pctucgrads, type, everything())\n\ncahsgrads_1993_2028 <- cahsgrads_1993_2028 %>%\n    mutate(pctucgrads = ifelse(year_ch >= \"9293\", uccsu / total_grads, pctucgrads))\n\nNow that we have the data, let’s make the chart I presented to the group, showing actual & projected high school grads in California, breaking out the UC/CSU eligible grads.\nWhat does the chart tell us? Well…\n\n67% increase in grads from 1993 to 2018\nUC/CSU eligibility 33% in 1993, 50% in 2018\nGrads expected to peak in 2023, then decline slightly\n\nBut these assumptions were all pre-COVID - I gave the talks in early February of 2020. Given factors such as migration patterns within the state, people moving out of CA, parents moving their kids to private schools, etc., the actual graduation picture is sure to change.\n\n\nshow the enrollment charts code\ncahsgrads_1993_2028 %>%\n    select(YEAR, uccsu, notuccsu) %>%\n    pivot_longer(-YEAR, names_to = \"ucelig\", values_to = \"n\") %>%\n    ggplot(aes(YEAR, n, fill = rev(ucelig))) +\n    geom_bar(stat = \"identity\", color = \"black\") +\n    geom_segment(aes(x = 27.5, y = 0, xend = 27.5, yend = 500000),\n                             size = 2, color = \"grey\") +\n    scale_y_continuous(labels = scales::comma, limits = c(0, 500000)) + \n    scale_x_discrete(breaks = c(\"9293\", \"9798\", \"0203\", \"0708\", \"1213\", \"1718\", \"2223\", \"2829\")) +\n    scale_fill_manual(values = c(\"#1295D8\", \"white\"),\n                                        labels = c(\"UC CSU Eligible\", \"Not UC/CSU Elig\")) +\n    labs(x = \"\", y = \"\", caption = \"Sources: Actual: CA Dept of Education. Projections: CA Dept of Finance\",\n             fill = \"UC/CSU Eligible?\") +\n    annotate(\"text\", x = 28, y = 500000, label = \"Projected\",\n                     size = 6, fontface = \"italic\", hjust = -.25) +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(),   panel.grid.minor = element_blank(),\n                legend.position = c(.2, .9),\n                legend.title=element_text(size=12), legend.text=element_text(size=12),\n                axis.text.x = element_text(size = 7))\n\n\n\n\n\nNext I presented some data on USF enrollment and compared USF to their Jesuit college peers. For that I used two sources: data from the Delta Cost Project (DCP), and since DCP stops at 2015, downloaded some files directly from IPEDS and read in the CSVs. I could also have used r tools like the Urban Insitute’s educationdata package an API wrapper to that scrapes NCES and other websites for data. (I also referenced that package in my HBCU post).\nThe import code isn’t all that challenging - read in the CSV, select the fields I needed, do a bit of basic cleaning. So no need to show it. You can see it in the github repo - go to the file ’02_ipeds_enroll.R`. Though if anything’s worth highlighting it’s the need to create an object of IPEDS unitids for Jesuit colleges so I could group them during analysis. The Jesuit colleges include names you know: Georgetown, Gonazaga, Boston College, the Loyolas (Chicago, LA, New Orleans, Baltimore), etc…\n\njesids <- c(\"164924\", \"181002\", \"186432\", \"122931\", \"169716\", \"159656\", \"127918\", \"192323\", \n                    \"163046\", \"122612\", \"236595\", \"239105\", \"203368\", \"179159\", \"215770\", \n                        \"215929\", \"131496\", \"166124\", \"102234\", \"117946\", \"206622\", \"102234\", \n                        \"166124\", \"117946\", \"206622\", \"235316\", \"129242\")\n\nFirst up is USF enrollment from Fall 1987 to Fall 2018 (the latest year that IPEDS had available as of February 2020). The ggplot code is mostly basic, so I’ve folded it…click the arrow to show the code. It’s mostly worth checking out for this neat solution to a crowded x axis - the every_nth function to count every n value, and apply it to the breaks - in this case I set it to n=3. I’d tried scales::pretty_breaks() but it didn’t work. I also used ggrepel to move the labels a bit.\nWhat’s the enrollment picture at USF? Well, this chart tells us that:\n\nUndergraduate enrollment have increased by 60% since 1987.\nGraduate enrollments hovering around 3,500 for a number of years, and up to +/- 4,000 since 2016.\nRatio of undergraduate::graduate enrollments steady over time, generally +/- 2% points from 60%.\n\nHow does this relate to the high school graduation trends in CA & nearby states?\n\nWith 63% of new students coming from California, high school enrollments here will have most impact.\nWestern Interstate Commission for Higher Education (WICHE) projects HS grads from all western states to peak in 2024 at 862,000, then decline for a few years, rebounding again around 2032. (Knocking at the College Door, https://knocking.wiche.edu)\nDuring 1980s the Gen X population drop mitigated by increased college-going rates – what will happen this time?\n\n\n\nshow the enrollment charts code\nevery_nth = function(n) {\n  return(function(x) {x[c(TRUE, rep(FALSE, n - 1))]})\n}\n\nplot_usfenr_ug <-\n    ipeds_fallenroll_8718 %>%\n    filter(UNITID == \"122612\", level == \"Undergraduate\") %>%\n    select(year, level, tot_enr) %>%\n    ggplot(aes(year, tot_enr)) +\n    geom_bar(stat = \"identity\", fill = \"#00543C\") +\n    # geom_text(aes(label = scales::comma(round(tot_enr), accuracy = 1)), \n    #                   color = \"#919194\", vjust = -.75, size = 3) +\n    geom_text_repel(data = ipeds_fallenroll_8718 %>%\n                                        filter(UNITID == \"122612\", level == \"Undergraduate\",\n                                        year %in% c(\"Fall 1987\", \"Fall 1990\", \"Fall 1990\", \n                                        \"Fall 1993\", \"Fall 1996\", \"Fall 1999\",\n                                        \"Fall 2002\", \"Fall 2005\", \"Fall 2008\", \"Fall 2011\", \n                                        \"Fall 2014\", \"Fall 2017\", \"Fall 2018\")),\n                                    aes(label = scales::comma(round(tot_enr), \n                                            accuracy = 1)), nudge_y = 400,\n                                        min.segment.length = 0,\n                                        size = 3, color = \"#919194\") +\n    scale_x_discrete(breaks = every_nth(n = 3)) +\n    scale_y_continuous(label = scales::comma, limits = c(0, 7000),\n                                         breaks = c(0, 1750, 3500, 5250, 7000)) +\n    labs(x = \"\", y = \"\") +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(),   panel.grid.minor = element_blank(),\n                axis.text.y = element_text(size = 10))\n\nplot_usfenr_gr <-\n    ipeds_fallenroll_8718 %>%\n    filter(UNITID == \"122612\", level == \"Graduate\") %>%\n    select(year, level, tot_enr) %>%\n    ggplot(aes(year, tot_enr)) +\n    geom_bar(stat = \"identity\", fill = \"#FDBB30\") +\n    geom_text_repel(data = ipeds_fallenroll_8718 %>%\n                                        filter(UNITID == \"122612\", level == \"Graduate\",\n                                     year %in% c(\"Fall 1987\", \"Fall 1990\", \"Fall 1990\", \n                                   \"Fall 1993\", \"Fall 1996\", \"Fall 1999\",\n                                   \"Fall 2002\", \"Fall 2005\", \"Fall 2008\", \"Fall 2011\", \n                                   \"Fall 2014\", \"Fall 2017\", \"Fall 2018\")),\n                                    aes(label = scales::comma(round(tot_enr), \n                                                            accuracy = 1)), nudge_y = 200,\n                                    min.segment.length = 0,\n                                    size = 3, color = \"#919194\") +\n    scale_x_discrete(breaks = every_nth(n = 3)) +\n    scale_y_continuous(label = scales::comma, limits = c(0, 4500),\n                                         breaks = c(0, 1500, 3000, 4500)) +\n    labs(x = \"\", y = \"\") +\n    theme_minimal() +\n    theme(panel.grid.major = element_blank(),   panel.grid.minor = element_blank(),\n                axis.text.y = element_text(size = 10))\n\nplot_usfenr_all <- plot_usfenr_ug + plot_usfenr_gr +\n    plot_layout(ncol = 1) + plot_annotation(\n    title = 'Fall Enrollment at USF: 1987 - 2018',\n    subtitle = \"<span style = 'color:#00543C;'>Green = Undergraduate</span>, \n    <span style = 'color:#FDBB30;'>Gold = Graduate</span>\",\n    caption = \"Sources: Delta Cost Project & IPEDS\", \n    theme = theme(plot.subtitle = element_markdown(face = \"italic\", size = 9)))\nplot_usfenr_all\n\n\n\n\n\nQuick note about this plot…I was getting an annoying Error in grid.Call(C_textBounds, as.graphicsAnnot(x\\(label), x\\)x, x$y, : polygon edge not found message trying to run these plots. For the presentation I was using the Calibri font, with this call in the theme() section: text = element_text(family = \"Calibri\"). I removed that & the error went away. But this after trying everything from reinstalling Quartz, shutting down all browser windows, running `dev.off()’ in the console…got rid of the special font & no error.\nAnyway…back to the charts.\nI wanted to show USF undergraduate enrollment indexed over time relative to their Jesuit college peers. The final version of the chart is below, complete with annotations I added in power point. There are ways to do similar annotations on r; I used power point because I could do it quicker, with less fuzting around with annotation placement after rendering and saving the image.\nWe see that USF is in the upper quarter of total enrollment growth. Gonzaga & St. Louis University, two schools well-known thanks to success in the NCAA men’s basketball tournament, showed significant growth in the period. Here you might say “but wait, Georgetown has had NCAA success”, and I’d reply “yes, but their success started before 1987, so within this period didn’t grow as much as Gonzaga & St. Louis”.\n\n\n\nJesuit College Enrollment\n\n\nSo how did I make this chart? How did we get the green line for USF, with all else in gray?\nFirst I created a dataframe of the Jesuit colleges, and indexed changes in enrollment to 1.\n\nenrollindex_jes <-\nipeds_fallenroll_8718 %>%\n    filter(jescoll == 1, level == \"Undergraduate\") %>%\n  mutate(enr_pct_change2 = enr_pct_change / 100) %>%\n    mutate(enr_pct_change2 = ifelse(year == \"Fall 1987\", 1, enr_pct_change2)) %>%\n    arrange(UNITID, year) %>%\n    group_by(UNITID) %>%\n    mutate(index_enr_inst = 1) %>%\n    mutate(index_enr_inst = ifelse(year >= \"Fall 1988\", cumsum(enr_pct_change2),\n                                                                 index_enr_inst)) %>%\n    ungroup() %>%\n    ## fix loyola NO b/c of enroll drop after katrina\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year == \"Fall 2006\"), \n                                                    0.833399497, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2007\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2008\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2009\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2010\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2011\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2012\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2013\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2014\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2015\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2016\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2017\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    mutate(index_enr_inst = ifelse((UNITID == \"159656\" & year >= \"Fall 2018\"),\n                lag(index_enr_inst) + enr_pct_change2, index_enr_inst)) %>%\n    select(UNITID, inst_name, year, tot_enr, enr_change, enr_pct_change, \n                 enr_pct_change2, index_enr_inst)\n\nWhy all the manual fixes to Loyola in New Orleans? Well, look at the chart again. See the dip in enrollment around 2005? What might have tanked enrollment at a New Orleans-based college in 2005? Oh right…Hurricane Katrina. To smooth out the drop, I reindexed from the 2006 enrollment point, and added to the index sum after that. For some reason I couldn’t identify, the usual lag from prior year wasn’t working so I just did it manually.\nAs for the plot…to get the green line, I first plotted everything but USF in grey, then plotted USF in green. Saved the plot, then added annotations in power point.\n\nggplot(enrollindex_jes, aes(year, index_enr_inst, group = UNITID)) +\n    geom_line(data = subset(enrollindex_jes, UNITID != \"122612\"), color = \"grey\") +\n    geom_line(data = subset(enrollindex_jes, UNITID == \"122612\"), \n        color = \"#00543C\", size = 1) +\n    scale_y_continuous(limits = c(-.5, 2),\n        breaks = c(-.5, 0, .5, 1, 1.5, 2)) +\n    labs(x = \"\", y = \"\") +\n    theme_minimal() +\n    theme(#text = element_text(family = \"Calibri\"),\n                panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n                axis.text.y = element_text(size = 14))\n\nI had planned to use the Delta Cost/IPEDS tuition & fees data for some charts in the section on affordability, but it worked out better to pull data from the College Board.\nAgain, you can access the presentation in the github repo. If you have questions or comments about the code or the content, you can find me on LinkedIn or Twitter by clicking on the icon links at the bottom of the post or on the main page. Or send me an email if you already know how to find me that way."
  },
  {
    "objectID": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html",
    "href": "posts/tidy-tuesday-apr-07-2020-LeTour-stage1/index.html",
    "title": "Tidy Tuesday, April 07, 2020 - Le Tour! (Stage 1, cleaning the data)",
    "section": "",
    "text": "| \n\nA mountain of data about the Tour de France…\nHaving looked at hiking trails in Washington state and bridges in Maryland I poked around the #TidyTuesday repo and saw this trove of data from back in April on The Tour de France. I love this race, I cycle for exercise, and I love the Kraftwerk album, so of course I had to dig in.\nSo I don’t bury the lede, this is a two-part post. Why? Because there was a lot of data munging & cleaning needed to get the data into shape for whart I wanted to do. So this post is all about what I needed to do on that end. The analysis post will come soon. Also, I’m trying to work out how to do a code show/hide thing in hugo academic so bear with me that the code takes up lots of pixels.\n(*update - migrating to ‘Quarto’ means a native code-fold feature…hooray!)\nSo let’s dig in…first we’ll load packages and create a ’%notin% operator…\n\n# load packages\nlibrary(tidytuesdayR) # to load tidytuesday data\nlibrary(tidyverse) # to do tidyverse things\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(tdf) # to get original stage results file\n\n# create notin operator to help with cleaning & analysis\n`%notin%` <- negate(`%in%`)\n\nThere’s a ton of data here, sourced from the tdf package from Alastair Rushworth and (Thomas Camminady’s data set) (https://github.com/camminady/LeTourDataSet), via Kaggle\nThere are three distinct sets to work thru, each going back to the first run of the race in 1903:\n* A dataframe of overall (General Classification, or Yellow Jersey / maillot jaune) winners from 1903 to 2019 comes from the Tidy Tuesday frame.\n* A dataframe with stage winners for races 1903 to 2017, also in the Tidy Tuesday set, sourced from Kaggle.\n* A frame of overall stage results, sourced from the tdf pacakge due to issues with date conversion in the data included in the Tidy Tuesday set.\nThe stage winner set needs a bit of mungung…I created a stage_results_id column similar to the one in the stage results set. But it needs leading zeros for stages 1-9 so it sorts properly.\nI then got it in my head I wanted results through 2020, so I grabbed them from wikipedia; but the hard way, with copy-paste since my scraping skills aren’t there & I just wanted it done. Data is uploaded to my github repo if you want to use it. (yes, it’s in an excel file…)\n\n\nShow tdf data cleaning pt1\n# load main file from tt repo\ntt_tdf <- tidytuesdayR::tt_load('2020-04-07')\n\n\n\n    Downloading file 1 of 3: `stage_data.csv`\n    Downloading file 2 of 3: `tdf_stages.csv`\n    Downloading file 3 of 3: `tdf_winners.csv`\n\n\nShow tdf data cleaning pt1\n# create race winners set. comes from tdf package. includes up to 2019\ntdf_winners <- as_tibble(tt_tdf$tdf_winners)\n\n# create stage winner set. in tt file, comes from kaggle, includes up to 2017\ntdf_stagewin1 <- tt_tdf$tdf_stages %>%\n  mutate_if(is.character, str_trim)\n  \n# pulled 2018 - 2020 from wikipedia\n# read in excel - need to separate route field to Origin & Destination\ntdf_stagewin2 <- readxl::read_excel(\"data/tdf_stagewinners_2018-20.xlsx\") %>%\n  mutate(Stage = as.character(Stage)) %>%\n  mutate(Date = lubridate::as_date(Date)) %>% \n  separate(Course, c(\"Origin\", \"Destination\"), \"to\", extra = \"merge\") %>%\n  mutate_if(is.character, str_trim) %>%\n  select(Stage, Date, Distance, Origin, Destination, Type, Winner, Winner_Country = Winner_country)\n\n# join with rbind (since I made sure to put 2018-2020 data in same shape as tt set)\n# clean up a bit\ntdf_stagewin <- rbind(tdf_stagewin1, tdf_stagewin2) %>%\n  mutate(race_year = lubridate::year(Date)) %>% \n  mutate(Stage = ifelse(Stage == \"P\", \"0\", Stage)) %>%\n  mutate(stage_ltr = case_when(str_detect(Stage, \"a\") ~ \"a\",\n                               str_detect(Stage, \"b\") ~ \"b\",\n                               str_detect(Stage, \"c\") ~ \"c\",\n                               TRUE ~ \"\")) %>%\n  mutate(stage_num = str_remove_all(Stage, \"[abc]\")) %>%\n  mutate(stage_num = stringr::str_pad(stage_num, 2, side = \"left\", pad = 0)) %>% \n  mutate(stage_results_id = paste0(\"stage-\", stage_num, stage_ltr)) %>%\n  mutate(split_stage = ifelse(stage_ltr %in% c(\"a\", \"b\", \"c\"), \"yes\", \"no\")) %>%\n  \n  # extract first and last names from winner field\n  mutate(winner_first = str_match(Winner, \"(^.+)\\\\s\")[, 2]) %>%\n  mutate(winner_last= gsub(\".* \", \"\", Winner)) %>%\n\n  # clean up stage types, collapse into fewer groups\n  mutate(stage_type = case_when(Type %in% c(\"Flat cobblestone stage\", \"Flat stage\", \"Flat\",\n                                            \"Flat Stage\", \"Hilly stage\", \"Plain stage\", \n                                            \"Plain stage with cobblestones\") \n                                ~ \"Flat / Plain / Hilly\",\n                                Type %in% c(\"High mountain stage\", \"Medium mountain stage\",\n                                            \"Mountain stage\", \"Mountain Stage\", \"Stage with mountain\",\n                                            \"Stage with mountain(s)\", \"Transition stage\")\n                                ~ \"Mountain\",\n                                Type %in% c(\"Individual time trial\", \"Mountain time trial\") \n                                ~ \"Time Trail - Indiv\",\n                                Type == \"Team time trial\" ~ \"Time Trail - Team\",\n                                TRUE ~ \"Other\")) %>% \n  mutate_if(is.character, str_trim) %>%\n  arrange(desc(race_year), stage_results_id) %>%\n  select(race_year, stage_results_id, stage_date = Date, stage_type, Type, split_stage,\n         Origin, Destination, Distance, Winner, winner_first, winner_last,\n         Winner_Country, everything())\n\n# take a look at this awesome dataset\nglimpse(tdf_stagewin)\n\n\nRows: 2,299\nColumns: 16\n$ race_year        <dbl> 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020,…\n$ stage_results_id <chr> \"stage-01\", \"stage-02\", \"stage-03\", \"stage-04\", \"stag…\n$ stage_date       <date> 2020-08-29, 2020-08-30, 2020-08-31, 2020-09-01, 2020…\n$ stage_type       <chr> \"Flat / Plain / Hilly\", \"Mountain\", \"Flat / Plain / H…\n$ Type             <chr> \"Flat stage\", \"Medium mountain stage\", \"Flat stage\", …\n$ split_stage      <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ Origin           <chr> \"Nice\", \"Nice\", \"Nice\", \"Sisteron\", \"Gap\", \"Le Teil\",…\n$ Destination      <chr> \"Nice\", \"Nice\", \"Sisteron\", \"Orcières-Merlette\", \"Pri…\n$ Distance         <dbl> 156.0, 186.0, 198.0, 160.5, 183.0, 191.0, 168.0, 141.…\n$ Winner           <chr> \"Alexander Kristoff\", \"Julian Alaphilippe\", \"Caleb Ew…\n$ winner_first     <chr> \"Alexander\", \"Julian\", \"Caleb\", \"Primož\", \"Wout van\",…\n$ winner_last      <chr> \"Kristoff\", \"Alaphilippe\", \"Ewan\", \"Roglič\", \"Aert\", …\n$ Winner_Country   <chr> \"NOR\", \"FRA\", \"AUS\", \"SLO\", \"BEL\", \"KAZ\", \"BEL\", \"FRA…\n$ Stage            <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"1…\n$ stage_ltr        <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ stage_num        <chr> \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\",…\n\n\nStage data in CSV from the tidy tuesday repository seems to have truncated the times, leaving only the seconds in a character field. To get complete results we need to pull from tdf package using the cleaning script from the Tidy Tuesday page. Some operations will take a while, so best to run as a background job if you want to do something else while it runs. Or go get a cup of coffee.\nIn terms of cleaning:\n* The stage_results_id & rank fields needs leading zeros.\n* The rank field needs a bit of clean-up to fix the 1000s codes.\n* Since rider names were last-first, I wanted to separate out first and last, and also make a field with the full name, but first name in front. Stackoverlflow was my regex friend here.\n* Other minor fixes\nIn the process of cleaning and comparing to the stage winners set, I noticed there were some problems in years where individual stages were split into 2 or 3 legs (A, B & C). Either while it was scraped or combined, the A leg results ended up repeating to the B leg, and in some cases the C leg wasn’t reported. I put it in as an issue in the github repo. But that shouldn’t take away from what’s an amazing dataset to work with. In the analysis section I’ll work around the problems with those stages.\n\n\nShow tdf data cleaning pt2\nall_years <- tdf::editions %>%\n  unnest_longer(stage_results) %>%\n  mutate(stage_results = map(stage_results, ~ mutate(.x, rank = as.character(rank)))) %>%\n  unnest_longer(stage_results)\n\nstage_all <- all_years %>%\n  select(stage_results) %>%\n  flatten_df()\n\ncombo_df <- bind_cols(all_years, stage_all) %>%\n  select(-stage_results)\n\ntdf_stagedata <- as_tibble(combo_df %>%\n  select(edition, start_date,stage_results_id:last_col()) %>%\n  mutate(race_year = lubridate::year(start_date)) %>%\n  rename(age = age...25) %>%\n\n  # to add leading 0 to stage, extract num, create letter, add 0s to num, paste\n  mutate(stage_num = str_replace(stage_results_id, \"stage-\", \"\")) %>%\n  mutate(stage_ltr = case_when(str_detect(stage_num, \"a\") ~ \"a\",\n                               str_detect(stage_num, \"b\") ~ \"b\",\n                               TRUE ~ \"\"))) %>%\n  mutate(stage_num = str_remove_all(stage_num, \"[ab]\")) %>%\n  mutate(stage_num = stringr::str_pad(stage_num, 2, side = \"left\", pad = 0)) %>%\n  mutate(stage_results_id2 = paste0(\"stage-\", stage_num, stage_ltr)) %>%\n  mutate(split_stage = ifelse(stage_ltr %in% c(\"a\", \"b\"), \"yes\", \"no\")) %>%\n\n  # fix 1000s rank. change to DNF\n  mutate(rank = ifelse(rank %in% c(\"1003\", \"1005\", \"1006\"), \"DNF\", rank)) %>%\n  mutate(rank2 = ifelse(rank %notin% c(\"DF\", \"DNF\", \"DNS\", \"DSQ\",\"NQ\",\"OTL\"),\n                        stringr::str_pad(rank, 3, side = \"left\", pad = 0), rank)) %>%\n\n  # extract first and last names from rider field\n  mutate(rider_last = str_match(rider, \"(^.+)\\\\s\")[, 2]) %>%\n  mutate(rider_first= gsub(\".* \", \"\", rider)) %>%\n  mutate(rider_firstlast = paste0(rider_first, \" \", rider_last)) %>%\n  select(-stage_results_id, -start_date, ) %>%\n\n  # fix 1967 & 1968\n  mutate(stage_results_id2 = ifelse((race_year %in% c(1967, 1968) & stage_results_id2 == \"stage-00\"),\n         \"stage-01a\", stage_results_id2)) %>%\n  mutate(stage_results_id2 = ifelse((race_year %in% c(1967, 1968) & stage_results_id2 == \"stage-01\"),\n         \"stage-01b\", stage_results_id2)) %>%\n  mutate(split_stage = ifelse((race_year %in% c(1967, 1968) &\n                                 stage_results_id2 %in% c(\"stage-01a\", \"stage-01b\")),\n                              \"yes\", split_stage)) %>%\n\n  select(edition, race_year, stage_results_id = stage_results_id2, split_stage,\n         rider, rider_first, rider_last, rider_firstlast, rank2,\n         time, elapsed, points, bib_number, team, age, everything())\n\nsaveRDS(tdf_stagedata, \"data/tdf_stagedata.rds\")\n\n\n\ntdf_stagedata <- readRDS(\"data/tdf_stagedata.rds\")\nglimpse(tdf_stagedata)\n\nRows: 255,752\nColumns: 18\n$ edition          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ race_year        <dbl> 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903,…\n$ stage_results_id <chr> \"stage-01\", \"stage-01\", \"stage-01\", \"stage-01\", \"stag…\n$ split_stage      <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ rider            <chr> \"Garin Maurice\", \"Pagie Émile\", \"Georget Léon\", \"Auge…\n$ rider_first      <chr> \"Maurice\", \"Émile\", \"Léon\", \"Fernand\", \"Jean\", \"Marce…\n$ rider_last       <chr> \"Garin\", \"Pagie\", \"Georget\", \"Augereau\", \"Fischer\", \"…\n$ rider_firstlast  <chr> \"Maurice Garin\", \"Émile Pagie\", \"Léon Georget\", \"Fern…\n$ rank2            <chr> \"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008…\n$ time             <Period> 17H 45M 13S, 55S, 34M 59S, 1H 2M 48S, 1H 4M 53S, 1…\n$ elapsed          <Period> 17H 45M 13S, 17H 46M 8S, 18H 20M 12S, 18H 48M 1S, …\n$ points           <int> 100, 70, 50, 40, 32, 26, 22, 18, 14, 10, 8, 6, 4, 2, …\n$ bib_number       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ team             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ age              <int> 32, 32, 23, 20, 36, 37, 25, 33, NA, 22, 26, 28, 21, 2…\n$ rank             <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"1…\n$ stage_num        <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n$ stage_ltr        <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n\n\nPoking around the Kaggle site referenced above I found these datasets of final results for all riders in all races since 1903. A few different fields than in the tidy tuesday winners set.\nNow this is a ton of data to work with, and I won’t use it all. Figured I’d include the code to get it all in case you get inspired to grab it and take a look.\nOk…that’s it for cleaning & prepping…charts and tables in Stage 2.\nThis post was last updated on 2023-05-19"
  },
  {
    "objectID": "posts/tidy-tuesday-apr-07-2020-LeTour-stage2/index.html",
    "href": "posts/tidy-tuesday-apr-07-2020-LeTour-stage2/index.html",
    "title": "Tidy Tuesday, April 07, 2020 - Le Tour! (Stage 2, charts!)",
    "section": "",
    "text": "Back in the saddle for Stage 2 of the Tour de France data ride\nStage 1 ended up being all about wrangling and cleaning the #TidyTuesday Tour de France data. When I first dug into the data I wasn’t sure what I wanted to visualize. It wasn’t until I spent some time living with the data, seeing what was there and looking at the #tidytuesday TdF submissions on Twitter so I didn’t repeat what was done that I decided I wanted to look at results by stage, specifically the gaps between the winners of each stage and the times recorded for the next-best group and the last rider(s) across the line. Charlie Gallagher took a similar approach at the data, using overall race results for the GC riders.\nA quick but important aside - in the Tour, as in most (all?) UCI races, while each rider is accorded a place - 1, 2, 3, etc… - times are calculated by identifiable groups crossing the line. So let’s say you are 2nd to 15th in the 1st group (of 15 total riders) that crosses with barely any daylight between riders; you each get the same time as the winner. But only 1 rider wins the stage. In any stage, there could be only 2 or 3 identifiable time groups, or there could be many groups. Depends on the stage type and other factors - crashes, where in the race the stage took place, etc…\nWhat this means for my project here is I needed to wrangle data so that I was able to identify two time groups apart from the winner; the next best group and the last group. Each group could have more than 1 rider. Download and clean the stage results data and you’ll see what I mean.\nSo let’s look at some code and charts.\nAt the end of Stage 1 we had a number of data frames. I’m joining two for this analysis, one with stage winners (which has important stage characteristic data) and a set of all riders in every stage from 1903 to 2019. We’ll first load the packages we need…\n\n# load packages\nlibrary(tidyverse) # to do tidyverse things\nlibrary(lubridate) # to do things with dates & times\nlibrary(tidylog) # to get a log of what's happening to the data\nlibrary(patchwork) # to stitch together plots\n\n# create notin operator to help with cleaning & analysis\n`%notin%` <- negate(`%in%`)\n\nThen join the sets. For the purposes of this post I’ll just load an RDS I created (it’s not uploaded to the repo, sorry, but you can recreate it with the code.\n\ntdf_stageall <- merge(tdf_stagedata, tdf_stagewin, by.x = c(\"race_year\", \"stage_results_id\"),\n                      by.y = c(\"race_year\", \"stage_results_id\"), all = T)\n\n\ntdf_stageall <- readRDS(\"data/tdf_stageall.rds\")\nglimpse(tdf_stageall)\n\nRows: 255,807\nColumns: 32\n$ race_year        <dbl> 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903, 1903,…\n$ stage_results_id <chr> \"stage-01\", \"stage-01\", \"stage-01\", \"stage-01\", \"stag…\n$ edition          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ split_stage.x    <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ rider            <chr> \"Garin Maurice\", \"Pagie Émile\", \"Georget Léon\", \"Auge…\n$ rider_first      <chr> \"Maurice\", \"Émile\", \"Léon\", \"Fernand\", \"Jean\", \"Marce…\n$ rider_last       <chr> \"Garin\", \"Pagie\", \"Georget\", \"Augereau\", \"Fischer\", \"…\n$ rider_firstlast  <chr> \"Maurice Garin\", \"Émile Pagie\", \"Léon Georget\", \"Fern…\n$ rank2            <chr> \"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008…\n$ time             <Period> 17H 45M 13S, 55S, 34M 59S, 1H 2M 48S, 1H 4M 53S, 1…\n$ elapsed          <Period> 17H 45M 13S, 17H 46M 8S, 18H 20M 12S, 18H 48M 1S, …\n$ points           <int> 100, 70, 50, 40, 32, 26, 22, 18, 14, 10, 8, 6, 4, 2, …\n$ bib_number       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ team             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ age              <int> 32, 32, 23, 20, 36, 37, 25, 33, NA, 22, 26, 28, 21, 2…\n$ rank             <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"1…\n$ stage_num.x      <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n$ stage_ltr.x      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ stage_date       <date> 1903-07-01, 1903-07-01, 1903-07-01, 1903-07-01, 1903…\n$ stage_type       <chr> \"Flat / Plain / Hilly\", \"Flat / Plain / Hilly\", \"Flat…\n$ Type             <chr> \"Plain stage\", \"Plain stage\", \"Plain stage\", \"Plain s…\n$ split_stage.y    <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ Origin           <chr> \"Paris\", \"Paris\", \"Paris\", \"Paris\", \"Paris\", \"Paris\",…\n$ Destination      <chr> \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon\", \"Lyon…\n$ Distance         <dbl> 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467…\n$ Winner           <chr> \"Maurice Garin\", \"Maurice Garin\", \"Maurice Garin\", \"M…\n$ winner_first     <chr> \"Maurice\", \"Maurice\", \"Maurice\", \"Maurice\", \"Maurice\"…\n$ winner_last      <chr> \"Garin\", \"Garin\", \"Garin\", \"Garin\", \"Garin\", \"Garin\",…\n$ Winner_Country   <chr> \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA\", \"FRA…\n$ Stage            <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ stage_ltr.y      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ stage_num.y      <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n\n\nThis set has many columns that we’ll build off of to use in analysis going forward. To get the changes in gaps by stage types, we’ll build another set. Because we want to look both at changes in stage types and gaps between winners and the field, the trick here is to sort out for each stage in each race year who the winners are (easy), who has the slowest time (mostly easy) and who has the 2nd best record time.\nThat last item it tough because of the time & rank method I described above. The script below is commented to show why I did what I did. Much of the code comes from looking at the data and seeing errors, issues, etc. Not including that code here. Also, much of my ability to spot errors comes from knowledge about the race, how it’s timed, some history. Domain knowledge helps a lot when cleaning & analyzing data.\n\nstage_gap <-\ntdf_stageall %>%\n  arrange(race_year, stage_results_id, rank2) %>%\n  #  delete 1995 stage 16 - neutralized due to death in stage 15, all times the same\n  mutate(out = ifelse((race_year == 1995 & stage_results_id == \"stage-16\"),\n                       \"drop\", \"keep\")) %>%\n  filter(out != \"drop\") %>%\n  # delete  missing times\n  filter(!is.na(time)) %>%\n  # remove non-finishers/starters, change outside time limit rank to numeric to keep in set\n  filter(rank %notin% c(\"DF\", \"DNF\", \"DNS\", \"DSQ\", \"NQ\")) %>%\n  filter(!is.na(rank)) %>%\n\n  # OTLs are ejected from the race because they finished outside a time limit. But we need them in the set.\n  mutate(rank_clean = case_when(rank == \"OTL\" ~ \"999\",\n                           TRUE ~ rank)) %>% \n  # sortable rank field\n  mutate(rank_n = as.integer(rank_clean)) %>%\n  # creates total time in minutes as numeric, round it to 2 digits\n  mutate(time_minutes = ifelse(!is.na(elapsed),\n                              day(elapsed)*1440 + hour(elapsed)*60 + minute(elapsed) + second(elapsed)/60,\n                               NA)) %>%\n  mutate(time_minutes = round(time_minutes, 2)) %>%\n  \n  # create field for difference from winner\n  group_by(race_year, stage_results_id) %>% \n  arrange(race_year, stage_results_id, time_minutes, rank2) %>%\n\n  mutate(time_diff = time_minutes - min(time_minutes)) %>%\n  mutate(time_diff_secs = time_diff*60) %>%\n  mutate(time_diff = round(time_diff, 2)) %>%\n  mutate(time_diff_secs = round(time_diff_secs, 0)) %>%\n  mutate(time_diff_period = seconds_to_period(time_diff_secs)) %>%\n  mutate(rank_mins = rank(time_minutes, ties.method = \"first\")) %>%\n  # create rank field to use to select winner, next best, last\n  mutate(compare_grp = case_when(rank_n == 1 ~ \"Winner\",\n                                 (rank_n > 1 & time_diff_secs > 0 & rank_mins != max(rank_mins))\n                                 ~ \"Next best2\",\n                                  rank_mins == max(rank_mins) ~ \"Last\",\n                                 TRUE ~ \"Other\")) %>%\n  ungroup() %>%\n  group_by(race_year, stage_results_id, compare_grp) %>% \n  arrange(race_year, stage_results_id, rank_mins) %>%\n  mutate(compare_grp = ifelse((compare_grp == \"Next best2\" & rank_mins == min(rank_mins)),\n                               \"Next best\", compare_grp)) %>%\n  mutate(compare_grp = ifelse(compare_grp == \"Next best2\", \"Other\", compare_grp)) %>%\n  ungroup() %>%\n  mutate(compare_grp = factor(compare_grp, levels = c(\"Winner\", \"Next best\", \"Last\", \"Other\"))) %>%\n  # create race decade field\n  mutate(race_decade = floor(race_year / 10) * 10) %>%\n  mutate(race_decade = as.character(paste0(race_decade, \"s\"))) %>%\n  # keep only winner, next, last\n  filter(compare_grp != \"Other\") %>%\n  select(race_year, race_decade, stage_results_id, stage_type, rider_firstlast, bib_number, Winner_Country,\n         rank, rank_clean, rank_n, time, elapsed, time_minutes, time_diff, time_diff_secs, time_diff_period, \n         rank_mins, compare_grp) \n\nOk, finally, let’s see what this data looks like. First a chart to show averages and quartile ranges for the gaps by stage type. Create a data object with the values, then the plots. Faceting by stage type didn’t work because the y axis ranges were very different. So we’ll use patchwork to stitch them together in one plot. The medians are the red dots, interquartile ranges at either end of the line, and means are in black. I included both means & medians because the spread for some stage types was so great.\n\n\nShow stage gap charts code\ngapranges <- stage_gap %>%\n  filter(compare_grp != \"Winner\") %>%\n  filter(stage_type %notin% c(\"Other\", \"Time Trial - Team\")) %>%\n  group_by(stage_type, compare_grp) %>%\n  summarise(num = n(), \n            lq = quantile(time_diff_secs, 0.25),\n            medgap = median(time_diff_secs),\n            uq = quantile(time_diff_secs, 0.75),\n            lq_tp = (seconds_to_period(quantile(time_diff_secs, 0.25))),\n            medgap_tp = (seconds_to_period(median(time_diff_secs))),\n            uq_tp = (seconds_to_period(quantile(time_diff_secs, 0.75))),\n            avggap = round(mean(time_diff_secs),2),\n            avggap_tp = round(seconds_to_period(mean(time_diff_secs)), 2))\n\ngapplot1 <-\ngapranges %>%\n  filter(compare_grp == \"Next best\") %>%\n  ggplot(aes(stage_type, medgap, color = avggap)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"#0055A4\") +\n  geom_point(size = 2, color = \"#EF4135\") +\n  geom_point(aes(y = avggap), size = 2, color = \"black\", alpha = .8) +\n  geom_text(aes(label = medgap_tp), \n            size = 3, color = \"#EF4135\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = uq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = lq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.2) +\n  geom_text(aes(label = avggap_tp, y = avggap_tp),\n            size = 3, color = \"black\", alpha = .8, hjust = -.1) +\n  labs(title = \"Time Gap from Stage Winner to Next Best Time\",\n       subtitle = \"Median & Inter-quartile Ranges (avg in black)\",\n       y = \"Time Gap from Winner\", x = \"Stage Type\") +\n  theme_light() +\n  theme(plot.title = element_text(color = \"#0055A4\", size = 9),\n        plot.subtitle = element_text(face = \"italic\", color = \"#EF4135\",\n                                     size = 8),\n        axis.title.x = element_text(color = \"#0055A4\"),\n        axis.title.y = element_text(color = \"#0055A4\"), \n        axis.text.x = element_text(color = \"#0055A4\"),\n        axis.text.y=element_blank())\n\ngapplot2 <-\ngapranges %>%\n  filter(compare_grp == \"Last\") %>%\n  ggplot(aes(stage_type, medgap, color = avggap)) +\n  geom_linerange(aes(ymin = lq, ymax = uq), size = 2, color = \"#0055A4\") +\n  geom_point(size = 2, color = \"#EF4135\") +\n  geom_point(aes(y = avggap), size = 2, color = \"black\", alpha = .8) +\n  geom_text(aes(label = medgap_tp), \n            size = 3, color = \"#EF4135\", hjust = 1.2) +\n  geom_text(aes(y = uq, label = uq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.2) +\n  geom_text(aes(y = lq, label = lq_tp), \n            size = 3, color = \"#0055A4\", hjust = 1.1) +\n  geom_text(aes(label = avggap_tp, y = avggap_tp),\n            size = 3, color = \"black\", alpha = .8, hjust = -.1) +\n  labs(title = \"Time Gap from Stage Winner to Slowest Time\",\n       subtitle = \"Median & Inter-quartile Ranges (avg in black)\",\n       y = \"\", x = \"Stage Type\") +\n  theme_light() +\n  theme(plot.title = element_text(color = \"#0055A4\", size = 9),\n        plot.subtitle = element_text(face = \"italic\", color = \"#EF4135\",\n                                     size = 8),\n        axis.title.x = element_text(color = \"#0055A4\", size = 9),\n        axis.text.x = element_text(color = \"#0055A4\", size = 9),\n        axis.text.y=element_blank())\n\ngapplot1 + gapplot2 +\n  plot_annotation(title = \"Tour de France Stages, 1903 to 2019\",\n                  theme = theme(plot.title = \n                                  element_text(color = \"#0055A4\", size = 10)))\n\n\n\n\n\nWhat do these charts tell us? Well unsurprisingly mountain stages tend to have longer gaps between winners and the rest of the field than do flat/plain/hilly stages. Time trials are usually on flat or hilly stages, so they behave more like all other flat/plain/hilly stages. Even looking at the median to smooth for outliers, half of the last men in on mountain stages came in under 36 minutes, half over 36 minutes. The last 25% of mountain-stage riders came in an hour or more after the winner.\nHow has this changed over time? Well let’s facet out by degree decade.\nFirst thing that needs doing is to build a dataframe for analysis - it will have medians my race year and stage type. But for the chart we want to have a decade field. Turns out this was a bit complicated in order to get the chart I wanted. You can see in the code comments why I did what I did.\n\n\nShow df build code\ngaprangesyrdec <- \nstage_gap %>%\n  filter(compare_grp != \"Winner\") %>%\n  filter(stage_type %notin% c(\"Other\", \"Time Trial - Team\")) %>%\n  group_by(stage_type, compare_grp, race_year) %>%\n  summarise(num = n(), \n            lq = quantile(time_diff_secs, 0.25),\n            medgap = median(time_diff_secs),\n            uq = quantile(time_diff_secs, 0.75),\n            lq_tp = (seconds_to_period(quantile(time_diff_secs, 0.25))),\n            medgap_tp = (seconds_to_period(median(time_diff_secs))),\n            uq_tp = (seconds_to_period(quantile(time_diff_secs, 0.75))),\n            avggap = round(mean(time_diff_secs),2),\n            avggap_tp = round(seconds_to_period(mean(time_diff_secs)), 2)) %>%\n  ungroup() %>%\n  # need to hard code in rows so x axis & faceting works in by decade charts\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1915, .before = 13) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1916, .before = 14) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1917, .before = 15) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Next best\",\n          race_year = 1918, .before = 16) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1915, .before = 123) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1916, .before = 124) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1917, .before = 125) %>%\n  add_row(stage_type = \"Flat / Plain / Hilly\",  compare_grp = \"Last\",\n          race_year = 1918, .before = 126) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1915, .before = 233) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1916, .before = 234) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1917, .before = 235) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Next best\",\n          race_year = 1918, .before = 236) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1915, .before = 343) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1916, .before = 344) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1917, .before = 345) %>%\n  add_row(stage_type = \"Mountain\",  compare_grp = \"Last\",\n          race_year = 1918, .before = 346) %>%\n\n    # need field for x axis when faciting by decade\n  mutate(year_n = str_sub(race_year,4,4)) %>%\n  # create race decade field\n  mutate(race_decade = floor(race_year / 10) * 10) %>%\n  mutate(race_decade = as.character(paste0(race_decade, \"s\"))) %>%\n#  mutate(race_decade = ifelse(race_year %in%))\n  arrange(stage_type, compare_grp, race_year) %>%\n  select(stage_type, compare_grp, race_year, year_n, race_decade, everything())\n\n\nNow that we have a dataframe to work from, let’s make a chart. But to do that we have to make a few charts and then put them together with the patchwork package.\nFirst up is changes in the mountain stages and the median gaps between winner and next best recorded time. I grouped into three decade sets. Note that because of changes in the gaps over time, the y axes are a bit different in the early decades of the race. Also note at how I was able to get hours:seconds:minutes to show up on the y axis. The x axis digits are that way because race year would repeat in each facet, so I had to create a proxy year.\n\n\nShow mountain stage gap charts code\n# mountain winner to next best\nplot_dec_mtnb1 <-\ngaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n#  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Mountain\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\" , size = 8),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtnb2 <-\ngaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 420), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\" , size = 8),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtnb3 <-\ngaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 420), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\" , size = 8),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtnb1 / plot_dec_mtnb2 / plot_dec_mtnb3 +\n  plot_annotation(title = \"Gaps Between Winner & Next Best Times are Narrowing\",\n                  subtitle = \"Median gap on mountain stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nWhat does this chart tell us? As you look at it, keep in mind the y axis is different in the 1900s - 1930s chart because in the early years of the race the gaps were much wider.\nMost obviously, and not surprisingly, the gaps between winner and next best time shrank as the race professionalized and sports science got better. There are of course outliers here and there in the last few decades, but the course changes year-to-year, and some years the race organizers have made some years more difficult than other in the mountains.\nWe also see the effect of war. The two world wars not only interrupted the race in those years, but especially in the years immediately after WWII the gaps were larger than in the late 1930s. We can imagine what the war did to the pool of riders. The sport needed time to recover, for riders to train and get back to full fitness.\nOk, now let’s look at the changes in the mountains from the winners to the time for the last rider(s). The only change from the last set of charts is filter(compare_grp == \"Last\")\n\n\nShow mountain stage gap charts code\n# mountain winner to last\nplot_dec_mtla1 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n  #  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Last\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtla2 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 5400), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtla3 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Mountain\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 5400), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_mtla1 / plot_dec_mtla2 / plot_dec_mtla3  +\n  plot_annotation(title = \"Gaps Between Winner & Last Rider Times Mostly Stable Since 1950s\",\n                  subtitle = \"Median gap on mountain stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nWhat do we see here? Well first, notice that the gaps in the 1900s to 1930s were huge, especially before the 1930s. By the 1930s the gaps was usually around 30-40 minutes, similar to post-WWII years. But in the early years of the race, the last man in sometimes wouldn’t arrive until 10+ hours after the winner!\nBut since then the gaps are mostly around 30+ minutes. And again, I adjusted to include racers who finish outside of the time-stage cut off, and are thus eliminated from the race overall.\nOk, last two charts in this series…this time we’ll look at the flat & hilly stages. The only code changes are to the filters: filter(compare_grp == \"Next best\") or filter(compare_grp == \"Last\") and filter(stage_type == \"Flat / Plain / Hilly\").\n\n\nShow flat/hilly stage gap charts code\n# flat/hilly next best\nplot_dec_flnb1 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n  #  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Mountain\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flnb2 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 300), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flnb3 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Next best\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 300), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 7),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flnb1 / plot_dec_flnb2 / plot_dec_flnb3 +\n  plot_annotation(title = \"Gaps Between Winner & Next Best Times Mostly < 1 Minute Since 1970s\",\n                  subtitle = \"Median gap on flat & hilly stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nPerhaps the most surprising thing in the Flat/Hilly stage gaps between winners & next best is that the gaps were similar to mountain stages. But then from watching the race all these years I remember that the climbers finish in groups fairly near to each other, even if the mountain stages are so hard.\nNo surprise of course that for many decades now the gaps have been around or under a minute. After the bunch sprints, the next group of riders, those not contesting the win, are right behind that pack.\n\n\nShow flat/hilly stage gap charts code\n### flat / hilly winner to last\nplot_dec_flla1 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1900s\", \"1910s\", \"1920s\", \"1930s\")) %>%\n  #  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  geom_point(data = subset(gaprangesyrdec, \n                           (race_year == 1919 & stage_type == \"Mountain\" & \n                              compare_grp == \"Next best\" & year_n == \"9\")), \n             aes(x = year_n, y = medgap), color = \"#EF4135\") +\n  scale_y_time(labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flla2 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1940s\", \"1950s\", \"1960s\", \"1970s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 2340), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flla3 <-\n  gaprangesyrdec %>%\n  filter(compare_grp == \"Last\") %>%\n  filter(stage_type == \"Flat / Plain / Hilly\") %>%\n  filter(race_decade %in% c(\"1980s\", \"1990s\", \"2000s\", \"2010s\")) %>%\n  ggplot(aes(year_n, medgap)) +\n  geom_line(group = 1, color = \"#EF4135\") +\n  scale_y_time(limits = c(0, 2340), labels = waiver()) +\n  labs(x = \"Year\", y = \"H:Min:Sec\") + \n  facet_grid( ~ race_decade) +\n  theme_light() +\n  theme(axis.title.x = element_text(color = \"#0055A4\", size = 8),\n        axis.title.y = element_text(color = \"#0055A4\", size = 7),\n        axis.text.x = element_text(color = \"#0055A4\", size = 8),\n        axis.text.y = element_text(color = \"#0055A4\", size = 7),\n        strip.background = element_rect(fill = \"#0055A4\"), strip.text.x = element_text(size = 8))\n\nplot_dec_flla1 / plot_dec_flla2 / plot_dec_flla3 +\n  plot_annotation(title = \"Gaps Between Winner & Last Rider Times Very Tight by 1970s, Stabilized to ~ 10 min since\",\n                  subtitle = \"Median gap on flat & hilly stages, by year & decade; no race during world wars\",\n                  theme = \n                    theme(plot.title = element_text(color = \"#0055A4\", size = 10),\n                          plot.subtitle = element_text(color = \"#EF4135\", \n                                                       face = \"italic\", size = 9)))\n\n\n\n\n\nThe gap from winner to last was much less than winner-to-last in mountains, which isn’t a surprise. The sprinters tend to suffer in the Alps, Pyrenees and other mountain stages. As long as they come in under the time threshold, they are likely to be well behind on the day. But on flat stages, the only thing that keeps a rider more than a few minutes back is a spill, flat tire, or just having a bad day.\nNow it’s worth noting that I did not normalize for stage distance or elevation gain (for mountain stages) in terms of comparing year to year. I went with the assumption that since I was grouping multiple stages into a year, that even over time this would normalize itself. If this were a more serious analysis I’d do it.\nAnother extension of this analysis would be a model to predict time gaps. Then I’d include stage distance & gain, rider height/weight, and other factors.\nSome shout-outs are in order. First of course to the #tidytuesday crew. For the data here:\n* Alastair Rushworth and his tdf package\n* Thomas Camminady and his Le Tour dataset\nThis post was last updated on 2023-05-19"
  }
]